{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from model.WGAN import WGAN ## not readymade module \n",
    "from utils.loaders import load_cifar ## not readymade module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SECTION = 'gan'\n",
    "RUN_ID = '002'\n",
    "DATA_NAME = 'horses'\n",
    "RUN_FOLDER = 'run/{}/'.format(SECTION)\n",
    "RUN_FOLDER += '_'.join([RUN_ID, DATA_NAME])\n",
    "\n",
    "if not os.path.exists(RUN_FOLDER):\n",
    "    os.makedirs(RUN_FOLDER)\n",
    "    os.mkdir(os.path.join(RUN_FOLDER, 'viz'))\n",
    "    os.mkdir(os.path.join(RUN_FOLDER, 'images'))\n",
    "    os.mkdir(os.path.join(RUN_FOLDER, 'weights'))\n",
    "    \n",
    "mode = 'build'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATA_NAME == 'cars':\n",
    "    label = 1 \n",
    "elif DATA_NAME == 'horses':\n",
    "    label = 7 \n",
    "(x_train, y_train) = load_cifar(label, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x24186288f98>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[150, :,:,:]* 0.5+1*0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == 'build':\n",
    "    gan = WGAN()\n",
    "    \n",
    "    #gan.save(RUN_FOLDER)\n",
    "    \n",
    "else:\n",
    "    gan.load_weights(os.path.join(RUN_FOLDER, 'weights/weights.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "critic_input (InputLayer)    [(None, 32, 32, 3)]       0         \n",
      "_________________________________________________________________\n",
      "critic_conv_0 (Conv2D)       (None, 16, 16, 32)        2432      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "critic_conv_1 (Conv2D)       (None, 8, 8, 64)          51264     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "critic_conv_2 (Conv2D)       (None, 4, 4, 128)         204928    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "critic_conv_3 (Conv2D)       (None, 4, 4, 128)         409728    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 2049      \n",
      "=================================================================\n",
      "Total params: 670,401\n",
      "Trainable params: 670,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan.critic.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "generator_input (InputLayer) [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2048)              206848    \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 2048)              8192      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d (UpSampling2D) (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "generator_conv_0 (Conv2D)    (None, 8, 8, 128)         409728    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "generator_conv_1 (Conv2D)    (None, 16, 16, 64)        204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "generator_conv_2 (Conv2D)    (None, 32, 32, 32)        51232     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "generator_conv_3 (Conv2DTran (None, 32, 32, 3)         2403      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 32, 32, 3)         0         \n",
      "=================================================================\n",
      "Total params: 884,163\n",
      "Trainable params: 879,619\n",
      "Non-trainable params: 4,544\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan.generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "model_input (InputLayer)     [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "model_1 (Model)              (None, 32, 32, 3)         884163    \n",
      "_________________________________________________________________\n",
      "model (Model)                (None, 1)                 670401    \n",
      "=================================================================\n",
      "Total params: 1,554,564\n",
      "Trainable params: 1,550,020\n",
      "Non-trainable params: 4,544\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCHS = 6000\n",
    "PRINT_EVERY_N_BATCHES = 10\n",
    "N_CRITIC = 5\n",
    "CLIP_THRESHOLD = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: (-0.000)(R -0.001, F 0.000)]  [G loss: -0.001] \n",
      "1 [D loss: (-0.000)(R -0.001, F 0.000)]  [G loss: -0.001] \n",
      "2 [D loss: (-0.001)(R -0.002, F 0.001)]  [G loss: -0.003] \n",
      "3 [D loss: (-0.002)(R -0.006, F 0.001)]  [G loss: -0.008] \n",
      "4 [D loss: (-0.002)(R -0.011, F 0.006)]  [G loss: -0.013] \n",
      "5 [D loss: (-0.005)(R -0.020, F 0.010)]  [G loss: -0.022] \n",
      "6 [D loss: (-0.003)(R -0.028, F 0.021)]  [G loss: -0.033] \n",
      "7 [D loss: (-0.002)(R -0.030, F 0.026)]  [G loss: -0.039] \n",
      "8 [D loss: (-0.002)(R -0.031, F 0.026)]  [G loss: -0.037] \n",
      "9 [D loss: (-0.005)(R -0.031, F 0.020)]  [G loss: -0.034] \n",
      "10 [D loss: (-0.008)(R -0.036, F 0.021)]  [G loss: -0.037] \n",
      "11 [D loss: (-0.013)(R -0.043, F 0.017)]  [G loss: -0.051] \n",
      "12 [D loss: (-0.021)(R -0.064, F 0.022)]  [G loss: -0.085] \n",
      "13 [D loss: (-0.038)(R -0.098, F 0.021)]  [G loss: -0.144] \n",
      "14 [D loss: (-0.043)(R -0.121, F 0.036)]  [G loss: -0.201] \n",
      "15 [D loss: (-0.037)(R -0.139, F 0.066)]  [G loss: -0.216] \n",
      "16 [D loss: (-0.036)(R -0.148, F 0.077)]  [G loss: -0.219] \n",
      "17 [D loss: (-0.043)(R -0.157, F 0.071)]  [G loss: -0.245] \n",
      "18 [D loss: (-0.059)(R -0.169, F 0.051)]  [G loss: -0.272] \n",
      "19 [D loss: (-0.028)(R -0.163, F 0.107)]  [G loss: -0.285] \n",
      "20 [D loss: (-0.037)(R -0.157, F 0.084)]  [G loss: -0.328] \n",
      "21 [D loss: (-0.019)(R -0.140, F 0.101)]  [G loss: -0.457] \n",
      "22 [D loss: (-0.063)(R -0.296, F 0.170)]  [G loss: -0.566] \n",
      "23 [D loss: (-0.047)(R -0.303, F 0.208)]  [G loss: -0.620] \n",
      "24 [D loss: (-0.010)(R -0.246, F 0.227)]  [G loss: -0.554] \n",
      "25 [D loss: (0.004)(R -0.188, F 0.196)]  [G loss: -0.409] \n",
      "26 [D loss: (-0.020)(R -0.171, F 0.131)]  [G loss: -0.325] \n",
      "27 [D loss: (-0.024)(R -0.136, F 0.089)]  [G loss: -0.276] \n",
      "28 [D loss: (-0.018)(R -0.115, F 0.079)]  [G loss: -0.198] \n",
      "29 [D loss: (-0.022)(R -0.092, F 0.048)]  [G loss: -0.104] \n",
      "30 [D loss: (-0.045)(R -0.081, F -0.009)]  [G loss: -0.020] \n",
      "31 [D loss: (-0.053)(R -0.062, F -0.044)]  [G loss: 0.057] \n",
      "32 [D loss: (-0.065)(R -0.047, F -0.082)]  [G loss: 0.123] \n",
      "33 [D loss: (-0.073)(R -0.030, F -0.115)]  [G loss: 0.179] \n",
      "34 [D loss: (-0.078)(R -0.011, F -0.144)]  [G loss: 0.229] \n",
      "35 [D loss: (-0.079)(R 0.017, F -0.174)]  [G loss: 0.258] \n",
      "36 [D loss: (-0.064)(R 0.036, F -0.165)]  [G loss: 0.246] \n",
      "37 [D loss: (-0.045)(R 0.036, F -0.127)]  [G loss: 0.222] \n",
      "38 [D loss: (-0.040)(R 0.041, F -0.122)]  [G loss: 0.204] \n",
      "39 [D loss: (-0.036)(R 0.043, F -0.116)]  [G loss: 0.195] \n",
      "40 [D loss: (-0.049)(R 0.038, F -0.136)]  [G loss: 0.212] \n",
      "41 [D loss: (-0.078)(R 0.020, F -0.175)]  [G loss: 0.248] \n",
      "42 [D loss: (-0.090)(R 0.025, F -0.205)]  [G loss: 0.293] \n",
      "43 [D loss: (-0.108)(R 0.022, F -0.237)]  [G loss: 0.313] \n",
      "44 [D loss: (-0.113)(R 0.021, F -0.246)]  [G loss: 0.336] \n",
      "45 [D loss: (-0.106)(R 0.050, F -0.261)]  [G loss: 0.322] \n",
      "46 [D loss: (-0.087)(R 0.057, F -0.231)]  [G loss: 0.290] \n",
      "47 [D loss: (-0.069)(R 0.074, F -0.211)]  [G loss: 0.250] \n",
      "48 [D loss: (-0.053)(R 0.081, F -0.187)]  [G loss: 0.217] \n",
      "49 [D loss: (-0.045)(R 0.069, F -0.160)]  [G loss: 0.215] \n",
      "50 [D loss: (-0.045)(R 0.046, F -0.136)]  [G loss: 0.186] \n",
      "51 [D loss: (-0.034)(R 0.037, F -0.105)]  [G loss: 0.166] \n",
      "52 [D loss: (-0.049)(R 0.022, F -0.119)]  [G loss: 0.155] \n",
      "53 [D loss: (-0.050)(R 0.001, F -0.100)]  [G loss: 0.147] \n",
      "54 [D loss: (-0.052)(R -0.004, F -0.099)]  [G loss: 0.136] \n",
      "55 [D loss: (-0.044)(R -0.018, F -0.071)]  [G loss: 0.145] \n",
      "56 [D loss: (-0.048)(R -0.011, F -0.084)]  [G loss: 0.169] \n",
      "57 [D loss: (-0.065)(R -0.006, F -0.125)]  [G loss: 0.176] \n",
      "58 [D loss: (-0.054)(R 0.027, F -0.136)]  [G loss: 0.180] \n",
      "59 [D loss: (-0.058)(R 0.028, F -0.144)]  [G loss: 0.169] \n",
      "60 [D loss: (-0.062)(R 0.011, F -0.134)]  [G loss: 0.155] \n",
      "61 [D loss: (-0.042)(R 0.056, F -0.139)]  [G loss: 0.154] \n",
      "62 [D loss: (-0.041)(R 0.081, F -0.163)]  [G loss: 0.163] \n",
      "63 [D loss: (-0.028)(R 0.138, F -0.194)]  [G loss: 0.180] \n",
      "64 [D loss: (-0.016)(R 0.149, F -0.181)]  [G loss: 0.205] \n",
      "65 [D loss: (-0.000)(R 0.159, F -0.159)]  [G loss: 0.215] \n",
      "66 [D loss: (0.012)(R 0.158, F -0.133)]  [G loss: 0.213] \n",
      "67 [D loss: (-0.006)(R 0.127, F -0.139)]  [G loss: 0.217] \n",
      "68 [D loss: (-0.011)(R 0.116, F -0.137)]  [G loss: 0.210] \n",
      "69 [D loss: (-0.015)(R 0.118, F -0.149)]  [G loss: 0.215] \n",
      "70 [D loss: (-0.017)(R 0.126, F -0.160)]  [G loss: 0.225] \n",
      "71 [D loss: (-0.044)(R 0.111, F -0.198)]  [G loss: 0.248] \n",
      "72 [D loss: (-0.041)(R 0.129, F -0.210)]  [G loss: 0.266] \n",
      "73 [D loss: (-0.053)(R 0.133, F -0.238)]  [G loss: 0.283] \n",
      "74 [D loss: (-0.050)(R 0.139, F -0.239)]  [G loss: 0.289] \n",
      "75 [D loss: (-0.029)(R 0.132, F -0.190)]  [G loss: 0.254] \n",
      "76 [D loss: (-0.012)(R 0.158, F -0.183)]  [G loss: 0.238] \n",
      "77 [D loss: (-0.018)(R 0.136, F -0.171)]  [G loss: 0.217] \n",
      "78 [D loss: (-0.012)(R 0.146, F -0.170)]  [G loss: 0.211] \n",
      "79 [D loss: (-0.026)(R 0.132, F -0.185)]  [G loss: 0.224] \n",
      "80 [D loss: (-0.028)(R 0.144, F -0.199)]  [G loss: 0.238] \n",
      "81 [D loss: (-0.022)(R 0.161, F -0.205)]  [G loss: 0.250] \n",
      "82 [D loss: (-0.031)(R 0.167, F -0.228)]  [G loss: 0.259] \n",
      "83 [D loss: (-0.025)(R 0.186, F -0.235)]  [G loss: 0.280] \n",
      "84 [D loss: (-0.032)(R 0.177, F -0.241)]  [G loss: 0.288] \n",
      "85 [D loss: (-0.030)(R 0.181, F -0.241)]  [G loss: 0.290] \n",
      "86 [D loss: (-0.037)(R 0.179, F -0.254)]  [G loss: 0.295] \n",
      "87 [D loss: (-0.048)(R 0.182, F -0.277)]  [G loss: 0.309] \n",
      "88 [D loss: (-0.054)(R 0.188, F -0.296)]  [G loss: 0.325] \n",
      "89 [D loss: (-0.060)(R 0.195, F -0.315)]  [G loss: 0.339] \n",
      "90 [D loss: (-0.063)(R 0.208, F -0.333)]  [G loss: 0.356] \n",
      "91 [D loss: (-0.063)(R 0.219, F -0.345)]  [G loss: 0.370] \n",
      "92 [D loss: (-0.055)(R 0.231, F -0.342)]  [G loss: 0.380] \n",
      "93 [D loss: (-0.057)(R 0.236, F -0.350)]  [G loss: 0.381] \n",
      "94 [D loss: (-0.049)(R 0.260, F -0.358)]  [G loss: 0.394] \n",
      "95 [D loss: (-0.048)(R 0.275, F -0.371)]  [G loss: 0.417] \n",
      "96 [D loss: (-0.057)(R 0.290, F -0.403)]  [G loss: 0.440] \n",
      "97 [D loss: (-0.052)(R 0.311, F -0.415)]  [G loss: 0.472] \n",
      "98 [D loss: (-0.058)(R 0.341, F -0.457)]  [G loss: 0.513] \n",
      "99 [D loss: (-0.069)(R 0.363, F -0.502)]  [G loss: 0.557] \n",
      "100 [D loss: (-0.081)(R 0.376, F -0.539)]  [G loss: 0.591] \n",
      "101 [D loss: (-0.087)(R 0.399, F -0.574)]  [G loss: 0.621] \n",
      "102 [D loss: (-0.078)(R 0.434, F -0.591)]  [G loss: 0.663] \n",
      "103 [D loss: (-0.092)(R 0.449, F -0.633)]  [G loss: 0.694] \n",
      "104 [D loss: (-0.088)(R 0.476, F -0.653)]  [G loss: 0.731] \n",
      "105 [D loss: (-0.094)(R 0.489, F -0.677)]  [G loss: 0.746] \n",
      "106 [D loss: (-0.081)(R 0.531, F -0.693)]  [G loss: 0.783] \n",
      "107 [D loss: (-0.087)(R 0.551, F -0.726)]  [G loss: 0.803] \n",
      "108 [D loss: (-0.075)(R 0.588, F -0.738)]  [G loss: 0.827] \n",
      "109 [D loss: (-0.076)(R 0.627, F -0.779)]  [G loss: 0.849] \n",
      "110 [D loss: (-0.055)(R 0.666, F -0.777)]  [G loss: 0.853] \n",
      "111 [D loss: (-0.067)(R 0.661, F -0.796)]  [G loss: 0.886] \n",
      "112 [D loss: (-0.072)(R 0.681, F -0.825)]  [G loss: 0.890] \n",
      "113 [D loss: (-0.080)(R 0.688, F -0.847)]  [G loss: 0.935] \n",
      "114 [D loss: (-0.039)(R 0.745, F -0.824)]  [G loss: 0.916] \n",
      "115 [D loss: (-0.047)(R 0.753, F -0.847)]  [G loss: 0.911] \n",
      "116 [D loss: (-0.044)(R 0.740, F -0.828)]  [G loss: 0.915] \n",
      "117 [D loss: (-0.029)(R 0.752, F -0.811)]  [G loss: 0.910] \n",
      "118 [D loss: (-0.036)(R 0.750, F -0.822)]  [G loss: 0.910] \n",
      "119 [D loss: (-0.034)(R 0.745, F -0.813)]  [G loss: 0.896] \n",
      "120 [D loss: (-0.021)(R 0.755, F -0.798)]  [G loss: 0.875] \n",
      "121 [D loss: (-0.005)(R 0.770, F -0.780)]  [G loss: 0.871] \n",
      "122 [D loss: (-0.015)(R 0.735, F -0.766)]  [G loss: 0.869] \n",
      "123 [D loss: (-0.020)(R 0.725, F -0.765)]  [G loss: 0.852] \n",
      "124 [D loss: (-0.020)(R 0.717, F -0.757)]  [G loss: 0.837] \n",
      "125 [D loss: (-0.023)(R 0.705, F -0.752)]  [G loss: 0.825] \n",
      "126 [D loss: (0.011)(R 0.726, F -0.705)]  [G loss: 0.798] \n",
      "127 [D loss: (0.008)(R 0.720, F -0.705)]  [G loss: 0.773] \n",
      "128 [D loss: (0.000)(R 0.665, F -0.664)]  [G loss: 0.750] \n",
      "129 [D loss: (0.007)(R 0.656, F -0.642)]  [G loss: 0.719] \n",
      "130 [D loss: (0.011)(R 0.651, F -0.629)]  [G loss: 0.704] \n",
      "131 [D loss: (-0.014)(R 0.587, F -0.614)]  [G loss: 0.686] \n",
      "132 [D loss: (0.002)(R 0.587, F -0.582)]  [G loss: 0.661] \n",
      "133 [D loss: (-0.010)(R 0.568, F -0.587)]  [G loss: 0.649] \n",
      "134 [D loss: (-0.006)(R 0.546, F -0.557)]  [G loss: 0.626] \n",
      "135 [D loss: (-0.006)(R 0.532, F -0.544)]  [G loss: 0.607] \n",
      "136 [D loss: (-0.014)(R 0.489, F -0.517)]  [G loss: 0.584] \n",
      "137 [D loss: (-0.018)(R 0.451, F -0.487)]  [G loss: 0.549] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138 [D loss: (-0.008)(R 0.448, F -0.464)]  [G loss: 0.522] \n",
      "139 [D loss: (-0.012)(R 0.418, F -0.443)]  [G loss: 0.500] \n",
      "140 [D loss: (-0.006)(R 0.410, F -0.422)]  [G loss: 0.486] \n",
      "141 [D loss: (-0.015)(R 0.376, F -0.407)]  [G loss: 0.468] \n",
      "142 [D loss: (-0.017)(R 0.368, F -0.401)]  [G loss: 0.449] \n",
      "143 [D loss: (-0.020)(R 0.347, F -0.386)]  [G loss: 0.436] \n",
      "144 [D loss: (-0.022)(R 0.323, F -0.367)]  [G loss: 0.420] \n",
      "145 [D loss: (-0.007)(R 0.320, F -0.334)]  [G loss: 0.396] \n",
      "146 [D loss: (-0.008)(R 0.309, F -0.324)]  [G loss: 0.381] \n",
      "147 [D loss: (-0.022)(R 0.279, F -0.324)]  [G loss: 0.373] \n",
      "148 [D loss: (-0.014)(R 0.272, F -0.300)]  [G loss: 0.354] \n",
      "149 [D loss: (-0.024)(R 0.251, F -0.299)]  [G loss: 0.347] \n",
      "150 [D loss: (-0.014)(R 0.255, F -0.283)]  [G loss: 0.329] \n",
      "151 [D loss: (-0.018)(R 0.224, F -0.261)]  [G loss: 0.315] \n",
      "152 [D loss: (-0.010)(R 0.235, F -0.255)]  [G loss: 0.312] \n",
      "153 [D loss: (-0.023)(R 0.216, F -0.262)]  [G loss: 0.304] \n",
      "154 [D loss: (-0.021)(R 0.216, F -0.258)]  [G loss: 0.295] \n",
      "155 [D loss: (-0.004)(R 0.240, F -0.247)]  [G loss: 0.293] \n",
      "156 [D loss: (-0.036)(R 0.211, F -0.282)]  [G loss: 0.296] \n",
      "157 [D loss: (-0.010)(R 0.211, F -0.231)]  [G loss: 0.290] \n",
      "158 [D loss: (-0.020)(R 0.202, F -0.243)]  [G loss: 0.280] \n",
      "159 [D loss: (-0.029)(R 0.193, F -0.250)]  [G loss: 0.278] \n",
      "160 [D loss: (-0.014)(R 0.218, F -0.246)]  [G loss: 0.285] \n",
      "161 [D loss: (-0.049)(R 0.178, F -0.275)]  [G loss: 0.265] \n",
      "162 [D loss: (-0.014)(R 0.216, F -0.244)]  [G loss: 0.282] \n",
      "163 [D loss: (-0.026)(R 0.192, F -0.245)]  [G loss: 0.262] \n",
      "164 [D loss: (-0.009)(R 0.204, F -0.221)]  [G loss: 0.261] \n",
      "165 [D loss: (-0.025)(R 0.190, F -0.240)]  [G loss: 0.237] \n",
      "166 [D loss: (-0.009)(R 0.172, F -0.190)]  [G loss: 0.233] \n",
      "167 [D loss: (-0.011)(R 0.151, F -0.173)]  [G loss: 0.220] \n",
      "168 [D loss: (-0.008)(R 0.157, F -0.174)]  [G loss: 0.212] \n",
      "169 [D loss: (-0.015)(R 0.145, F -0.175)]  [G loss: 0.207] \n",
      "170 [D loss: (-0.012)(R 0.130, F -0.154)]  [G loss: 0.197] \n",
      "171 [D loss: (-0.017)(R 0.115, F -0.148)]  [G loss: 0.188] \n",
      "172 [D loss: (-0.016)(R 0.110, F -0.142)]  [G loss: 0.177] \n",
      "173 [D loss: (-0.013)(R 0.105, F -0.131)]  [G loss: 0.170] \n",
      "174 [D loss: (-0.016)(R 0.105, F -0.137)]  [G loss: 0.162] \n",
      "175 [D loss: (-0.013)(R 0.104, F -0.129)]  [G loss: 0.163] \n",
      "176 [D loss: (-0.014)(R 0.111, F -0.140)]  [G loss: 0.159] \n",
      "177 [D loss: (-0.017)(R 0.108, F -0.142)]  [G loss: 0.175] \n",
      "178 [D loss: (-0.014)(R 0.097, F -0.124)]  [G loss: 0.155] \n",
      "179 [D loss: (-0.009)(R 0.105, F -0.124)]  [G loss: 0.158] \n",
      "180 [D loss: (-0.014)(R 0.103, F -0.131)]  [G loss: 0.162] \n",
      "181 [D loss: (-0.015)(R 0.102, F -0.132)]  [G loss: 0.167] \n",
      "182 [D loss: (-0.017)(R 0.098, F -0.132)]  [G loss: 0.167] \n",
      "183 [D loss: (-0.019)(R 0.101, F -0.139)]  [G loss: 0.170] \n",
      "184 [D loss: (-0.019)(R 0.103, F -0.142)]  [G loss: 0.170] \n",
      "185 [D loss: (-0.020)(R 0.100, F -0.139)]  [G loss: 0.168] \n",
      "186 [D loss: (-0.025)(R 0.091, F -0.140)]  [G loss: 0.167] \n",
      "187 [D loss: (-0.023)(R 0.093, F -0.139)]  [G loss: 0.175] \n",
      "188 [D loss: (-0.028)(R 0.098, F -0.153)]  [G loss: 0.174] \n",
      "189 [D loss: (-0.023)(R 0.099, F -0.145)]  [G loss: 0.178] \n",
      "190 [D loss: (-0.029)(R 0.086, F -0.144)]  [G loss: 0.175] \n",
      "191 [D loss: (-0.032)(R 0.085, F -0.148)]  [G loss: 0.182] \n",
      "192 [D loss: (-0.037)(R 0.081, F -0.155)]  [G loss: 0.182] \n",
      "193 [D loss: (-0.035)(R 0.083, F -0.154)]  [G loss: 0.190] \n",
      "194 [D loss: (-0.043)(R 0.075, F -0.162)]  [G loss: 0.193] \n",
      "195 [D loss: (-0.046)(R 0.102, F -0.195)]  [G loss: 0.211] \n",
      "196 [D loss: (-0.034)(R 0.096, F -0.165)]  [G loss: 0.202] \n",
      "197 [D loss: (-0.038)(R 0.094, F -0.170)]  [G loss: 0.202] \n",
      "198 [D loss: (-0.039)(R 0.088, F -0.165)]  [G loss: 0.202] \n",
      "199 [D loss: (-0.042)(R 0.092, F -0.176)]  [G loss: 0.206] \n",
      "200 [D loss: (-0.050)(R 0.093, F -0.194)]  [G loss: 0.215] \n",
      "201 [D loss: (-0.051)(R 0.096, F -0.198)]  [G loss: 0.238] \n",
      "202 [D loss: (-0.057)(R 0.096, F -0.211)]  [G loss: 0.251] \n",
      "203 [D loss: (-0.066)(R 0.093, F -0.226)]  [G loss: 0.255] \n",
      "204 [D loss: (-0.066)(R 0.100, F -0.232)]  [G loss: 0.264] \n",
      "205 [D loss: (-0.059)(R 0.114, F -0.232)]  [G loss: 0.272] \n",
      "206 [D loss: (-0.062)(R 0.111, F -0.234)]  [G loss: 0.285] \n",
      "207 [D loss: (-0.065)(R 0.135, F -0.265)]  [G loss: 0.293] \n",
      "208 [D loss: (-0.076)(R 0.103, F -0.255)]  [G loss: 0.305] \n",
      "209 [D loss: (-0.088)(R 0.124, F -0.300)]  [G loss: 0.284] \n",
      "210 [D loss: (-0.051)(R 0.137, F -0.239)]  [G loss: 0.282] \n",
      "211 [D loss: (-0.073)(R 0.116, F -0.262)]  [G loss: 0.298] \n",
      "212 [D loss: (-0.071)(R 0.115, F -0.257)]  [G loss: 0.311] \n",
      "213 [D loss: (-0.063)(R 0.133, F -0.258)]  [G loss: 0.322] \n",
      "214 [D loss: (-0.072)(R 0.127, F -0.270)]  [G loss: 0.331] \n",
      "215 [D loss: (-0.065)(R 0.132, F -0.261)]  [G loss: 0.328] \n",
      "216 [D loss: (-0.084)(R 0.131, F -0.298)]  [G loss: 0.310] \n",
      "217 [D loss: (-0.068)(R 0.141, F -0.277)]  [G loss: 0.354] \n",
      "218 [D loss: (-0.070)(R 0.150, F -0.290)]  [G loss: 0.347] \n",
      "219 [D loss: (-0.081)(R 0.129, F -0.291)]  [G loss: 0.355] \n",
      "220 [D loss: (-0.085)(R 0.142, F -0.312)]  [G loss: 0.362] \n",
      "221 [D loss: (-0.076)(R 0.161, F -0.312)]  [G loss: 0.372] \n",
      "222 [D loss: (-0.058)(R 0.156, F -0.272)]  [G loss: 0.294] \n",
      "223 [D loss: (-0.059)(R 0.131, F -0.248)]  [G loss: 0.296] \n",
      "224 [D loss: (-0.054)(R 0.103, F -0.210)]  [G loss: 0.265] \n",
      "225 [D loss: (-0.057)(R 0.093, F -0.207)]  [G loss: 0.278] \n",
      "226 [D loss: (-0.057)(R 0.092, F -0.206)]  [G loss: 0.285] \n",
      "227 [D loss: (-0.037)(R 0.110, F -0.183)]  [G loss: 0.280] \n",
      "228 [D loss: (-0.049)(R 0.096, F -0.194)]  [G loss: 0.269] \n",
      "229 [D loss: (-0.049)(R 0.081, F -0.178)]  [G loss: 0.270] \n",
      "230 [D loss: (-0.051)(R 0.087, F -0.188)]  [G loss: 0.262] \n",
      "231 [D loss: (-0.060)(R 0.102, F -0.222)]  [G loss: 0.297] \n",
      "232 [D loss: (-0.071)(R 0.125, F -0.267)]  [G loss: 0.331] \n",
      "233 [D loss: (-0.068)(R 0.148, F -0.284)]  [G loss: 0.372] \n",
      "234 [D loss: (-0.084)(R 0.157, F -0.325)]  [G loss: 0.407] \n",
      "235 [D loss: (-0.103)(R 0.143, F -0.349)]  [G loss: 0.439] \n",
      "236 [D loss: (-0.079)(R 0.186, F -0.345)]  [G loss: 0.435] \n",
      "237 [D loss: (-0.076)(R 0.207, F -0.359)]  [G loss: 0.407] \n",
      "238 [D loss: (-0.055)(R 0.232, F -0.343)]  [G loss: 0.425] \n",
      "239 [D loss: (-0.077)(R 0.211, F -0.365)]  [G loss: 0.448] \n",
      "240 [D loss: (-0.068)(R 0.228, F -0.364)]  [G loss: 0.462] \n",
      "241 [D loss: (-0.074)(R 0.232, F -0.381)]  [G loss: 0.485] \n",
      "242 [D loss: (-0.094)(R 0.219, F -0.407)]  [G loss: 0.477] \n",
      "243 [D loss: (-0.085)(R 0.227, F -0.398)]  [G loss: 0.464] \n",
      "244 [D loss: (-0.078)(R 0.256, F -0.412)]  [G loss: 0.448] \n",
      "245 [D loss: (-0.088)(R 0.228, F -0.405)]  [G loss: 0.458] \n",
      "246 [D loss: (-0.090)(R 0.230, F -0.410)]  [G loss: 0.509] \n",
      "247 [D loss: (-0.129)(R 0.218, F -0.476)]  [G loss: 0.586] \n",
      "248 [D loss: (-0.126)(R 0.231, F -0.483)]  [G loss: 0.637] \n",
      "249 [D loss: (-0.146)(R 0.258, F -0.549)]  [G loss: 0.663] \n",
      "250 [D loss: (-0.128)(R 0.260, F -0.516)]  [G loss: 0.611] \n",
      "251 [D loss: (-0.078)(R 0.307, F -0.462)]  [G loss: 0.535] \n",
      "252 [D loss: (-0.106)(R 0.270, F -0.483)]  [G loss: 0.540] \n",
      "253 [D loss: (-0.065)(R 0.372, F -0.502)]  [G loss: 0.593] \n",
      "254 [D loss: (-0.093)(R 0.318, F -0.505)]  [G loss: 0.596] \n",
      "255 [D loss: (-0.112)(R 0.333, F -0.557)]  [G loss: 0.643] \n",
      "256 [D loss: (-0.119)(R 0.338, F -0.575)]  [G loss: 0.713] \n",
      "257 [D loss: (-0.149)(R 0.347, F -0.645)]  [G loss: 0.726] \n",
      "258 [D loss: (-0.134)(R 0.392, F -0.660)]  [G loss: 0.751] \n",
      "259 [D loss: (-0.151)(R 0.357, F -0.658)]  [G loss: 0.738] \n",
      "260 [D loss: (-0.102)(R 0.391, F -0.594)]  [G loss: 0.729] \n",
      "261 [D loss: (-0.135)(R 0.388, F -0.658)]  [G loss: 0.758] \n",
      "262 [D loss: (-0.145)(R 0.384, F -0.674)]  [G loss: 0.806] \n",
      "263 [D loss: (-0.187)(R 0.328, F -0.703)]  [G loss: 0.836] \n",
      "264 [D loss: (-0.185)(R 0.333, F -0.702)]  [G loss: 0.821] \n",
      "265 [D loss: (-0.161)(R 0.330, F -0.653)]  [G loss: 0.745] \n",
      "266 [D loss: (-0.160)(R 0.329, F -0.649)]  [G loss: 0.734] \n",
      "267 [D loss: (-0.179)(R 0.295, F -0.652)]  [G loss: 0.763] \n",
      "268 [D loss: (-0.189)(R 0.301, F -0.679)]  [G loss: 0.842] \n",
      "269 [D loss: (-0.188)(R 0.345, F -0.722)]  [G loss: 0.874] \n",
      "270 [D loss: (-0.199)(R 0.342, F -0.740)]  [G loss: 0.897] \n",
      "271 [D loss: (-0.216)(R 0.354, F -0.786)]  [G loss: 0.914] \n",
      "272 [D loss: (-0.205)(R 0.362, F -0.772)]  [G loss: 0.889] \n",
      "273 [D loss: (-0.194)(R 0.353, F -0.741)]  [G loss: 0.849] \n",
      "274 [D loss: (-0.220)(R 0.318, F -0.758)]  [G loss: 0.854] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275 [D loss: (-0.225)(R 0.328, F -0.777)]  [G loss: 0.921] \n",
      "276 [D loss: (-0.253)(R 0.344, F -0.850)]  [G loss: 0.974] \n",
      "277 [D loss: (-0.241)(R 0.401, F -0.883)]  [G loss: 1.070] \n",
      "278 [D loss: (-0.252)(R 0.386, F -0.889)]  [G loss: 1.039] \n",
      "279 [D loss: (-0.208)(R 0.458, F -0.875)]  [G loss: 0.984] \n",
      "280 [D loss: (-0.170)(R 0.472, F -0.811)]  [G loss: 0.909] \n",
      "281 [D loss: (-0.177)(R 0.440, F -0.793)]  [G loss: 0.874] \n",
      "282 [D loss: (-0.179)(R 0.415, F -0.774)]  [G loss: 0.885] \n",
      "283 [D loss: (-0.209)(R 0.338, F -0.756)]  [G loss: 0.896] \n",
      "284 [D loss: (-0.210)(R 0.379, F -0.800)]  [G loss: 0.933] \n",
      "285 [D loss: (-0.171)(R 0.348, F -0.690)]  [G loss: 0.839] \n",
      "286 [D loss: (-0.153)(R 0.351, F -0.656)]  [G loss: 0.763] \n",
      "287 [D loss: (-0.099)(R 0.416, F -0.615)]  [G loss: 0.677] \n",
      "288 [D loss: (-0.146)(R 0.341, F -0.634)]  [G loss: 0.657] \n",
      "289 [D loss: (-0.093)(R 0.398, F -0.584)]  [G loss: 0.654] \n",
      "290 [D loss: (-0.142)(R 0.385, F -0.669)]  [G loss: 0.735] \n",
      "291 [D loss: (-0.105)(R 0.435, F -0.645)]  [G loss: 0.762] \n",
      "292 [D loss: (-0.105)(R 0.473, F -0.683)]  [G loss: 0.804] \n",
      "293 [D loss: (-0.116)(R 0.502, F -0.735)]  [G loss: 0.909] \n",
      "294 [D loss: (-0.143)(R 0.519, F -0.805)]  [G loss: 0.969] \n",
      "295 [D loss: (-0.123)(R 0.584, F -0.830)]  [G loss: 0.968] \n",
      "296 [D loss: (-0.098)(R 0.647, F -0.843)]  [G loss: 0.909] \n",
      "297 [D loss: (-0.145)(R 0.570, F -0.859)]  [G loss: 0.925] \n",
      "298 [D loss: (-0.128)(R 0.603, F -0.858)]  [G loss: 0.975] \n",
      "299 [D loss: (-0.157)(R 0.573, F -0.887)]  [G loss: 1.043] \n",
      "300 [D loss: (-0.162)(R 0.554, F -0.877)]  [G loss: 1.053] \n",
      "301 [D loss: (-0.184)(R 0.524, F -0.893)]  [G loss: 1.043] \n",
      "302 [D loss: (-0.194)(R 0.515, F -0.903)]  [G loss: 1.036] \n",
      "303 [D loss: (-0.186)(R 0.503, F -0.876)]  [G loss: 0.986] \n",
      "304 [D loss: (-0.145)(R 0.560, F -0.850)]  [G loss: 0.965] \n",
      "305 [D loss: (-0.142)(R 0.583, F -0.866)]  [G loss: 0.982] \n",
      "306 [D loss: (-0.177)(R 0.566, F -0.921)]  [G loss: 1.058] \n",
      "307 [D loss: (-0.218)(R 0.516, F -0.952)]  [G loss: 1.169] \n",
      "308 [D loss: (-0.215)(R 0.571, F -1.001)]  [G loss: 1.236] \n",
      "309 [D loss: (-0.212)(R 0.638, F -1.062)]  [G loss: 1.317] \n",
      "310 [D loss: (-0.202)(R 0.662, F -1.065)]  [G loss: 1.309] \n",
      "311 [D loss: (-0.190)(R 0.686, F -1.066)]  [G loss: 1.219] \n",
      "312 [D loss: (-0.152)(R 0.704, F -1.008)]  [G loss: 1.075] \n",
      "313 [D loss: (-0.132)(R 0.662, F -0.926)]  [G loss: 0.976] \n",
      "314 [D loss: (-0.115)(R 0.621, F -0.851)]  [G loss: 0.992] \n",
      "315 [D loss: (-0.165)(R 0.544, F -0.874)]  [G loss: 1.018] \n",
      "316 [D loss: (-0.210)(R 0.486, F -0.906)]  [G loss: 1.051] \n",
      "317 [D loss: (-0.191)(R 0.452, F -0.835)]  [G loss: 1.026] \n",
      "318 [D loss: (-0.164)(R 0.448, F -0.777)]  [G loss: 0.917] \n",
      "319 [D loss: (-0.161)(R 0.413, F -0.734)]  [G loss: 0.811] \n",
      "320 [D loss: (-0.139)(R 0.424, F -0.702)]  [G loss: 0.718] \n",
      "321 [D loss: (-0.135)(R 0.406, F -0.676)]  [G loss: 0.726] \n",
      "322 [D loss: (-0.116)(R 0.395, F -0.627)]  [G loss: 0.704] \n",
      "323 [D loss: (-0.100)(R 0.450, F -0.650)]  [G loss: 0.801] \n",
      "324 [D loss: (-0.106)(R 0.490, F -0.702)]  [G loss: 0.883] \n",
      "325 [D loss: (-0.165)(R 0.455, F -0.785)]  [G loss: 0.965] \n",
      "326 [D loss: (-0.191)(R 0.503, F -0.885)]  [G loss: 1.089] \n",
      "327 [D loss: (-0.123)(R 0.661, F -0.907)]  [G loss: 1.057] \n",
      "328 [D loss: (-0.116)(R 0.630, F -0.861)]  [G loss: 0.959] \n",
      "329 [D loss: (-0.097)(R 0.591, F -0.785)]  [G loss: 0.814] \n",
      "330 [D loss: (-0.061)(R 0.571, F -0.692)]  [G loss: 0.700] \n",
      "331 [D loss: (-0.064)(R 0.519, F -0.647)]  [G loss: 0.786] \n",
      "332 [D loss: (-0.095)(R 0.423, F -0.612)]  [G loss: 0.747] \n",
      "333 [D loss: (-0.095)(R 0.433, F -0.622)]  [G loss: 0.802] \n",
      "334 [D loss: (-0.121)(R 0.390, F -0.631)]  [G loss: 0.792] \n",
      "335 [D loss: (-0.123)(R 0.395, F -0.642)]  [G loss: 0.789] \n",
      "336 [D loss: (-0.127)(R 0.404, F -0.658)]  [G loss: 0.828] \n",
      "337 [D loss: (-0.142)(R 0.452, F -0.736)]  [G loss: 0.886] \n",
      "338 [D loss: (-0.156)(R 0.455, F -0.768)]  [G loss: 0.969] \n",
      "339 [D loss: (-0.158)(R 0.537, F -0.853)]  [G loss: 1.079] \n",
      "340 [D loss: (-0.207)(R 0.594, F -1.008)]  [G loss: 1.234] \n",
      "341 [D loss: (-0.232)(R 0.659, F -1.122)]  [G loss: 1.376] \n",
      "342 [D loss: (-0.271)(R 0.727, F -1.269)]  [G loss: 1.527] \n",
      "343 [D loss: (-0.265)(R 0.767, F -1.298)]  [G loss: 1.561] \n",
      "344 [D loss: (-0.238)(R 0.927, F -1.403)]  [G loss: 1.612] \n",
      "345 [D loss: (-0.185)(R 0.962, F -1.331)]  [G loss: 1.557] \n",
      "346 [D loss: (-0.222)(R 0.919, F -1.363)]  [G loss: 1.565] \n",
      "347 [D loss: (-0.291)(R 0.914, F -1.497)]  [G loss: 1.719] \n",
      "348 [D loss: (-0.258)(R 0.933, F -1.448)]  [G loss: 1.685] \n",
      "349 [D loss: (-0.276)(R 0.907, F -1.459)]  [G loss: 1.624] \n",
      "350 [D loss: (-0.244)(R 0.982, F -1.470)]  [G loss: 1.590] \n",
      "351 [D loss: (-0.209)(R 0.909, F -1.328)]  [G loss: 1.426] \n",
      "352 [D loss: (-0.151)(R 0.972, F -1.274)]  [G loss: 1.394] \n",
      "353 [D loss: (-0.123)(R 0.878, F -1.123)]  [G loss: 1.235] \n",
      "354 [D loss: (-0.141)(R 0.828, F -1.109)]  [G loss: 1.246] \n",
      "355 [D loss: (-0.170)(R 0.800, F -1.141)]  [G loss: 1.312] \n",
      "356 [D loss: (-0.159)(R 0.803, F -1.122)]  [G loss: 1.384] \n",
      "357 [D loss: (-0.183)(R 0.791, F -1.157)]  [G loss: 1.385] \n",
      "358 [D loss: (-0.183)(R 0.762, F -1.127)]  [G loss: 1.324] \n",
      "359 [D loss: (-0.160)(R 0.730, F -1.049)]  [G loss: 1.158] \n",
      "360 [D loss: (-0.050)(R 0.827, F -0.927)]  [G loss: 0.993] \n",
      "361 [D loss: (-0.074)(R 0.734, F -0.883)]  [G loss: 0.967] \n",
      "362 [D loss: (-0.075)(R 0.733, F -0.883)]  [G loss: 0.958] \n",
      "363 [D loss: (-0.096)(R 0.674, F -0.866)]  [G loss: 1.021] \n",
      "364 [D loss: (-0.140)(R 0.611, F -0.891)]  [G loss: 1.062] \n",
      "365 [D loss: (-0.157)(R 0.593, F -0.907)]  [G loss: 1.120] \n",
      "366 [D loss: (-0.199)(R 0.515, F -0.912)]  [G loss: 1.088] \n",
      "367 [D loss: (-0.196)(R 0.505, F -0.897)]  [G loss: 1.096] \n",
      "368 [D loss: (-0.194)(R 0.482, F -0.870)]  [G loss: 1.036] \n",
      "369 [D loss: (-0.144)(R 0.582, F -0.869)]  [G loss: 0.993] \n",
      "370 [D loss: (-0.207)(R 0.510, F -0.923)]  [G loss: 1.056] \n",
      "371 [D loss: (-0.222)(R 0.510, F -0.954)]  [G loss: 1.149] \n",
      "372 [D loss: (-0.229)(R 0.553, F -1.010)]  [G loss: 1.219] \n",
      "373 [D loss: (-0.283)(R 0.583, F -1.148)]  [G loss: 1.394] \n",
      "374 [D loss: (-0.288)(R 0.679, F -1.256)]  [G loss: 1.474] \n",
      "375 [D loss: (-0.329)(R 0.726, F -1.384)]  [G loss: 1.640] \n",
      "376 [D loss: (-0.279)(R 0.851, F -1.408)]  [G loss: 1.626] \n",
      "377 [D loss: (-0.281)(R 0.817, F -1.380)]  [G loss: 1.575] \n",
      "378 [D loss: (-0.220)(R 0.915, F -1.355)]  [G loss: 1.452] \n",
      "379 [D loss: (-0.191)(R 0.918, F -1.300)]  [G loss: 1.359] \n",
      "380 [D loss: (-0.188)(R 0.892, F -1.269)]  [G loss: 1.394] \n",
      "381 [D loss: (-0.253)(R 0.831, F -1.338)]  [G loss: 1.387] \n",
      "382 [D loss: (-0.185)(R 0.873, F -1.242)]  [G loss: 1.498] \n",
      "383 [D loss: (-0.216)(R 0.777, F -1.209)]  [G loss: 1.439] \n",
      "384 [D loss: (-0.172)(R 0.777, F -1.121)]  [G loss: 1.347] \n",
      "385 [D loss: (-0.175)(R 0.743, F -1.093)]  [G loss: 1.358] \n",
      "386 [D loss: (-0.204)(R 0.727, F -1.135)]  [G loss: 1.238] \n",
      "387 [D loss: (-0.152)(R 0.796, F -1.100)]  [G loss: 1.261] \n",
      "388 [D loss: (-0.151)(R 0.837, F -1.139)]  [G loss: 1.244] \n",
      "389 [D loss: (-0.147)(R 0.759, F -1.053)]  [G loss: 1.234] \n",
      "390 [D loss: (-0.118)(R 0.801, F -1.038)]  [G loss: 1.303] \n",
      "391 [D loss: (-0.161)(R 0.801, F -1.123)]  [G loss: 1.377] \n",
      "392 [D loss: (-0.171)(R 0.935, F -1.277)]  [G loss: 1.547] \n",
      "393 [D loss: (-0.216)(R 0.920, F -1.351)]  [G loss: 1.643] \n",
      "394 [D loss: (-0.260)(R 0.924, F -1.444)]  [G loss: 1.713] \n",
      "395 [D loss: (-0.173)(R 1.028, F -1.374)]  [G loss: 1.701] \n",
      "396 [D loss: (-0.172)(R 1.029, F -1.374)]  [G loss: 1.584] \n",
      "397 [D loss: (-0.168)(R 0.994, F -1.329)]  [G loss: 1.514] \n",
      "398 [D loss: (-0.211)(R 0.892, F -1.314)]  [G loss: 1.536] \n",
      "399 [D loss: (-0.201)(R 0.874, F -1.277)]  [G loss: 1.545] \n",
      "400 [D loss: (-0.224)(R 0.866, F -1.315)]  [G loss: 1.632] \n",
      "401 [D loss: (-0.280)(R 0.802, F -1.362)]  [G loss: 1.660] \n",
      "402 [D loss: (-0.283)(R 0.867, F -1.434)]  [G loss: 1.772] \n",
      "403 [D loss: (-0.257)(R 0.888, F -1.403)]  [G loss: 1.671] \n",
      "404 [D loss: (-0.326)(R 0.812, F -1.465)]  [G loss: 1.690] \n",
      "405 [D loss: (-0.283)(R 0.895, F -1.460)]  [G loss: 1.732] \n",
      "406 [D loss: (-0.344)(R 0.865, F -1.554)]  [G loss: 1.762] \n",
      "407 [D loss: (-0.278)(R 0.958, F -1.515)]  [G loss: 1.754] \n",
      "408 [D loss: (-0.291)(R 0.962, F -1.543)]  [G loss: 1.790] \n",
      "409 [D loss: (-0.247)(R 1.070, F -1.565)]  [G loss: 1.803] \n",
      "410 [D loss: (-0.208)(R 1.126, F -1.542)]  [G loss: 1.821] \n",
      "411 [D loss: (-0.188)(R 1.136, F -1.511)]  [G loss: 1.731] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "412 [D loss: (-0.158)(R 1.156, F -1.471)]  [G loss: 1.750] \n",
      "413 [D loss: (-0.105)(R 1.268, F -1.478)]  [G loss: 1.687] \n",
      "414 [D loss: (-0.129)(R 1.179, F -1.437)]  [G loss: 1.673] \n",
      "415 [D loss: (-0.182)(R 1.056, F -1.420)]  [G loss: 1.581] \n",
      "416 [D loss: (-0.144)(R 0.986, F -1.273)]  [G loss: 1.650] \n",
      "417 [D loss: (-0.167)(R 0.842, F -1.176)]  [G loss: 1.424] \n",
      "418 [D loss: (-0.130)(R 0.961, F -1.222)]  [G loss: 1.538] \n",
      "419 [D loss: (-0.101)(R 0.973, F -1.175)]  [G loss: 1.440] \n",
      "420 [D loss: (-0.060)(R 0.981, F -1.101)]  [G loss: 1.470] \n",
      "421 [D loss: (-0.141)(R 1.033, F -1.314)]  [G loss: 1.551] \n",
      "422 [D loss: (-0.138)(R 1.080, F -1.357)]  [G loss: 1.717] \n",
      "423 [D loss: (-0.205)(R 1.035, F -1.446)]  [G loss: 1.801] \n",
      "424 [D loss: (-0.189)(R 1.197, F -1.574)]  [G loss: 1.897] \n",
      "425 [D loss: (-0.174)(R 1.264, F -1.613)]  [G loss: 1.956] \n",
      "426 [D loss: (-0.257)(R 1.231, F -1.744)]  [G loss: 2.020] \n",
      "427 [D loss: (-0.285)(R 1.254, F -1.825)]  [G loss: 2.205] \n",
      "428 [D loss: (-0.262)(R 1.261, F -1.785)]  [G loss: 2.132] \n",
      "429 [D loss: (-0.276)(R 1.331, F -1.884)]  [G loss: 2.175] \n",
      "430 [D loss: (-0.271)(R 1.370, F -1.912)]  [G loss: 2.231] \n",
      "431 [D loss: (-0.260)(R 1.392, F -1.912)]  [G loss: 2.240] \n",
      "432 [D loss: (-0.236)(R 1.457, F -1.929)]  [G loss: 2.238] \n",
      "433 [D loss: (-0.325)(R 1.363, F -2.012)]  [G loss: 2.293] \n",
      "434 [D loss: (-0.276)(R 1.407, F -1.959)]  [G loss: 2.279] \n",
      "435 [D loss: (-0.251)(R 1.477, F -1.979)]  [G loss: 2.274] \n",
      "436 [D loss: (-0.248)(R 1.435, F -1.931)]  [G loss: 2.210] \n",
      "437 [D loss: (-0.218)(R 1.429, F -1.866)]  [G loss: 2.101] \n",
      "438 [D loss: (-0.203)(R 1.443, F -1.848)]  [G loss: 2.141] \n",
      "439 [D loss: (-0.228)(R 1.303, F -1.758)]  [G loss: 2.049] \n",
      "440 [D loss: (-0.167)(R 1.330, F -1.665)]  [G loss: 1.943] \n",
      "441 [D loss: (-0.124)(R 1.336, F -1.585)]  [G loss: 1.900] \n",
      "442 [D loss: (-0.147)(R 1.376, F -1.670)]  [G loss: 2.009] \n",
      "443 [D loss: (-0.160)(R 1.361, F -1.682)]  [G loss: 1.977] \n",
      "444 [D loss: (-0.138)(R 1.403, F -1.679)]  [G loss: 2.034] \n",
      "445 [D loss: (-0.129)(R 1.502, F -1.760)]  [G loss: 2.144] \n",
      "446 [D loss: (-0.118)(R 1.460, F -1.696)]  [G loss: 2.040] \n",
      "447 [D loss: (-0.169)(R 1.440, F -1.777)]  [G loss: 2.088] \n",
      "448 [D loss: (-0.172)(R 1.381, F -1.724)]  [G loss: 2.076] \n",
      "449 [D loss: (-0.187)(R 1.298, F -1.671)]  [G loss: 2.013] \n",
      "450 [D loss: (-0.235)(R 1.184, F -1.654)]  [G loss: 1.956] \n",
      "451 [D loss: (-0.195)(R 1.183, F -1.572)]  [G loss: 1.947] \n",
      "452 [D loss: (-0.247)(R 1.228, F -1.722)]  [G loss: 2.034] \n",
      "453 [D loss: (-0.307)(R 1.265, F -1.880)]  [G loss: 2.076] \n",
      "454 [D loss: (-0.284)(R 1.327, F -1.896)]  [G loss: 2.168] \n",
      "455 [D loss: (-0.240)(R 1.309, F -1.789)]  [G loss: 2.165] \n",
      "456 [D loss: (-0.197)(R 1.409, F -1.802)]  [G loss: 2.217] \n",
      "457 [D loss: (-0.263)(R 1.364, F -1.890)]  [G loss: 2.192] \n",
      "458 [D loss: (-0.287)(R 1.421, F -1.995)]  [G loss: 2.328] \n",
      "459 [D loss: (-0.209)(R 1.523, F -1.941)]  [G loss: 2.262] \n",
      "460 [D loss: (-0.167)(R 1.594, F -1.927)]  [G loss: 2.217] \n",
      "461 [D loss: (-0.244)(R 1.544, F -2.031)]  [G loss: 2.312] \n",
      "462 [D loss: (-0.163)(R 1.548, F -1.873)]  [G loss: 2.229] \n",
      "463 [D loss: (-0.129)(R 1.581, F -1.840)]  [G loss: 2.189] \n",
      "464 [D loss: (-0.195)(R 1.546, F -1.936)]  [G loss: 2.212] \n",
      "465 [D loss: (-0.109)(R 1.642, F -1.861)]  [G loss: 2.184] \n",
      "466 [D loss: (-0.106)(R 1.696, F -1.909)]  [G loss: 2.347] \n",
      "467 [D loss: (-0.196)(R 1.688, F -2.080)]  [G loss: 2.336] \n",
      "468 [D loss: (-0.221)(R 1.574, F -2.015)]  [G loss: 2.323] \n",
      "469 [D loss: (-0.199)(R 1.520, F -1.918)]  [G loss: 2.265] \n",
      "470 [D loss: (-0.123)(R 1.622, F -1.868)]  [G loss: 2.285] \n",
      "471 [D loss: (-0.103)(R 1.721, F -1.927)]  [G loss: 2.327] \n",
      "472 [D loss: (-0.187)(R 1.652, F -2.027)]  [G loss: 2.477] \n",
      "473 [D loss: (-0.211)(R 1.716, F -2.138)]  [G loss: 2.443] \n",
      "474 [D loss: (-0.160)(R 1.858, F -2.179)]  [G loss: 2.597] \n",
      "475 [D loss: (-0.160)(R 1.812, F -2.132)]  [G loss: 2.453] \n",
      "476 [D loss: (-0.166)(R 1.691, F -2.023)]  [G loss: 2.470] \n",
      "477 [D loss: (-0.218)(R 1.631, F -2.066)]  [G loss: 2.458] \n",
      "478 [D loss: (-0.199)(R 1.664, F -2.062)]  [G loss: 2.397] \n",
      "479 [D loss: (-0.189)(R 1.682, F -2.061)]  [G loss: 2.484] \n",
      "480 [D loss: (-0.174)(R 1.666, F -2.014)]  [G loss: 2.437] \n",
      "481 [D loss: (-0.188)(R 1.680, F -2.057)]  [G loss: 2.384] \n",
      "482 [D loss: (-0.204)(R 1.606, F -2.014)]  [G loss: 2.463] \n",
      "483 [D loss: (-0.163)(R 1.700, F -2.027)]  [G loss: 2.417] \n",
      "484 [D loss: (-0.256)(R 1.570, F -2.082)]  [G loss: 2.400] \n",
      "485 [D loss: (-0.067)(R 1.688, F -1.821)]  [G loss: 2.264] \n",
      "486 [D loss: (-0.201)(R 1.666, F -2.069)]  [G loss: 2.490] \n",
      "487 [D loss: (-0.068)(R 1.700, F -1.835)]  [G loss: 2.257] \n",
      "488 [D loss: (-0.090)(R 1.693, F -1.873)]  [G loss: 2.223] \n",
      "489 [D loss: (-0.092)(R 1.675, F -1.860)]  [G loss: 2.257] \n",
      "490 [D loss: (-0.089)(R 1.674, F -1.852)]  [G loss: 2.251] \n",
      "491 [D loss: (-0.137)(R 1.600, F -1.875)]  [G loss: 2.290] \n",
      "492 [D loss: (-0.112)(R 1.689, F -1.914)]  [G loss: 2.305] \n",
      "493 [D loss: (-0.236)(R 1.652, F -2.124)]  [G loss: 2.522] \n",
      "494 [D loss: (-0.140)(R 1.567, F -1.847)]  [G loss: 2.241] \n",
      "495 [D loss: (-0.142)(R 1.611, F -1.896)]  [G loss: 2.353] \n",
      "496 [D loss: (-0.220)(R 1.570, F -2.010)]  [G loss: 2.339] \n",
      "497 [D loss: (-0.150)(R 1.690, F -1.991)]  [G loss: 2.394] \n",
      "498 [D loss: (-0.254)(R 1.551, F -2.059)]  [G loss: 2.370] \n",
      "499 [D loss: (-0.201)(R 1.635, F -2.036)]  [G loss: 2.456] \n",
      "500 [D loss: (-0.210)(R 1.504, F -1.924)]  [G loss: 2.367] \n",
      "501 [D loss: (-0.239)(R 1.549, F -2.027)]  [G loss: 2.362] \n",
      "502 [D loss: (-0.185)(R 1.523, F -1.894)]  [G loss: 2.243] \n",
      "503 [D loss: (-0.184)(R 1.489, F -1.857)]  [G loss: 2.202] \n",
      "504 [D loss: (-0.217)(R 1.444, F -1.879)]  [G loss: 2.166] \n",
      "505 [D loss: (-0.180)(R 1.587, F -1.948)]  [G loss: 2.279] \n",
      "506 [D loss: (-0.180)(R 1.664, F -2.023)]  [G loss: 2.432] \n",
      "507 [D loss: (-0.208)(R 1.504, F -1.919)]  [G loss: 2.309] \n",
      "508 [D loss: (-0.173)(R 1.605, F -1.951)]  [G loss: 2.330] \n",
      "509 [D loss: (-0.148)(R 1.601, F -1.897)]  [G loss: 2.230] \n",
      "510 [D loss: (-0.037)(R 1.684, F -1.757)]  [G loss: 2.200] \n",
      "511 [D loss: (-0.127)(R 1.599, F -1.854)]  [G loss: 2.229] \n",
      "512 [D loss: (-0.208)(R 1.352, F -1.768)]  [G loss: 2.124] \n",
      "513 [D loss: (-0.199)(R 1.370, F -1.767)]  [G loss: 2.060] \n",
      "514 [D loss: (-0.182)(R 1.610, F -1.974)]  [G loss: 2.382] \n",
      "515 [D loss: (-0.065)(R 1.519, F -1.650)]  [G loss: 2.145] \n",
      "516 [D loss: (-0.162)(R 1.564, F -1.888)]  [G loss: 2.311] \n",
      "517 [D loss: (-0.186)(R 1.604, F -1.976)]  [G loss: 2.353] \n",
      "518 [D loss: (-0.170)(R 1.676, F -2.017)]  [G loss: 2.457] \n",
      "519 [D loss: (-0.187)(R 1.687, F -2.061)]  [G loss: 2.379] \n",
      "520 [D loss: (-0.179)(R 1.578, F -1.936)]  [G loss: 2.361] \n",
      "521 [D loss: (-0.128)(R 1.674, F -1.930)]  [G loss: 2.325] \n",
      "522 [D loss: (-0.249)(R 1.617, F -2.115)]  [G loss: 2.461] \n",
      "523 [D loss: (-0.111)(R 1.514, F -1.736)]  [G loss: 2.206] \n",
      "524 [D loss: (-0.192)(R 1.553, F -1.937)]  [G loss: 2.334] \n",
      "525 [D loss: (-0.247)(R 1.573, F -2.067)]  [G loss: 2.308] \n",
      "526 [D loss: (-0.090)(R 1.487, F -1.668)]  [G loss: 2.007] \n",
      "527 [D loss: (-0.385)(R 1.393, F -2.164)]  [G loss: 2.503] \n",
      "528 [D loss: (0.043)(R 1.639, F -1.553)]  [G loss: 1.954] \n",
      "529 [D loss: (-0.044)(R 1.375, F -1.463)]  [G loss: 1.959] \n",
      "530 [D loss: (-0.109)(R 1.374, F -1.591)]  [G loss: 2.072] \n",
      "531 [D loss: (-0.086)(R 1.430, F -1.603)]  [G loss: 1.980] \n",
      "532 [D loss: (-0.148)(R 1.433, F -1.728)]  [G loss: 2.139] \n",
      "533 [D loss: (-0.117)(R 1.464, F -1.698)]  [G loss: 2.124] \n",
      "534 [D loss: (-0.174)(R 1.454, F -1.803)]  [G loss: 2.176] \n",
      "535 [D loss: (-0.343)(R 1.461, F -2.148)]  [G loss: 2.249] \n",
      "536 [D loss: (0.039)(R 1.621, F -1.544)]  [G loss: 2.114] \n",
      "537 [D loss: (-0.204)(R 1.478, F -1.886)]  [G loss: 2.438] \n",
      "538 [D loss: (-0.121)(R 1.481, F -1.724)]  [G loss: 2.115] \n",
      "539 [D loss: (-0.136)(R 1.490, F -1.761)]  [G loss: 2.123] \n",
      "540 [D loss: (-0.268)(R 1.393, F -1.928)]  [G loss: 2.229] \n",
      "541 [D loss: (-0.142)(R 1.419, F -1.703)]  [G loss: 2.126] \n",
      "542 [D loss: (-0.230)(R 1.467, F -1.927)]  [G loss: 2.248] \n",
      "543 [D loss: (-0.135)(R 1.499, F -1.769)]  [G loss: 2.144] \n",
      "544 [D loss: (-0.123)(R 1.563, F -1.810)]  [G loss: 2.247] \n",
      "545 [D loss: (-0.120)(R 1.543, F -1.783)]  [G loss: 2.208] \n",
      "546 [D loss: (-0.156)(R 1.684, F -1.996)]  [G loss: 2.357] \n",
      "547 [D loss: (0.021)(R 1.777, F -1.735)]  [G loss: 2.216] \n",
      "548 [D loss: (-0.070)(R 1.656, F -1.795)]  [G loss: 2.243] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "549 [D loss: (-0.061)(R 1.639, F -1.762)]  [G loss: 2.173] \n",
      "550 [D loss: (-0.148)(R 1.558, F -1.855)]  [G loss: 2.207] \n",
      "551 [D loss: (-0.087)(R 1.530, F -1.703)]  [G loss: 2.110] \n",
      "552 [D loss: (-0.105)(R 1.509, F -1.718)]  [G loss: 2.130] \n",
      "553 [D loss: (-0.115)(R 1.485, F -1.715)]  [G loss: 1.982] \n",
      "554 [D loss: (-0.044)(R 1.522, F -1.610)]  [G loss: 2.077] \n",
      "555 [D loss: (-0.099)(R 1.447, F -1.645)]  [G loss: 2.016] \n",
      "556 [D loss: (-0.054)(R 1.540, F -1.649)]  [G loss: 2.111] \n",
      "557 [D loss: (-0.098)(R 1.579, F -1.774)]  [G loss: 2.106] \n",
      "558 [D loss: (-0.100)(R 1.627, F -1.826)]  [G loss: 2.318] \n",
      "559 [D loss: (-0.129)(R 1.538, F -1.795)]  [G loss: 2.175] \n",
      "560 [D loss: (-0.154)(R 1.617, F -1.924)]  [G loss: 2.316] \n",
      "561 [D loss: (-0.074)(R 1.551, F -1.700)]  [G loss: 2.117] \n",
      "562 [D loss: (-0.041)(R 1.519, F -1.601)]  [G loss: 2.057] \n",
      "563 [D loss: (-0.105)(R 1.411, F -1.621)]  [G loss: 1.996] \n",
      "564 [D loss: (-0.126)(R 1.491, F -1.743)]  [G loss: 2.176] \n",
      "565 [D loss: (-0.075)(R 1.535, F -1.686)]  [G loss: 2.071] \n",
      "566 [D loss: (-0.087)(R 1.601, F -1.774)]  [G loss: 2.175] \n",
      "567 [D loss: (-0.070)(R 1.612, F -1.752)]  [G loss: 2.199] \n",
      "568 [D loss: (-0.134)(R 1.591, F -1.859)]  [G loss: 2.156] \n",
      "569 [D loss: (-0.100)(R 1.772, F -1.972)]  [G loss: 2.440] \n",
      "570 [D loss: (-0.082)(R 1.720, F -1.883)]  [G loss: 2.208] \n",
      "571 [D loss: (-0.201)(R 1.555, F -1.956)]  [G loss: 2.278] \n",
      "572 [D loss: (0.014)(R 1.634, F -1.605)]  [G loss: 2.093] \n",
      "573 [D loss: (-0.080)(R 1.529, F -1.689)]  [G loss: 2.058] \n",
      "574 [D loss: (-0.055)(R 1.453, F -1.563)]  [G loss: 2.000] \n",
      "575 [D loss: (-0.061)(R 1.524, F -1.646)]  [G loss: 2.021] \n",
      "576 [D loss: (-0.143)(R 1.458, F -1.744)]  [G loss: 2.214] \n",
      "577 [D loss: (-0.082)(R 1.558, F -1.723)]  [G loss: 2.116] \n",
      "578 [D loss: (-0.078)(R 1.624, F -1.780)]  [G loss: 2.124] \n",
      "579 [D loss: (-0.073)(R 1.576, F -1.722)]  [G loss: 2.103] \n",
      "580 [D loss: (-0.053)(R 1.515, F -1.621)]  [G loss: 2.009] \n",
      "581 [D loss: (-0.152)(R 1.353, F -1.656)]  [G loss: 1.962] \n",
      "582 [D loss: (-0.070)(R 1.449, F -1.589)]  [G loss: 2.022] \n",
      "583 [D loss: (-0.143)(R 1.504, F -1.790)]  [G loss: 2.082] \n",
      "584 [D loss: (-0.183)(R 1.569, F -1.935)]  [G loss: 2.184] \n",
      "585 [D loss: (-0.076)(R 1.463, F -1.614)]  [G loss: 2.082] \n",
      "586 [D loss: (-0.100)(R 1.632, F -1.833)]  [G loss: 2.279] \n",
      "587 [D loss: (-0.120)(R 1.637, F -1.877)]  [G loss: 2.230] \n",
      "588 [D loss: (-0.098)(R 1.688, F -1.883)]  [G loss: 2.330] \n",
      "589 [D loss: (-0.157)(R 1.554, F -1.868)]  [G loss: 2.189] \n",
      "590 [D loss: (-0.115)(R 1.583, F -1.812)]  [G loss: 2.164] \n",
      "591 [D loss: (-0.237)(R 1.432, F -1.905)]  [G loss: 2.213] \n",
      "592 [D loss: (-0.166)(R 1.458, F -1.790)]  [G loss: 2.102] \n",
      "593 [D loss: (-0.153)(R 1.462, F -1.768)]  [G loss: 2.100] \n",
      "594 [D loss: (-0.097)(R 1.450, F -1.644)]  [G loss: 2.001] \n",
      "595 [D loss: (-0.170)(R 1.479, F -1.818)]  [G loss: 2.144] \n",
      "596 [D loss: (-0.116)(R 1.489, F -1.721)]  [G loss: 1.997] \n",
      "597 [D loss: (-0.115)(R 1.642, F -1.871)]  [G loss: 2.238] \n",
      "598 [D loss: (-0.105)(R 1.477, F -1.687)]  [G loss: 2.131] \n",
      "599 [D loss: (-0.101)(R 1.531, F -1.732)]  [G loss: 2.133] \n",
      "600 [D loss: (-0.097)(R 1.481, F -1.675)]  [G loss: 2.073] \n",
      "601 [D loss: (-0.108)(R 1.432, F -1.647)]  [G loss: 2.003] \n",
      "602 [D loss: (-0.146)(R 1.350, F -1.641)]  [G loss: 1.961] \n",
      "603 [D loss: (-0.155)(R 1.266, F -1.576)]  [G loss: 1.916] \n",
      "604 [D loss: (-0.104)(R 1.333, F -1.542)]  [G loss: 1.998] \n",
      "605 [D loss: (-0.113)(R 1.333, F -1.560)]  [G loss: 1.983] \n",
      "606 [D loss: (-0.188)(R 1.295, F -1.671)]  [G loss: 2.011] \n",
      "607 [D loss: (-0.160)(R 1.342, F -1.662)]  [G loss: 2.060] \n",
      "608 [D loss: (-0.111)(R 1.310, F -1.532)]  [G loss: 1.938] \n",
      "609 [D loss: (-0.129)(R 1.337, F -1.595)]  [G loss: 1.984] \n",
      "610 [D loss: (-0.145)(R 1.327, F -1.618)]  [G loss: 2.008] \n",
      "611 [D loss: (-0.156)(R 1.285, F -1.596)]  [G loss: 2.007] \n",
      "612 [D loss: (-0.135)(R 1.289, F -1.559)]  [G loss: 1.970] \n",
      "613 [D loss: (-0.127)(R 1.367, F -1.622)]  [G loss: 1.972] \n",
      "614 [D loss: (-0.134)(R 1.313, F -1.580)]  [G loss: 2.003] \n",
      "615 [D loss: (-0.146)(R 1.388, F -1.681)]  [G loss: 2.020] \n",
      "616 [D loss: (-0.159)(R 1.457, F -1.774)]  [G loss: 2.226] \n",
      "617 [D loss: (-0.242)(R 1.457, F -1.940)]  [G loss: 2.210] \n",
      "618 [D loss: (-0.107)(R 1.466, F -1.680)]  [G loss: 2.105] \n",
      "619 [D loss: (-0.140)(R 1.363, F -1.644)]  [G loss: 1.983] \n",
      "620 [D loss: (-0.108)(R 1.334, F -1.549)]  [G loss: 1.971] \n",
      "621 [D loss: (-0.172)(R 1.236, F -1.581)]  [G loss: 1.983] \n",
      "622 [D loss: (-0.181)(R 1.421, F -1.782)]  [G loss: 2.124] \n",
      "623 [D loss: (-0.109)(R 1.344, F -1.561)]  [G loss: 2.041] \n",
      "624 [D loss: (-0.159)(R 1.350, F -1.668)]  [G loss: 2.103] \n",
      "625 [D loss: (-0.114)(R 1.464, F -1.693)]  [G loss: 2.044] \n",
      "626 [D loss: (-0.137)(R 1.468, F -1.741)]  [G loss: 2.110] \n",
      "627 [D loss: (-0.121)(R 1.421, F -1.663)]  [G loss: 2.133] \n",
      "628 [D loss: (-0.164)(R 1.565, F -1.893)]  [G loss: 2.238] \n",
      "629 [D loss: (-0.102)(R 1.498, F -1.701)]  [G loss: 2.135] \n",
      "630 [D loss: (-0.167)(R 1.455, F -1.788)]  [G loss: 2.270] \n",
      "631 [D loss: (-0.111)(R 1.691, F -1.914)]  [G loss: 2.251] \n",
      "632 [D loss: (-0.194)(R 1.507, F -1.895)]  [G loss: 2.336] \n",
      "633 [D loss: (-0.152)(R 1.559, F -1.863)]  [G loss: 2.250] \n",
      "634 [D loss: (-0.185)(R 1.573, F -1.943)]  [G loss: 2.256] \n",
      "635 [D loss: (-0.124)(R 1.618, F -1.866)]  [G loss: 2.275] \n",
      "636 [D loss: (-0.148)(R 1.541, F -1.837)]  [G loss: 2.289] \n",
      "637 [D loss: (-0.117)(R 1.621, F -1.854)]  [G loss: 2.238] \n",
      "638 [D loss: (-0.044)(R 1.661, F -1.749)]  [G loss: 2.202] \n",
      "639 [D loss: (-0.091)(R 1.633, F -1.815)]  [G loss: 2.176] \n",
      "640 [D loss: (-0.149)(R 1.507, F -1.804)]  [G loss: 2.241] \n",
      "641 [D loss: (-0.122)(R 1.573, F -1.818)]  [G loss: 2.082] \n",
      "642 [D loss: (-0.156)(R 1.452, F -1.764)]  [G loss: 2.236] \n",
      "643 [D loss: (-0.226)(R 1.534, F -1.987)]  [G loss: 2.051] \n",
      "644 [D loss: (-0.187)(R 1.469, F -1.843)]  [G loss: 2.347] \n",
      "645 [D loss: (-0.135)(R 1.406, F -1.675)]  [G loss: 2.058] \n",
      "646 [D loss: (-0.177)(R 1.241, F -1.596)]  [G loss: 2.010] \n",
      "647 [D loss: (-0.223)(R 1.283, F -1.729)]  [G loss: 1.895] \n",
      "648 [D loss: (-0.135)(R 1.360, F -1.631)]  [G loss: 2.140] \n",
      "649 [D loss: (-0.152)(R 1.437, F -1.741)]  [G loss: 1.975] \n",
      "650 [D loss: (-0.168)(R 1.494, F -1.830)]  [G loss: 2.347] \n",
      "651 [D loss: (-0.115)(R 1.539, F -1.769)]  [G loss: 2.185] \n",
      "652 [D loss: (-0.152)(R 1.450, F -1.755)]  [G loss: 2.137] \n",
      "653 [D loss: (-0.177)(R 1.412, F -1.767)]  [G loss: 2.136] \n",
      "654 [D loss: (-0.240)(R 1.293, F -1.773)]  [G loss: 2.021] \n",
      "655 [D loss: (-0.218)(R 1.279, F -1.716)]  [G loss: 2.167] \n",
      "656 [D loss: (-0.131)(R 1.355, F -1.618)]  [G loss: 2.008] \n",
      "657 [D loss: (-0.136)(R 1.342, F -1.614)]  [G loss: 2.023] \n",
      "658 [D loss: (-0.189)(R 1.308, F -1.686)]  [G loss: 2.141] \n",
      "659 [D loss: (-0.133)(R 1.456, F -1.721)]  [G loss: 2.044] \n",
      "660 [D loss: (-0.112)(R 1.480, F -1.704)]  [G loss: 2.099] \n",
      "661 [D loss: (-0.109)(R 1.408, F -1.626)]  [G loss: 2.058] \n",
      "662 [D loss: (-0.176)(R 1.344, F -1.696)]  [G loss: 2.104] \n",
      "663 [D loss: (-0.321)(R 1.384, F -2.027)]  [G loss: 2.148] \n",
      "664 [D loss: (-0.162)(R 1.411, F -1.735)]  [G loss: 2.176] \n",
      "665 [D loss: (-0.141)(R 1.383, F -1.666)]  [G loss: 2.123] \n",
      "666 [D loss: (-0.074)(R 1.458, F -1.607)]  [G loss: 2.085] \n",
      "667 [D loss: (-0.147)(R 1.385, F -1.679)]  [G loss: 2.074] \n",
      "668 [D loss: (-0.146)(R 1.435, F -1.726)]  [G loss: 2.154] \n",
      "669 [D loss: (-0.151)(R 1.417, F -1.718)]  [G loss: 2.077] \n",
      "670 [D loss: (-0.126)(R 1.372, F -1.624)]  [G loss: 2.066] \n",
      "671 [D loss: (-0.155)(R 1.412, F -1.721)]  [G loss: 2.152] \n",
      "672 [D loss: (-0.194)(R 1.414, F -1.803)]  [G loss: 2.242] \n",
      "673 [D loss: (-0.207)(R 1.446, F -1.860)]  [G loss: 2.118] \n",
      "674 [D loss: (-0.178)(R 1.438, F -1.793)]  [G loss: 2.226] \n",
      "675 [D loss: (-0.145)(R 1.494, F -1.784)]  [G loss: 2.203] \n",
      "676 [D loss: (-0.134)(R 1.611, F -1.878)]  [G loss: 2.347] \n",
      "677 [D loss: (-0.171)(R 1.583, F -1.925)]  [G loss: 2.298] \n",
      "678 [D loss: (-0.224)(R 1.535, F -1.984)]  [G loss: 2.350] \n",
      "679 [D loss: (-0.145)(R 1.534, F -1.823)]  [G loss: 2.280] \n",
      "680 [D loss: (-0.137)(R 1.539, F -1.814)]  [G loss: 2.258] \n",
      "681 [D loss: (-0.173)(R 1.536, F -1.882)]  [G loss: 2.285] \n",
      "682 [D loss: (-0.153)(R 1.592, F -1.898)]  [G loss: 2.302] \n",
      "683 [D loss: (-0.144)(R 1.591, F -1.879)]  [G loss: 2.321] \n",
      "684 [D loss: (-0.232)(R 1.530, F -1.994)]  [G loss: 2.359] \n",
      "685 [D loss: (-0.191)(R 1.758, F -2.140)]  [G loss: 2.387] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "686 [D loss: (-0.180)(R 1.693, F -2.052)]  [G loss: 2.380] \n",
      "687 [D loss: (-0.144)(R 1.655, F -1.942)]  [G loss: 2.370] \n",
      "688 [D loss: (-0.113)(R 1.716, F -1.942)]  [G loss: 2.425] \n",
      "689 [D loss: (-0.135)(R 1.741, F -2.011)]  [G loss: 2.430] \n",
      "690 [D loss: (-0.139)(R 1.687, F -1.966)]  [G loss: 2.378] \n",
      "691 [D loss: (-0.130)(R 1.587, F -1.848)]  [G loss: 2.313] \n",
      "692 [D loss: (-0.167)(R 1.561, F -1.895)]  [G loss: 2.297] \n",
      "693 [D loss: (-0.160)(R 1.502, F -1.821)]  [G loss: 2.265] \n",
      "694 [D loss: (-0.095)(R 1.637, F -1.826)]  [G loss: 2.314] \n",
      "695 [D loss: (-0.180)(R 1.594, F -1.954)]  [G loss: 2.323] \n",
      "696 [D loss: (-0.147)(R 1.687, F -1.981)]  [G loss: 2.430] \n",
      "697 [D loss: (-0.113)(R 1.639, F -1.865)]  [G loss: 2.265] \n",
      "698 [D loss: (-0.122)(R 1.738, F -1.983)]  [G loss: 2.326] \n",
      "699 [D loss: (-0.177)(R 1.390, F -1.745)]  [G loss: 2.138] \n",
      "700 [D loss: (-0.322)(R 1.427, F -2.071)]  [G loss: 2.416] \n",
      "701 [D loss: (-0.128)(R 1.492, F -1.747)]  [G loss: 2.181] \n",
      "702 [D loss: (-0.137)(R 1.522, F -1.796)]  [G loss: 2.163] \n",
      "703 [D loss: (-0.208)(R 1.592, F -2.009)]  [G loss: 2.408] \n",
      "704 [D loss: (-0.062)(R 1.604, F -1.729)]  [G loss: 2.250] \n",
      "705 [D loss: (-0.145)(R 1.514, F -1.804)]  [G loss: 2.182] \n",
      "706 [D loss: (-0.133)(R 1.535, F -1.801)]  [G loss: 2.205] \n",
      "707 [D loss: (-0.207)(R 1.399, F -1.813)]  [G loss: 2.152] \n",
      "708 [D loss: (-0.166)(R 1.490, F -1.823)]  [G loss: 2.215] \n",
      "709 [D loss: (-0.133)(R 1.470, F -1.736)]  [G loss: 2.118] \n",
      "710 [D loss: (-0.085)(R 1.530, F -1.700)]  [G loss: 2.115] \n",
      "711 [D loss: (-0.148)(R 1.494, F -1.790)]  [G loss: 2.167] \n",
      "712 [D loss: (-0.116)(R 1.504, F -1.736)]  [G loss: 2.190] \n",
      "713 [D loss: (-0.100)(R 1.459, F -1.658)]  [G loss: 2.110] \n",
      "714 [D loss: (-0.104)(R 1.382, F -1.589)]  [G loss: 2.020] \n",
      "715 [D loss: (-0.101)(R 1.465, F -1.667)]  [G loss: 2.057] \n",
      "716 [D loss: (-0.116)(R 1.490, F -1.721)]  [G loss: 2.134] \n",
      "717 [D loss: (-0.166)(R 1.556, F -1.888)]  [G loss: 2.108] \n",
      "718 [D loss: (-0.102)(R 1.577, F -1.782)]  [G loss: 2.234] \n",
      "719 [D loss: (-0.115)(R 1.510, F -1.741)]  [G loss: 2.187] \n",
      "720 [D loss: (-0.116)(R 1.539, F -1.771)]  [G loss: 2.191] \n",
      "721 [D loss: (-0.057)(R 1.539, F -1.653)]  [G loss: 2.157] \n",
      "722 [D loss: (-0.108)(R 1.488, F -1.703)]  [G loss: 2.123] \n",
      "723 [D loss: (-0.126)(R 1.495, F -1.747)]  [G loss: 2.198] \n",
      "724 [D loss: (-0.213)(R 1.554, F -1.981)]  [G loss: 2.210] \n",
      "725 [D loss: (-0.068)(R 1.570, F -1.706)]  [G loss: 2.211] \n",
      "726 [D loss: (-0.175)(R 1.679, F -2.029)]  [G loss: 2.505] \n",
      "727 [D loss: (-0.136)(R 1.577, F -1.849)]  [G loss: 2.245] \n",
      "728 [D loss: (-0.108)(R 1.593, F -1.808)]  [G loss: 2.199] \n",
      "729 [D loss: (-0.132)(R 1.545, F -1.810)]  [G loss: 2.200] \n",
      "730 [D loss: (-0.121)(R 1.548, F -1.790)]  [G loss: 2.251] \n",
      "731 [D loss: (-0.152)(R 1.432, F -1.736)]  [G loss: 2.129] \n",
      "732 [D loss: (-0.107)(R 1.501, F -1.714)]  [G loss: 2.174] \n",
      "733 [D loss: (-0.044)(R 1.614, F -1.703)]  [G loss: 2.139] \n",
      "734 [D loss: (-0.087)(R 1.699, F -1.872)]  [G loss: 2.246] \n",
      "735 [D loss: (-0.073)(R 1.546, F -1.692)]  [G loss: 2.166] \n",
      "736 [D loss: (-0.075)(R 1.501, F -1.652)]  [G loss: 2.086] \n",
      "737 [D loss: (-0.059)(R 1.480, F -1.598)]  [G loss: 2.028] \n",
      "738 [D loss: (-0.070)(R 1.398, F -1.538)]  [G loss: 1.995] \n",
      "739 [D loss: (-0.116)(R 1.498, F -1.730)]  [G loss: 2.143] \n",
      "740 [D loss: (-0.118)(R 1.468, F -1.705)]  [G loss: 2.128] \n",
      "741 [D loss: (-0.087)(R 1.473, F -1.646)]  [G loss: 2.032] \n",
      "742 [D loss: (-0.119)(R 1.431, F -1.669)]  [G loss: 2.040] \n",
      "743 [D loss: (-0.106)(R 1.419, F -1.632)]  [G loss: 2.059] \n",
      "744 [D loss: (-0.067)(R 1.498, F -1.633)]  [G loss: 2.032] \n",
      "745 [D loss: (-0.115)(R 1.464, F -1.694)]  [G loss: 2.053] \n",
      "746 [D loss: (-0.107)(R 1.394, F -1.608)]  [G loss: 2.031] \n",
      "747 [D loss: (-0.032)(R 1.473, F -1.538)]  [G loss: 1.968] \n",
      "748 [D loss: (-0.044)(R 1.417, F -1.505)]  [G loss: 1.940] \n",
      "749 [D loss: (-0.030)(R 1.334, F -1.395)]  [G loss: 1.871] \n",
      "750 [D loss: (-0.063)(R 1.339, F -1.466)]  [G loss: 1.932] \n",
      "751 [D loss: (-0.070)(R 1.440, F -1.580)]  [G loss: 2.007] \n",
      "752 [D loss: (-0.094)(R 1.542, F -1.730)]  [G loss: 2.013] \n",
      "753 [D loss: (-0.084)(R 1.418, F -1.585)]  [G loss: 1.951] \n",
      "754 [D loss: (-0.048)(R 1.380, F -1.476)]  [G loss: 1.891] \n",
      "755 [D loss: (-0.082)(R 1.402, F -1.566)]  [G loss: 1.946] \n",
      "756 [D loss: (-0.080)(R 1.418, F -1.579)]  [G loss: 2.009] \n",
      "757 [D loss: (-0.206)(R 1.601, F -2.012)]  [G loss: 2.361] \n",
      "758 [D loss: (-0.177)(R 1.550, F -1.903)]  [G loss: 2.364] \n",
      "759 [D loss: (-0.072)(R 1.655, F -1.799)]  [G loss: 2.214] \n",
      "760 [D loss: (-0.127)(R 1.558, F -1.811)]  [G loss: 2.125] \n",
      "761 [D loss: (-0.128)(R 1.537, F -1.792)]  [G loss: 2.245] \n",
      "762 [D loss: (-0.073)(R 1.591, F -1.736)]  [G loss: 2.120] \n",
      "763 [D loss: (-0.075)(R 1.676, F -1.826)]  [G loss: 2.215] \n",
      "764 [D loss: (-0.126)(R 1.620, F -1.872)]  [G loss: 2.277] \n",
      "765 [D loss: (-0.053)(R 1.652, F -1.757)]  [G loss: 2.166] \n",
      "766 [D loss: (-0.056)(R 1.608, F -1.720)]  [G loss: 2.149] \n",
      "767 [D loss: (-0.067)(R 1.590, F -1.723)]  [G loss: 2.065] \n",
      "768 [D loss: (-0.078)(R 1.566, F -1.723)]  [G loss: 2.068] \n",
      "769 [D loss: (-0.083)(R 1.572, F -1.738)]  [G loss: 2.111] \n",
      "770 [D loss: (-0.110)(R 1.436, F -1.656)]  [G loss: 2.018] \n",
      "771 [D loss: (-0.216)(R 1.438, F -1.871)]  [G loss: 2.131] \n",
      "772 [D loss: (-0.143)(R 1.437, F -1.724)]  [G loss: 2.128] \n",
      "773 [D loss: (-0.063)(R 1.493, F -1.618)]  [G loss: 2.007] \n",
      "774 [D loss: (-0.057)(R 1.473, F -1.588)]  [G loss: 2.034] \n",
      "775 [D loss: (-0.042)(R 1.477, F -1.562)]  [G loss: 1.958] \n",
      "776 [D loss: (-0.101)(R 1.387, F -1.590)]  [G loss: 1.898] \n",
      "777 [D loss: (-0.215)(R 1.218, F -1.649)]  [G loss: 1.989] \n",
      "778 [D loss: (-0.083)(R 1.282, F -1.447)]  [G loss: 1.877] \n",
      "779 [D loss: (-0.076)(R 1.305, F -1.456)]  [G loss: 1.878] \n",
      "780 [D loss: (-0.115)(R 1.295, F -1.525)]  [G loss: 1.918] \n",
      "781 [D loss: (-0.121)(R 1.288, F -1.529)]  [G loss: 1.902] \n",
      "782 [D loss: (-0.094)(R 1.313, F -1.501)]  [G loss: 1.880] \n",
      "783 [D loss: (-0.056)(R 1.333, F -1.444)]  [G loss: 1.834] \n",
      "784 [D loss: (-0.141)(R 1.223, F -1.505)]  [G loss: 1.866] \n",
      "785 [D loss: (-0.212)(R 1.166, F -1.590)]  [G loss: 1.882] \n",
      "786 [D loss: (-0.088)(R 1.238, F -1.414)]  [G loss: 1.886] \n",
      "787 [D loss: (-0.098)(R 1.325, F -1.521)]  [G loss: 1.953] \n",
      "788 [D loss: (-0.094)(R 1.309, F -1.496)]  [G loss: 1.946] \n",
      "789 [D loss: (-0.119)(R 1.396, F -1.634)]  [G loss: 1.943] \n",
      "790 [D loss: (-0.090)(R 1.417, F -1.598)]  [G loss: 2.070] \n",
      "791 [D loss: (-0.108)(R 1.454, F -1.671)]  [G loss: 1.978] \n",
      "792 [D loss: (-0.134)(R 1.425, F -1.693)]  [G loss: 2.120] \n",
      "793 [D loss: (-0.142)(R 1.311, F -1.594)]  [G loss: 1.942] \n",
      "794 [D loss: (-0.147)(R 1.405, F -1.698)]  [G loss: 2.113] \n",
      "795 [D loss: (-0.102)(R 1.396, F -1.601)]  [G loss: 1.948] \n",
      "796 [D loss: (-0.117)(R 1.337, F -1.571)]  [G loss: 1.952] \n",
      "797 [D loss: (-0.178)(R 1.215, F -1.570)]  [G loss: 1.928] \n",
      "798 [D loss: (-0.195)(R 1.234, F -1.623)]  [G loss: 1.977] \n",
      "799 [D loss: (-0.104)(R 1.341, F -1.549)]  [G loss: 1.908] \n",
      "800 [D loss: (-0.073)(R 1.401, F -1.548)]  [G loss: 1.835] \n",
      "801 [D loss: (-0.207)(R 1.297, F -1.712)]  [G loss: 1.977] \n",
      "802 [D loss: (-0.099)(R 1.280, F -1.478)]  [G loss: 1.885] \n",
      "803 [D loss: (-0.104)(R 1.268, F -1.476)]  [G loss: 1.815] \n",
      "804 [D loss: (-0.161)(R 1.307, F -1.629)]  [G loss: 1.972] \n",
      "805 [D loss: (-0.060)(R 1.361, F -1.482)]  [G loss: 1.830] \n",
      "806 [D loss: (-0.062)(R 1.360, F -1.484)]  [G loss: 1.859] \n",
      "807 [D loss: (-0.101)(R 1.223, F -1.426)]  [G loss: 1.788] \n",
      "808 [D loss: (-0.132)(R 1.223, F -1.488)]  [G loss: 1.823] \n",
      "809 [D loss: (-0.212)(R 1.193, F -1.617)]  [G loss: 1.962] \n",
      "810 [D loss: (-0.100)(R 1.329, F -1.529)]  [G loss: 1.963] \n",
      "811 [D loss: (-0.120)(R 1.448, F -1.687)]  [G loss: 1.926] \n",
      "812 [D loss: (-0.073)(R 1.380, F -1.526)]  [G loss: 1.944] \n",
      "813 [D loss: (-0.095)(R 1.293, F -1.482)]  [G loss: 1.826] \n",
      "814 [D loss: (-0.086)(R 1.258, F -1.431)]  [G loss: 1.794] \n",
      "815 [D loss: (-0.072)(R 1.324, F -1.468)]  [G loss: 1.835] \n",
      "816 [D loss: (-0.066)(R 1.287, F -1.419)]  [G loss: 1.827] \n",
      "817 [D loss: (-0.100)(R 1.298, F -1.499)]  [G loss: 1.759] \n",
      "818 [D loss: (-0.157)(R 1.272, F -1.585)]  [G loss: 1.961] \n",
      "819 [D loss: (-0.062)(R 1.250, F -1.374)]  [G loss: 1.783] \n",
      "820 [D loss: (-0.153)(R 1.146, F -1.452)]  [G loss: 1.792] \n",
      "821 [D loss: (-0.159)(R 1.059, F -1.377)]  [G loss: 1.763] \n",
      "822 [D loss: (-0.094)(R 1.152, F -1.339)]  [G loss: 1.640] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "823 [D loss: (-0.063)(R 1.226, F -1.352)]  [G loss: 1.755] \n",
      "824 [D loss: (-0.085)(R 1.106, F -1.275)]  [G loss: 1.689] \n",
      "825 [D loss: (-0.149)(R 1.109, F -1.407)]  [G loss: 1.653] \n",
      "826 [D loss: (-0.082)(R 1.111, F -1.276)]  [G loss: 1.569] \n",
      "827 [D loss: (-0.079)(R 1.192, F -1.350)]  [G loss: 1.641] \n",
      "828 [D loss: (-0.071)(R 1.238, F -1.380)]  [G loss: 1.798] \n",
      "829 [D loss: (-0.074)(R 1.176, F -1.324)]  [G loss: 1.683] \n",
      "830 [D loss: (-0.183)(R 1.042, F -1.407)]  [G loss: 1.666] \n",
      "831 [D loss: (-0.004)(R 1.276, F -1.285)]  [G loss: 1.600] \n",
      "832 [D loss: (-0.102)(R 1.070, F -1.274)]  [G loss: 1.565] \n",
      "833 [D loss: (0.008)(R 1.109, F -1.093)]  [G loss: 1.491] \n",
      "834 [D loss: (-0.005)(R 1.123, F -1.132)]  [G loss: 1.450] \n",
      "835 [D loss: (-0.014)(R 1.049, F -1.077)]  [G loss: 1.413] \n",
      "836 [D loss: (-0.002)(R 1.013, F -1.017)]  [G loss: 1.309] \n",
      "837 [D loss: (-0.011)(R 0.929, F -0.950)]  [G loss: 1.255] \n",
      "838 [D loss: (-0.113)(R 0.891, F -1.117)]  [G loss: 1.387] \n",
      "839 [D loss: (-0.055)(R 1.093, F -1.203)]  [G loss: 1.451] \n",
      "840 [D loss: (-0.010)(R 1.112, F -1.131)]  [G loss: 1.456] \n",
      "841 [D loss: (0.023)(R 1.085, F -1.040)]  [G loss: 1.406] \n",
      "842 [D loss: (-0.046)(R 0.928, F -1.020)]  [G loss: 1.266] \n",
      "843 [D loss: (-0.049)(R 0.892, F -0.990)]  [G loss: 1.269] \n",
      "844 [D loss: (0.046)(R 0.960, F -0.869)]  [G loss: 1.197] \n",
      "845 [D loss: (-0.003)(R 0.940, F -0.946)]  [G loss: 1.342] \n",
      "846 [D loss: (-0.009)(R 0.958, F -0.975)]  [G loss: 1.328] \n",
      "847 [D loss: (-0.056)(R 0.962, F -1.074)]  [G loss: 1.379] \n",
      "848 [D loss: (-0.034)(R 0.908, F -0.975)]  [G loss: 1.258] \n",
      "849 [D loss: (-0.001)(R 0.873, F -0.875)]  [G loss: 1.204] \n",
      "850 [D loss: (-0.002)(R 0.958, F -0.963)]  [G loss: 1.281] \n",
      "851 [D loss: (0.016)(R 1.010, F -0.978)]  [G loss: 1.315] \n",
      "852 [D loss: (-0.018)(R 0.977, F -1.014)]  [G loss: 1.279] \n",
      "853 [D loss: (-0.033)(R 0.994, F -1.060)]  [G loss: 1.340] \n",
      "854 [D loss: (-0.039)(R 0.863, F -0.942)]  [G loss: 1.229] \n",
      "855 [D loss: (-0.001)(R 0.922, F -0.924)]  [G loss: 1.184] \n",
      "856 [D loss: (-0.023)(R 0.875, F -0.921)]  [G loss: 1.170] \n",
      "857 [D loss: (-0.333)(R 1.126, F -1.791)]  [G loss: 1.859] \n",
      "858 [D loss: (0.058)(R 0.979, F -0.863)]  [G loss: 1.181] \n",
      "859 [D loss: (-0.040)(R 0.857, F -0.938)]  [G loss: 1.199] \n",
      "860 [D loss: (-0.025)(R 0.805, F -0.856)]  [G loss: 1.104] \n",
      "861 [D loss: (-0.032)(R 0.722, F -0.787)]  [G loss: 1.039] \n",
      "862 [D loss: (-0.008)(R 0.735, F -0.752)]  [G loss: 1.004] \n",
      "863 [D loss: (-0.068)(R 0.742, F -0.878)]  [G loss: 1.017] \n",
      "864 [D loss: (-0.020)(R 0.647, F -0.688)]  [G loss: 0.953] \n",
      "865 [D loss: (0.010)(R 0.702, F -0.683)]  [G loss: 0.899] \n",
      "866 [D loss: (0.004)(R 0.662, F -0.654)]  [G loss: 0.911] \n",
      "867 [D loss: (-0.010)(R 0.642, F -0.663)]  [G loss: 0.895] \n",
      "868 [D loss: (-0.013)(R 0.650, F -0.676)]  [G loss: 0.890] \n",
      "869 [D loss: (0.003)(R 0.662, F -0.655)]  [G loss: 0.883] \n",
      "870 [D loss: (-0.002)(R 0.621, F -0.625)]  [G loss: 0.882] \n",
      "871 [D loss: (-0.019)(R 0.623, F -0.661)]  [G loss: 0.921] \n",
      "872 [D loss: (-0.026)(R 0.759, F -0.810)]  [G loss: 1.058] \n",
      "873 [D loss: (-0.068)(R 0.781, F -0.917)]  [G loss: 1.134] \n",
      "874 [D loss: (-0.035)(R 0.747, F -0.816)]  [G loss: 1.082] \n",
      "875 [D loss: (-0.031)(R 0.713, F -0.776)]  [G loss: 1.038] \n",
      "876 [D loss: (-0.073)(R 0.686, F -0.831)]  [G loss: 1.102] \n",
      "877 [D loss: (-0.061)(R 0.691, F -0.814)]  [G loss: 1.011] \n",
      "878 [D loss: (-0.053)(R 0.691, F -0.797)]  [G loss: 1.045] \n",
      "879 [D loss: (-0.056)(R 0.677, F -0.789)]  [G loss: 1.004] \n",
      "880 [D loss: (-0.088)(R 0.635, F -0.811)]  [G loss: 1.015] \n",
      "881 [D loss: (-0.045)(R 0.674, F -0.763)]  [G loss: 1.009] \n",
      "882 [D loss: (-0.147)(R 0.769, F -1.063)]  [G loss: 1.293] \n",
      "883 [D loss: (-0.128)(R 0.702, F -0.959)]  [G loss: 1.162] \n",
      "884 [D loss: (-0.066)(R 0.718, F -0.850)]  [G loss: 1.077] \n",
      "885 [D loss: (-0.067)(R 0.701, F -0.836)]  [G loss: 1.135] \n",
      "886 [D loss: (-0.051)(R 0.722, F -0.823)]  [G loss: 1.063] \n",
      "887 [D loss: (-0.046)(R 0.683, F -0.776)]  [G loss: 1.039] \n",
      "888 [D loss: (-0.047)(R 0.631, F -0.725)]  [G loss: 0.988] \n",
      "889 [D loss: (-0.032)(R 0.628, F -0.691)]  [G loss: 0.910] \n",
      "890 [D loss: (-0.021)(R 0.602, F -0.645)]  [G loss: 0.884] \n",
      "891 [D loss: (-0.028)(R 0.592, F -0.648)]  [G loss: 0.846] \n",
      "892 [D loss: (0.006)(R 0.682, F -0.669)]  [G loss: 0.948] \n",
      "893 [D loss: (-0.025)(R 0.586, F -0.637)]  [G loss: 0.875] \n",
      "894 [D loss: (-0.039)(R 0.567, F -0.645)]  [G loss: 0.903] \n",
      "895 [D loss: (-0.082)(R 0.555, F -0.719)]  [G loss: 0.907] \n",
      "896 [D loss: (-0.044)(R 0.621, F -0.708)]  [G loss: 0.961] \n",
      "897 [D loss: (-0.055)(R 0.606, F -0.716)]  [G loss: 1.002] \n",
      "898 [D loss: (-0.063)(R 0.698, F -0.823)]  [G loss: 1.132] \n",
      "899 [D loss: (-0.123)(R 0.727, F -0.973)]  [G loss: 1.096] \n",
      "900 [D loss: (-0.068)(R 0.777, F -0.914)]  [G loss: 1.220] \n",
      "901 [D loss: (-0.105)(R 0.716, F -0.927)]  [G loss: 1.206] \n",
      "902 [D loss: (-0.113)(R 0.710, F -0.935)]  [G loss: 1.202] \n",
      "903 [D loss: (-0.113)(R 0.703, F -0.929)]  [G loss: 1.124] \n",
      "904 [D loss: (-0.118)(R 0.698, F -0.934)]  [G loss: 1.159] \n",
      "905 [D loss: (-0.097)(R 0.657, F -0.850)]  [G loss: 1.096] \n",
      "906 [D loss: (-0.038)(R 0.738, F -0.815)]  [G loss: 1.034] \n",
      "907 [D loss: (-0.071)(R 0.676, F -0.817)]  [G loss: 1.013] \n",
      "908 [D loss: (-0.112)(R 0.674, F -0.899)]  [G loss: 0.993] \n",
      "909 [D loss: (-0.112)(R 0.676, F -0.901)]  [G loss: 1.148] \n",
      "910 [D loss: (-0.070)(R 0.723, F -0.863)]  [G loss: 1.094] \n",
      "911 [D loss: (-0.027)(R 0.925, F -0.979)]  [G loss: 1.021] \n",
      "912 [D loss: (-0.057)(R 0.730, F -0.845)]  [G loss: 1.049] \n",
      "913 [D loss: (-0.055)(R 0.687, F -0.798)]  [G loss: 1.018] \n",
      "914 [D loss: (-0.056)(R 0.618, F -0.731)]  [G loss: 0.953] \n",
      "915 [D loss: (-0.079)(R 0.627, F -0.784)]  [G loss: 0.912] \n",
      "916 [D loss: (-0.052)(R 0.584, F -0.689)]  [G loss: 0.916] \n",
      "917 [D loss: (-0.058)(R 0.588, F -0.704)]  [G loss: 0.952] \n",
      "918 [D loss: (-0.033)(R 0.641, F -0.707)]  [G loss: 0.968] \n",
      "919 [D loss: (-0.034)(R 0.610, F -0.678)]  [G loss: 0.906] \n",
      "920 [D loss: (-0.044)(R 0.574, F -0.662)]  [G loss: 0.926] \n",
      "921 [D loss: (-0.095)(R 0.547, F -0.738)]  [G loss: 1.001] \n",
      "922 [D loss: (-0.093)(R 0.591, F -0.778)]  [G loss: 1.032] \n",
      "923 [D loss: (-0.053)(R 0.636, F -0.741)]  [G loss: 1.031] \n",
      "924 [D loss: (-0.066)(R 0.701, F -0.833)]  [G loss: 1.143] \n",
      "925 [D loss: (-0.162)(R 0.645, F -0.969)]  [G loss: 1.087] \n",
      "926 [D loss: (-0.074)(R 0.736, F -0.884)]  [G loss: 1.306] \n",
      "927 [D loss: (-0.112)(R 0.668, F -0.891)]  [G loss: 1.114] \n",
      "928 [D loss: (-0.122)(R 0.765, F -1.009)]  [G loss: 1.279] \n",
      "929 [D loss: (-0.082)(R 0.771, F -0.935)]  [G loss: 1.117] \n",
      "930 [D loss: (-0.080)(R 0.736, F -0.895)]  [G loss: 1.037] \n",
      "931 [D loss: (-0.025)(R 0.834, F -0.885)]  [G loss: 1.167] \n",
      "932 [D loss: (-0.053)(R 0.808, F -0.914)]  [G loss: 1.008] \n",
      "933 [D loss: (-0.048)(R 0.727, F -0.823)]  [G loss: 1.132] \n",
      "934 [D loss: (-0.022)(R 0.715, F -0.759)]  [G loss: 1.047] \n",
      "935 [D loss: (-0.010)(R 0.726, F -0.746)]  [G loss: 1.058] \n",
      "936 [D loss: (-0.066)(R 0.682, F -0.814)]  [G loss: 1.046] \n",
      "937 [D loss: (-0.082)(R 0.765, F -0.929)]  [G loss: 1.072] \n",
      "938 [D loss: (-0.003)(R 0.943, F -0.948)]  [G loss: 1.210] \n",
      "939 [D loss: (-0.053)(R 0.787, F -0.893)]  [G loss: 1.174] \n",
      "940 [D loss: (-0.057)(R 0.766, F -0.881)]  [G loss: 1.145] \n",
      "941 [D loss: (-0.032)(R 0.749, F -0.814)]  [G loss: 1.073] \n",
      "942 [D loss: (-0.035)(R 0.778, F -0.849)]  [G loss: 1.071] \n",
      "943 [D loss: (-0.029)(R 0.732, F -0.790)]  [G loss: 1.048] \n",
      "944 [D loss: (-0.018)(R 0.722, F -0.758)]  [G loss: 0.978] \n",
      "945 [D loss: (-0.037)(R 0.654, F -0.727)]  [G loss: 0.966] \n",
      "946 [D loss: (-0.029)(R 0.651, F -0.709)]  [G loss: 0.940] \n",
      "947 [D loss: (-0.062)(R 0.592, F -0.716)]  [G loss: 0.879] \n",
      "948 [D loss: (-0.044)(R 0.615, F -0.703)]  [G loss: 0.918] \n",
      "949 [D loss: (-0.040)(R 0.658, F -0.738)]  [G loss: 0.889] \n",
      "950 [D loss: (-0.044)(R 0.648, F -0.736)]  [G loss: 0.972] \n",
      "951 [D loss: (-0.060)(R 0.717, F -0.838)]  [G loss: 1.109] \n",
      "952 [D loss: (-0.052)(R 0.712, F -0.816)]  [G loss: 1.063] \n",
      "953 [D loss: (-0.054)(R 0.706, F -0.815)]  [G loss: 1.066] \n",
      "954 [D loss: (-0.132)(R 0.669, F -0.932)]  [G loss: 1.047] \n",
      "955 [D loss: (-0.043)(R 0.750, F -0.836)]  [G loss: 1.117] \n",
      "956 [D loss: (-0.136)(R 0.730, F -1.001)]  [G loss: 1.173] \n",
      "957 [D loss: (-0.092)(R 0.716, F -0.900)]  [G loss: 1.080] \n",
      "958 [D loss: (-0.038)(R 0.755, F -0.831)]  [G loss: 1.024] \n",
      "959 [D loss: (-0.075)(R 0.681, F -0.832)]  [G loss: 1.038] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "960 [D loss: (-0.098)(R 0.651, F -0.847)]  [G loss: 1.058] \n",
      "961 [D loss: (-0.063)(R 0.656, F -0.782)]  [G loss: 1.028] \n",
      "962 [D loss: (-0.057)(R 0.640, F -0.754)]  [G loss: 1.007] \n",
      "963 [D loss: (-0.070)(R 0.646, F -0.787)]  [G loss: 0.983] \n",
      "964 [D loss: (-0.053)(R 0.651, F -0.757)]  [G loss: 0.958] \n",
      "965 [D loss: (-0.083)(R 0.583, F -0.748)]  [G loss: 0.940] \n",
      "966 [D loss: (-0.063)(R 0.589, F -0.715)]  [G loss: 0.878] \n",
      "967 [D loss: (-0.036)(R 0.595, F -0.667)]  [G loss: 0.897] \n",
      "968 [D loss: (-0.027)(R 0.575, F -0.629)]  [G loss: 0.846] \n",
      "969 [D loss: (-0.058)(R 0.533, F -0.649)]  [G loss: 0.779] \n",
      "970 [D loss: (-0.013)(R 0.565, F -0.592)]  [G loss: 0.829] \n",
      "971 [D loss: (-0.027)(R 0.555, F -0.609)]  [G loss: 0.757] \n",
      "972 [D loss: (-0.063)(R 0.516, F -0.643)]  [G loss: 0.778] \n",
      "973 [D loss: (-0.182)(R 0.673, F -1.038)]  [G loss: 1.444] \n",
      "974 [D loss: (-0.052)(R 0.612, F -0.716)]  [G loss: 0.894] \n",
      "975 [D loss: (-0.119)(R 0.555, F -0.792)]  [G loss: 0.888] \n",
      "976 [D loss: (-0.074)(R 0.599, F -0.747)]  [G loss: 0.875] \n",
      "977 [D loss: (-0.094)(R 0.610, F -0.797)]  [G loss: 1.053] \n",
      "978 [D loss: (-0.078)(R 0.615, F -0.772)]  [G loss: 0.921] \n",
      "979 [D loss: (-0.124)(R 0.562, F -0.810)]  [G loss: 1.036] \n",
      "980 [D loss: (-0.066)(R 0.619, F -0.752)]  [G loss: 0.928] \n",
      "981 [D loss: (-0.109)(R 0.567, F -0.786)]  [G loss: 0.965] \n",
      "982 [D loss: (-0.115)(R 0.715, F -0.945)]  [G loss: 1.114] \n",
      "983 [D loss: (-0.084)(R 0.664, F -0.832)]  [G loss: 1.027] \n",
      "984 [D loss: (-0.131)(R 0.630, F -0.892)]  [G loss: 1.041] \n",
      "985 [D loss: (-0.111)(R 0.684, F -0.906)]  [G loss: 1.091] \n",
      "986 [D loss: (-0.136)(R 0.591, F -0.863)]  [G loss: 1.078] \n",
      "987 [D loss: (-0.093)(R 0.600, F -0.787)]  [G loss: 0.988] \n",
      "988 [D loss: (-0.124)(R 0.592, F -0.839)]  [G loss: 1.009] \n",
      "989 [D loss: (-0.057)(R 0.590, F -0.703)]  [G loss: 0.961] \n",
      "990 [D loss: (-0.058)(R 0.606, F -0.722)]  [G loss: 0.929] \n",
      "991 [D loss: (-0.070)(R 0.600, F -0.741)]  [G loss: 0.928] \n",
      "992 [D loss: (-0.111)(R 0.540, F -0.763)]  [G loss: 0.931] \n",
      "993 [D loss: (-0.062)(R 0.578, F -0.702)]  [G loss: 0.919] \n",
      "994 [D loss: (-0.068)(R 0.551, F -0.686)]  [G loss: 0.932] \n",
      "995 [D loss: (-0.068)(R 0.594, F -0.731)]  [G loss: 0.978] \n",
      "996 [D loss: (-0.062)(R 0.579, F -0.703)]  [G loss: 0.918] \n",
      "997 [D loss: (-0.083)(R 0.600, F -0.766)]  [G loss: 0.909] \n",
      "998 [D loss: (-0.046)(R 0.630, F -0.721)]  [G loss: 0.913] \n",
      "999 [D loss: (-0.052)(R 0.607, F -0.712)]  [G loss: 0.910] \n",
      "1000 [D loss: (-0.073)(R 0.618, F -0.764)]  [G loss: 0.918] \n",
      "1001 [D loss: (-0.016)(R 0.600, F -0.632)]  [G loss: 0.879] \n",
      "1002 [D loss: (-0.063)(R 0.559, F -0.685)]  [G loss: 0.912] \n",
      "1003 [D loss: (-0.061)(R 0.536, F -0.658)]  [G loss: 0.872] \n",
      "1004 [D loss: (-0.057)(R 0.544, F -0.658)]  [G loss: 0.887] \n",
      "1005 [D loss: (-0.072)(R 0.546, F -0.690)]  [G loss: 0.865] \n",
      "1006 [D loss: (-0.042)(R 0.573, F -0.658)]  [G loss: 0.869] \n",
      "1007 [D loss: (-0.061)(R 0.596, F -0.719)]  [G loss: 0.895] \n",
      "1008 [D loss: (-0.095)(R 0.544, F -0.733)]  [G loss: 0.930] \n",
      "1009 [D loss: (-0.068)(R 0.605, F -0.742)]  [G loss: 0.979] \n",
      "1010 [D loss: (-0.107)(R 0.696, F -0.910)]  [G loss: 1.090] \n",
      "1011 [D loss: (-0.087)(R 0.684, F -0.858)]  [G loss: 1.104] \n",
      "1012 [D loss: (-0.112)(R 0.634, F -0.858)]  [G loss: 1.097] \n",
      "1013 [D loss: (-0.093)(R 0.651, F -0.838)]  [G loss: 1.098] \n",
      "1014 [D loss: (-0.077)(R 0.715, F -0.869)]  [G loss: 1.066] \n",
      "1015 [D loss: (-0.101)(R 0.669, F -0.871)]  [G loss: 1.025] \n",
      "1016 [D loss: (-0.078)(R 0.706, F -0.862)]  [G loss: 1.029] \n",
      "1017 [D loss: (-0.089)(R 0.650, F -0.827)]  [G loss: 1.042] \n",
      "1018 [D loss: (-0.100)(R 0.699, F -0.899)]  [G loss: 1.084] \n",
      "1019 [D loss: (-0.109)(R 0.664, F -0.881)]  [G loss: 1.134] \n",
      "1020 [D loss: (-0.087)(R 0.701, F -0.875)]  [G loss: 1.127] \n",
      "1021 [D loss: (-0.061)(R 0.763, F -0.885)]  [G loss: 1.048] \n",
      "1022 [D loss: (-0.059)(R 0.711, F -0.828)]  [G loss: 1.004] \n",
      "1023 [D loss: (-0.037)(R 0.734, F -0.808)]  [G loss: 0.993] \n",
      "1024 [D loss: (-0.077)(R 0.648, F -0.802)]  [G loss: 0.952] \n",
      "1025 [D loss: (-0.119)(R 0.694, F -0.931)]  [G loss: 1.102] \n",
      "1026 [D loss: (-0.064)(R 0.703, F -0.831)]  [G loss: 1.054] \n",
      "1027 [D loss: (-0.050)(R 0.677, F -0.777)]  [G loss: 1.040] \n",
      "1028 [D loss: (-0.085)(R 0.693, F -0.862)]  [G loss: 1.060] \n",
      "1029 [D loss: (-0.053)(R 0.728, F -0.835)]  [G loss: 1.027] \n",
      "1030 [D loss: (-0.042)(R 0.729, F -0.813)]  [G loss: 1.058] \n",
      "1031 [D loss: (-0.055)(R 0.714, F -0.823)]  [G loss: 1.051] \n",
      "1032 [D loss: (-0.024)(R 0.737, F -0.785)]  [G loss: 1.072] \n",
      "1033 [D loss: (-0.079)(R 0.708, F -0.866)]  [G loss: 1.079] \n",
      "1034 [D loss: (-0.064)(R 0.695, F -0.822)]  [G loss: 1.066] \n",
      "1035 [D loss: (-0.087)(R 0.659, F -0.832)]  [G loss: 1.070] \n",
      "1036 [D loss: (-0.104)(R 0.647, F -0.856)]  [G loss: 1.036] \n",
      "1037 [D loss: (-0.088)(R 0.677, F -0.854)]  [G loss: 1.081] \n",
      "1038 [D loss: (-0.055)(R 0.702, F -0.811)]  [G loss: 1.054] \n",
      "1039 [D loss: (-0.030)(R 0.809, F -0.869)]  [G loss: 1.075] \n",
      "1040 [D loss: (-0.043)(R 0.753, F -0.839)]  [G loss: 1.076] \n",
      "1041 [D loss: (-0.070)(R 0.691, F -0.832)]  [G loss: 1.073] \n",
      "1042 [D loss: (-0.129)(R 0.614, F -0.872)]  [G loss: 1.065] \n",
      "1043 [D loss: (-0.090)(R 0.726, F -0.905)]  [G loss: 1.057] \n",
      "1044 [D loss: (-0.043)(R 0.715, F -0.801)]  [G loss: 1.010] \n",
      "1045 [D loss: (-0.033)(R 0.710, F -0.775)]  [G loss: 1.000] \n",
      "1046 [D loss: (-0.066)(R 0.646, F -0.777)]  [G loss: 0.956] \n",
      "1047 [D loss: (-0.088)(R 0.737, F -0.913)]  [G loss: 1.099] \n",
      "1048 [D loss: (-0.052)(R 0.701, F -0.805)]  [G loss: 0.994] \n",
      "1049 [D loss: (-0.102)(R 0.658, F -0.861)]  [G loss: 1.031] \n",
      "1050 [D loss: (-0.047)(R 0.742, F -0.835)]  [G loss: 1.080] \n",
      "1051 [D loss: (-0.092)(R 0.631, F -0.814)]  [G loss: 1.014] \n",
      "1052 [D loss: (-0.061)(R 0.634, F -0.756)]  [G loss: 0.985] \n",
      "1053 [D loss: (-0.057)(R 0.640, F -0.755)]  [G loss: 0.957] \n",
      "1054 [D loss: (-0.115)(R 0.587, F -0.817)]  [G loss: 0.980] \n",
      "1055 [D loss: (-0.055)(R 0.699, F -0.808)]  [G loss: 1.058] \n",
      "1056 [D loss: (-0.072)(R 0.667, F -0.811)]  [G loss: 1.017] \n",
      "1057 [D loss: (-0.057)(R 0.698, F -0.813)]  [G loss: 1.047] \n",
      "1058 [D loss: (-0.062)(R 0.718, F -0.843)]  [G loss: 1.075] \n",
      "1059 [D loss: (-0.056)(R 0.705, F -0.817)]  [G loss: 1.071] \n",
      "1060 [D loss: (-0.052)(R 0.765, F -0.868)]  [G loss: 1.107] \n",
      "1061 [D loss: (-0.054)(R 0.781, F -0.889)]  [G loss: 1.133] \n",
      "1062 [D loss: (-0.068)(R 0.748, F -0.884)]  [G loss: 1.100] \n",
      "1063 [D loss: (-0.054)(R 0.774, F -0.882)]  [G loss: 1.148] \n",
      "1064 [D loss: (-0.036)(R 0.824, F -0.896)]  [G loss: 1.132] \n",
      "1065 [D loss: (-0.051)(R 0.782, F -0.885)]  [G loss: 1.132] \n",
      "1066 [D loss: (-0.050)(R 0.801, F -0.900)]  [G loss: 1.139] \n",
      "1067 [D loss: (-0.055)(R 0.759, F -0.870)]  [G loss: 1.111] \n",
      "1068 [D loss: (-0.052)(R 0.739, F -0.844)]  [G loss: 1.064] \n",
      "1069 [D loss: (-0.096)(R 0.666, F -0.859)]  [G loss: 1.030] \n",
      "1070 [D loss: (-0.070)(R 0.672, F -0.813)]  [G loss: 1.024] \n",
      "1071 [D loss: (-0.063)(R 0.647, F -0.773)]  [G loss: 0.983] \n",
      "1072 [D loss: (-0.071)(R 0.619, F -0.762)]  [G loss: 0.973] \n",
      "1073 [D loss: (-0.137)(R 0.704, F -0.977)]  [G loss: 1.127] \n",
      "1074 [D loss: (-0.077)(R 0.668, F -0.823)]  [G loss: 0.941] \n",
      "1075 [D loss: (-0.120)(R 0.612, F -0.852)]  [G loss: 1.079] \n",
      "1076 [D loss: (-0.061)(R 0.596, F -0.718)]  [G loss: 0.927] \n",
      "1077 [D loss: (-0.092)(R 0.529, F -0.713)]  [G loss: 0.878] \n",
      "1078 [D loss: (-0.034)(R 0.607, F -0.674)]  [G loss: 0.869] \n",
      "1079 [D loss: (-0.083)(R 0.483, F -0.650)]  [G loss: 0.846] \n",
      "1080 [D loss: (-0.056)(R 0.492, F -0.604)]  [G loss: 0.839] \n",
      "1081 [D loss: (-0.065)(R 0.505, F -0.634)]  [G loss: 0.879] \n",
      "1082 [D loss: (-0.087)(R 0.495, F -0.669)]  [G loss: 0.884] \n",
      "1083 [D loss: (-0.102)(R 0.464, F -0.668)]  [G loss: 0.848] \n",
      "1084 [D loss: (-0.073)(R 0.535, F -0.682)]  [G loss: 0.866] \n",
      "1085 [D loss: (-0.110)(R 0.449, F -0.669)]  [G loss: 0.853] \n",
      "1086 [D loss: (-0.075)(R 0.506, F -0.656)]  [G loss: 0.839] \n",
      "1087 [D loss: (-0.081)(R 0.565, F -0.726)]  [G loss: 0.930] \n",
      "1088 [D loss: (-0.093)(R 0.553, F -0.738)]  [G loss: 0.923] \n",
      "1089 [D loss: (-0.072)(R 0.623, F -0.766)]  [G loss: 0.959] \n",
      "1090 [D loss: (-0.057)(R 0.588, F -0.701)]  [G loss: 0.951] \n",
      "1091 [D loss: (-0.053)(R 0.637, F -0.743)]  [G loss: 0.943] \n",
      "1092 [D loss: (-0.084)(R 0.631, F -0.799)]  [G loss: 0.965] \n",
      "1093 [D loss: (-0.066)(R 0.654, F -0.785)]  [G loss: 0.995] \n",
      "1094 [D loss: (-0.096)(R 0.685, F -0.877)]  [G loss: 1.005] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1095 [D loss: (-0.140)(R 0.673, F -0.953)]  [G loss: 1.125] \n",
      "1096 [D loss: (-0.070)(R 0.707, F -0.847)]  [G loss: 1.052] \n",
      "1097 [D loss: (-0.060)(R 0.743, F -0.863)]  [G loss: 1.090] \n",
      "1098 [D loss: (-0.053)(R 0.749, F -0.855)]  [G loss: 1.097] \n",
      "1099 [D loss: (-0.071)(R 0.783, F -0.925)]  [G loss: 1.164] \n",
      "1100 [D loss: (-0.084)(R 0.820, F -0.989)]  [G loss: 1.168] \n",
      "1101 [D loss: (-0.111)(R 0.775, F -0.997)]  [G loss: 1.205] \n",
      "1102 [D loss: (-0.081)(R 0.811, F -0.974)]  [G loss: 1.164] \n",
      "1103 [D loss: (-0.040)(R 0.878, F -0.959)]  [G loss: 1.144] \n",
      "1104 [D loss: (-0.079)(R 0.738, F -0.895)]  [G loss: 1.092] \n",
      "1105 [D loss: (-0.065)(R 0.713, F -0.843)]  [G loss: 1.075] \n",
      "1106 [D loss: (-0.097)(R 0.686, F -0.880)]  [G loss: 1.072] \n",
      "1107 [D loss: (-0.068)(R 0.705, F -0.841)]  [G loss: 1.040] \n",
      "1108 [D loss: (-0.045)(R 0.678, F -0.769)]  [G loss: 0.995] \n",
      "1109 [D loss: (-0.056)(R 0.672, F -0.783)]  [G loss: 1.003] \n",
      "1110 [D loss: (-0.107)(R 0.609, F -0.822)]  [G loss: 0.992] \n",
      "1111 [D loss: (-0.070)(R 0.641, F -0.781)]  [G loss: 1.017] \n",
      "1112 [D loss: (-0.090)(R 0.610, F -0.789)]  [G loss: 1.007] \n",
      "1113 [D loss: (-0.092)(R 0.615, F -0.799)]  [G loss: 1.045] \n",
      "1114 [D loss: (-0.107)(R 0.583, F -0.797)]  [G loss: 1.002] \n",
      "1115 [D loss: (-0.073)(R 0.616, F -0.763)]  [G loss: 0.953] \n",
      "1116 [D loss: (-0.067)(R 0.589, F -0.723)]  [G loss: 0.918] \n",
      "1117 [D loss: (-0.046)(R 0.600, F -0.691)]  [G loss: 0.908] \n",
      "1118 [D loss: (-0.047)(R 0.619, F -0.714)]  [G loss: 0.946] \n",
      "1119 [D loss: (-0.056)(R 0.611, F -0.723)]  [G loss: 0.960] \n",
      "1120 [D loss: (-0.033)(R 0.633, F -0.700)]  [G loss: 0.941] \n",
      "1121 [D loss: (-0.086)(R 0.579, F -0.751)]  [G loss: 0.948] \n",
      "1122 [D loss: (-0.063)(R 0.584, F -0.710)]  [G loss: 0.940] \n",
      "1123 [D loss: (-0.073)(R 0.668, F -0.814)]  [G loss: 0.900] \n",
      "1124 [D loss: (-0.112)(R 0.598, F -0.823)]  [G loss: 1.028] \n",
      "1125 [D loss: (-0.048)(R 0.659, F -0.755)]  [G loss: 1.001] \n",
      "1126 [D loss: (-0.081)(R 0.655, F -0.816)]  [G loss: 1.008] \n",
      "1127 [D loss: (-0.088)(R 0.649, F -0.826)]  [G loss: 1.002] \n",
      "1128 [D loss: (-0.090)(R 0.643, F -0.822)]  [G loss: 1.039] \n",
      "1129 [D loss: (-0.069)(R 0.699, F -0.836)]  [G loss: 1.078] \n",
      "1130 [D loss: (-0.058)(R 0.725, F -0.841)]  [G loss: 1.053] \n",
      "1131 [D loss: (-0.073)(R 0.666, F -0.812)]  [G loss: 1.048] \n",
      "1132 [D loss: (-0.079)(R 0.675, F -0.833)]  [G loss: 1.064] \n",
      "1133 [D loss: (-0.092)(R 0.651, F -0.835)]  [G loss: 1.054] \n",
      "1134 [D loss: (-0.137)(R 0.566, F -0.839)]  [G loss: 1.078] \n",
      "1135 [D loss: (-0.113)(R 0.626, F -0.852)]  [G loss: 1.055] \n",
      "1136 [D loss: (-0.091)(R 0.661, F -0.843)]  [G loss: 1.035] \n",
      "1137 [D loss: (-0.039)(R 0.675, F -0.752)]  [G loss: 1.005] \n",
      "1138 [D loss: (-0.053)(R 0.670, F -0.775)]  [G loss: 1.025] \n",
      "1139 [D loss: (-0.054)(R 0.655, F -0.762)]  [G loss: 0.989] \n",
      "1140 [D loss: (-0.071)(R 0.614, F -0.755)]  [G loss: 0.964] \n",
      "1141 [D loss: (-0.069)(R 0.578, F -0.715)]  [G loss: 0.934] \n",
      "1142 [D loss: (-0.066)(R 0.633, F -0.765)]  [G loss: 0.969] \n",
      "1143 [D loss: (-0.104)(R 0.653, F -0.860)]  [G loss: 0.983] \n",
      "1144 [D loss: (-0.106)(R 0.650, F -0.861)]  [G loss: 1.091] \n",
      "1145 [D loss: (-0.085)(R 0.648, F -0.817)]  [G loss: 1.048] \n",
      "1146 [D loss: (-0.042)(R 0.736, F -0.819)]  [G loss: 1.054] \n",
      "1147 [D loss: (-0.080)(R 0.670, F -0.831)]  [G loss: 1.025] \n",
      "1148 [D loss: (-0.083)(R 0.642, F -0.808)]  [G loss: 1.049] \n",
      "1149 [D loss: (-0.086)(R 0.611, F -0.782)]  [G loss: 1.031] \n",
      "1150 [D loss: (-0.077)(R 0.651, F -0.805)]  [G loss: 1.026] \n",
      "1151 [D loss: (-0.123)(R 0.687, F -0.933)]  [G loss: 1.072] \n",
      "1152 [D loss: (-0.084)(R 0.690, F -0.858)]  [G loss: 1.080] \n",
      "1153 [D loss: (-0.083)(R 0.684, F -0.850)]  [G loss: 1.078] \n",
      "1154 [D loss: (-0.090)(R 0.696, F -0.876)]  [G loss: 1.055] \n",
      "1155 [D loss: (-0.080)(R 0.661, F -0.821)]  [G loss: 1.044] \n",
      "1156 [D loss: (-0.068)(R 0.660, F -0.797)]  [G loss: 0.976] \n",
      "1157 [D loss: (-0.114)(R 0.562, F -0.791)]  [G loss: 0.960] \n",
      "1158 [D loss: (-0.059)(R 0.632, F -0.751)]  [G loss: 0.983] \n",
      "1159 [D loss: (-0.062)(R 0.591, F -0.715)]  [G loss: 0.935] \n",
      "1160 [D loss: (-0.066)(R 0.575, F -0.707)]  [G loss: 0.934] \n",
      "1161 [D loss: (-0.090)(R 0.553, F -0.733)]  [G loss: 0.935] \n",
      "1162 [D loss: (-0.043)(R 0.598, F -0.685)]  [G loss: 0.915] \n",
      "1163 [D loss: (-0.085)(R 0.614, F -0.785)]  [G loss: 0.973] \n",
      "1164 [D loss: (-0.063)(R 0.677, F -0.802)]  [G loss: 0.985] \n",
      "1165 [D loss: (-0.107)(R 0.638, F -0.853)]  [G loss: 0.979] \n",
      "1166 [D loss: (-0.068)(R 0.601, F -0.737)]  [G loss: 0.992] \n",
      "1167 [D loss: (-0.104)(R 0.602, F -0.809)]  [G loss: 1.023] \n",
      "1168 [D loss: (-0.069)(R 0.654, F -0.792)]  [G loss: 1.009] \n",
      "1169 [D loss: (-0.130)(R 0.596, F -0.857)]  [G loss: 1.001] \n",
      "1170 [D loss: (-0.088)(R 0.662, F -0.838)]  [G loss: 1.033] \n",
      "1171 [D loss: (-0.091)(R 0.653, F -0.835)]  [G loss: 1.067] \n",
      "1172 [D loss: (-0.055)(R 0.713, F -0.824)]  [G loss: 1.057] \n",
      "1173 [D loss: (-0.092)(R 0.649, F -0.833)]  [G loss: 1.042] \n",
      "1174 [D loss: (-0.086)(R 0.653, F -0.825)]  [G loss: 1.079] \n",
      "1175 [D loss: (-0.093)(R 0.639, F -0.826)]  [G loss: 1.042] \n",
      "1176 [D loss: (-0.105)(R 0.601, F -0.812)]  [G loss: 0.987] \n",
      "1177 [D loss: (-0.041)(R 0.698, F -0.779)]  [G loss: 1.019] \n",
      "1178 [D loss: (-0.059)(R 0.601, F -0.718)]  [G loss: 0.947] \n",
      "1179 [D loss: (-0.093)(R 0.630, F -0.816)]  [G loss: 1.006] \n",
      "1180 [D loss: (-0.058)(R 0.635, F -0.750)]  [G loss: 1.025] \n",
      "1181 [D loss: (-0.101)(R 0.662, F -0.865)]  [G loss: 1.046] \n",
      "1182 [D loss: (-0.093)(R 0.652, F -0.838)]  [G loss: 1.074] \n",
      "1183 [D loss: (-0.063)(R 0.703, F -0.829)]  [G loss: 1.080] \n",
      "1184 [D loss: (-0.089)(R 0.703, F -0.881)]  [G loss: 1.076] \n",
      "1185 [D loss: (-0.096)(R 0.744, F -0.936)]  [G loss: 1.121] \n",
      "1186 [D loss: (-0.108)(R 0.703, F -0.919)]  [G loss: 1.108] \n",
      "1187 [D loss: (-0.092)(R 0.650, F -0.834)]  [G loss: 1.090] \n",
      "1188 [D loss: (-0.114)(R 0.625, F -0.854)]  [G loss: 1.071] \n",
      "1189 [D loss: (-0.108)(R 0.627, F -0.842)]  [G loss: 1.022] \n",
      "1190 [D loss: (-0.085)(R 0.637, F -0.807)]  [G loss: 0.983] \n",
      "1191 [D loss: (-0.082)(R 0.622, F -0.787)]  [G loss: 0.962] \n",
      "1192 [D loss: (-0.078)(R 0.693, F -0.848)]  [G loss: 1.141] \n",
      "1193 [D loss: (-0.126)(R 0.636, F -0.887)]  [G loss: 1.124] \n",
      "1194 [D loss: (-0.100)(R 0.679, F -0.880)]  [G loss: 1.151] \n",
      "1195 [D loss: (-0.095)(R 0.728, F -0.919)]  [G loss: 1.139] \n",
      "1196 [D loss: (-0.080)(R 0.767, F -0.928)]  [G loss: 1.162] \n",
      "1197 [D loss: (-0.030)(R 0.857, F -0.918)]  [G loss: 1.058] \n",
      "1198 [D loss: (-0.067)(R 0.723, F -0.856)]  [G loss: 1.127] \n",
      "1199 [D loss: (-0.083)(R 0.757, F -0.924)]  [G loss: 1.155] \n",
      "1200 [D loss: (-0.076)(R 0.692, F -0.843)]  [G loss: 1.048] \n",
      "1201 [D loss: (-0.073)(R 0.641, F -0.787)]  [G loss: 1.004] \n",
      "1202 [D loss: (-0.063)(R 0.667, F -0.793)]  [G loss: 0.975] \n",
      "1203 [D loss: (-0.073)(R 0.566, F -0.713)]  [G loss: 0.931] \n",
      "1204 [D loss: (-0.119)(R 0.546, F -0.783)]  [G loss: 0.973] \n",
      "1205 [D loss: (-0.085)(R 0.601, F -0.770)]  [G loss: 0.993] \n",
      "1206 [D loss: (-0.130)(R 0.559, F -0.818)]  [G loss: 1.007] \n",
      "1207 [D loss: (-0.069)(R 0.677, F -0.815)]  [G loss: 1.077] \n",
      "1208 [D loss: (-0.112)(R 0.661, F -0.885)]  [G loss: 1.096] \n",
      "1209 [D loss: (-0.109)(R 0.712, F -0.930)]  [G loss: 1.173] \n",
      "1210 [D loss: (-0.120)(R 0.713, F -0.954)]  [G loss: 1.159] \n",
      "1211 [D loss: (-0.101)(R 0.693, F -0.895)]  [G loss: 1.119] \n",
      "1212 [D loss: (-0.124)(R 0.631, F -0.880)]  [G loss: 1.103] \n",
      "1213 [D loss: (-0.157)(R 0.618, F -0.932)]  [G loss: 1.133] \n",
      "1214 [D loss: (-0.145)(R 0.616, F -0.906)]  [G loss: 1.148] \n",
      "1215 [D loss: (-0.065)(R 0.678, F -0.809)]  [G loss: 1.065] \n",
      "1216 [D loss: (-0.093)(R 0.695, F -0.882)]  [G loss: 1.100] \n",
      "1217 [D loss: (-0.085)(R 0.727, F -0.896)]  [G loss: 1.076] \n",
      "1218 [D loss: (-0.090)(R 0.696, F -0.876)]  [G loss: 1.103] \n",
      "1219 [D loss: (-0.088)(R 0.681, F -0.857)]  [G loss: 1.103] \n",
      "1220 [D loss: (-0.067)(R 0.758, F -0.892)]  [G loss: 1.128] \n",
      "1221 [D loss: (-0.108)(R 0.664, F -0.881)]  [G loss: 1.122] \n",
      "1222 [D loss: (-0.085)(R 0.767, F -0.938)]  [G loss: 1.191] \n",
      "1223 [D loss: (-0.113)(R 0.731, F -0.956)]  [G loss: 1.142] \n",
      "1224 [D loss: (-0.090)(R 0.737, F -0.916)]  [G loss: 1.144] \n",
      "1225 [D loss: (-0.105)(R 0.731, F -0.941)]  [G loss: 1.124] \n",
      "1226 [D loss: (-0.080)(R 0.728, F -0.888)]  [G loss: 1.193] \n",
      "1227 [D loss: (-0.086)(R 0.751, F -0.923)]  [G loss: 1.099] \n",
      "1228 [D loss: (-0.106)(R 0.701, F -0.914)]  [G loss: 1.160] \n",
      "1229 [D loss: (-0.111)(R 0.736, F -0.958)]  [G loss: 1.245] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1230 [D loss: (-0.116)(R 0.740, F -0.971)]  [G loss: 1.190] \n",
      "1231 [D loss: (-0.104)(R 0.784, F -0.991)]  [G loss: 1.216] \n",
      "1232 [D loss: (-0.131)(R 0.719, F -0.982)]  [G loss: 1.159] \n",
      "1233 [D loss: (-0.065)(R 0.742, F -0.872)]  [G loss: 1.096] \n",
      "1234 [D loss: (-0.091)(R 0.707, F -0.889)]  [G loss: 1.123] \n",
      "1235 [D loss: (-0.091)(R 0.635, F -0.816)]  [G loss: 1.069] \n",
      "1236 [D loss: (-0.097)(R 0.590, F -0.783)]  [G loss: 0.999] \n",
      "1237 [D loss: (-0.089)(R 0.593, F -0.770)]  [G loss: 0.978] \n",
      "1238 [D loss: (-0.101)(R 0.581, F -0.784)]  [G loss: 1.042] \n",
      "1239 [D loss: (-0.071)(R 0.609, F -0.751)]  [G loss: 0.978] \n",
      "1240 [D loss: (-0.090)(R 0.540, F -0.720)]  [G loss: 0.975] \n",
      "1241 [D loss: (-0.105)(R 0.556, F -0.767)]  [G loss: 1.013] \n",
      "1242 [D loss: (-0.098)(R 0.614, F -0.811)]  [G loss: 1.038] \n",
      "1243 [D loss: (-0.107)(R 0.683, F -0.897)]  [G loss: 1.084] \n",
      "1244 [D loss: (-0.098)(R 0.684, F -0.881)]  [G loss: 1.106] \n",
      "1245 [D loss: (-0.091)(R 0.689, F -0.872)]  [G loss: 1.112] \n",
      "1246 [D loss: (-0.107)(R 0.604, F -0.818)]  [G loss: 1.057] \n",
      "1247 [D loss: (-0.072)(R 0.696, F -0.840)]  [G loss: 1.049] \n",
      "1248 [D loss: (-0.075)(R 0.636, F -0.786)]  [G loss: 1.041] \n",
      "1249 [D loss: (-0.083)(R 0.623, F -0.789)]  [G loss: 1.039] \n",
      "1250 [D loss: (-0.145)(R 0.601, F -0.891)]  [G loss: 1.120] \n",
      "1251 [D loss: (-0.054)(R 0.710, F -0.818)]  [G loss: 1.069] \n",
      "1252 [D loss: (-0.093)(R 0.640, F -0.825)]  [G loss: 1.021] \n",
      "1253 [D loss: (-0.072)(R 0.657, F -0.801)]  [G loss: 1.024] \n",
      "1254 [D loss: (-0.072)(R 0.627, F -0.772)]  [G loss: 1.009] \n",
      "1255 [D loss: (-0.079)(R 0.631, F -0.789)]  [G loss: 1.044] \n",
      "1256 [D loss: (-0.088)(R 0.649, F -0.825)]  [G loss: 1.037] \n",
      "1257 [D loss: (-0.081)(R 0.663, F -0.824)]  [G loss: 1.029] \n",
      "1258 [D loss: (-0.110)(R 0.648, F -0.868)]  [G loss: 1.152] \n",
      "1259 [D loss: (-0.113)(R 0.674, F -0.900)]  [G loss: 1.101] \n",
      "1260 [D loss: (-0.087)(R 0.698, F -0.872)]  [G loss: 1.150] \n",
      "1261 [D loss: (-0.087)(R 0.729, F -0.903)]  [G loss: 1.148] \n",
      "1262 [D loss: (-0.112)(R 0.700, F -0.925)]  [G loss: 1.118] \n",
      "1263 [D loss: (-0.075)(R 0.741, F -0.890)]  [G loss: 1.197] \n",
      "1264 [D loss: (-0.090)(R 0.731, F -0.911)]  [G loss: 1.141] \n",
      "1265 [D loss: (-0.079)(R 0.755, F -0.912)]  [G loss: 1.180] \n",
      "1266 [D loss: (-0.053)(R 0.865, F -0.971)]  [G loss: 1.270] \n",
      "1267 [D loss: (-0.100)(R 0.772, F -0.973)]  [G loss: 1.208] \n",
      "1268 [D loss: (-0.095)(R 0.761, F -0.950)]  [G loss: 1.216] \n",
      "1269 [D loss: (-0.048)(R 0.802, F -0.899)]  [G loss: 1.168] \n",
      "1270 [D loss: (-0.053)(R 0.776, F -0.883)]  [G loss: 1.166] \n",
      "1271 [D loss: (-0.074)(R 0.738, F -0.885)]  [G loss: 1.124] \n",
      "1272 [D loss: (-0.193)(R 0.734, F -1.121)]  [G loss: 1.083] \n",
      "1273 [D loss: (-0.094)(R 0.737, F -0.925)]  [G loss: 1.180] \n",
      "1274 [D loss: (-0.117)(R 0.674, F -0.908)]  [G loss: 1.164] \n",
      "1275 [D loss: (-0.092)(R 0.708, F -0.892)]  [G loss: 1.152] \n",
      "1276 [D loss: (-0.118)(R 0.658, F -0.895)]  [G loss: 1.143] \n",
      "1277 [D loss: (-0.184)(R 0.689, F -1.056)]  [G loss: 1.157] \n",
      "1278 [D loss: (-0.082)(R 0.660, F -0.824)]  [G loss: 1.102] \n",
      "1279 [D loss: (-0.084)(R 0.643, F -0.812)]  [G loss: 1.042] \n",
      "1280 [D loss: (-0.094)(R 0.651, F -0.840)]  [G loss: 1.080] \n",
      "1281 [D loss: (-0.095)(R 0.595, F -0.786)]  [G loss: 1.018] \n",
      "1282 [D loss: (-0.092)(R 0.626, F -0.811)]  [G loss: 1.024] \n",
      "1283 [D loss: (-0.121)(R 0.560, F -0.801)]  [G loss: 1.045] \n",
      "1284 [D loss: (-0.085)(R 0.638, F -0.809)]  [G loss: 1.028] \n",
      "1285 [D loss: (-0.096)(R 0.593, F -0.785)]  [G loss: 0.990] \n",
      "1286 [D loss: (-0.085)(R 0.608, F -0.778)]  [G loss: 1.015] \n",
      "1287 [D loss: (-0.083)(R 0.624, F -0.789)]  [G loss: 1.048] \n",
      "1288 [D loss: (-0.073)(R 0.657, F -0.803)]  [G loss: 1.074] \n",
      "1289 [D loss: (-0.099)(R 0.655, F -0.852)]  [G loss: 1.082] \n",
      "1290 [D loss: (-0.088)(R 0.693, F -0.869)]  [G loss: 1.105] \n",
      "1291 [D loss: (-0.115)(R 0.637, F -0.867)]  [G loss: 1.049] \n",
      "1292 [D loss: (-0.119)(R 0.665, F -0.902)]  [G loss: 1.061] \n",
      "1293 [D loss: (-0.098)(R 0.694, F -0.889)]  [G loss: 1.120] \n",
      "1294 [D loss: (-0.079)(R 0.747, F -0.906)]  [G loss: 1.135] \n",
      "1295 [D loss: (-0.063)(R 0.802, F -0.928)]  [G loss: 1.277] \n",
      "1296 [D loss: (-0.086)(R 0.767, F -0.940)]  [G loss: 1.212] \n",
      "1297 [D loss: (-0.090)(R 0.781, F -0.960)]  [G loss: 1.225] \n",
      "1298 [D loss: (-0.079)(R 0.786, F -0.944)]  [G loss: 1.213] \n",
      "1299 [D loss: (-0.077)(R 0.770, F -0.924)]  [G loss: 1.178] \n",
      "1300 [D loss: (-0.098)(R 0.726, F -0.922)]  [G loss: 1.174] \n",
      "1301 [D loss: (-0.068)(R 0.747, F -0.883)]  [G loss: 1.162] \n",
      "1302 [D loss: (-0.077)(R 0.717, F -0.871)]  [G loss: 1.134] \n",
      "1303 [D loss: (-0.076)(R 0.730, F -0.881)]  [G loss: 1.128] \n",
      "1304 [D loss: (-0.072)(R 0.694, F -0.839)]  [G loss: 1.146] \n",
      "1305 [D loss: (-0.074)(R 0.685, F -0.833)]  [G loss: 1.101] \n",
      "1306 [D loss: (-0.076)(R 0.653, F -0.805)]  [G loss: 1.081] \n",
      "1307 [D loss: (-0.085)(R 0.689, F -0.858)]  [G loss: 1.102] \n",
      "1308 [D loss: (-0.104)(R 0.700, F -0.909)]  [G loss: 1.097] \n",
      "1309 [D loss: (-0.067)(R 0.694, F -0.829)]  [G loss: 1.134] \n",
      "1310 [D loss: (-0.099)(R 0.651, F -0.848)]  [G loss: 1.107] \n",
      "1311 [D loss: (-0.071)(R 0.687, F -0.828)]  [G loss: 1.085] \n",
      "1312 [D loss: (-0.078)(R 0.675, F -0.831)]  [G loss: 1.076] \n",
      "1313 [D loss: (-0.116)(R 0.647, F -0.880)]  [G loss: 1.021] \n",
      "1314 [D loss: (-0.133)(R 0.655, F -0.920)]  [G loss: 1.165] \n",
      "1315 [D loss: (-0.053)(R 0.770, F -0.876)]  [G loss: 1.126] \n",
      "1316 [D loss: (-0.080)(R 0.681, F -0.840)]  [G loss: 1.115] \n",
      "1317 [D loss: (-0.087)(R 0.662, F -0.836)]  [G loss: 1.153] \n",
      "1318 [D loss: (-0.163)(R 0.716, F -1.043)]  [G loss: 1.127] \n",
      "1319 [D loss: (-0.148)(R 0.661, F -0.956)]  [G loss: 1.211] \n",
      "1320 [D loss: (-0.115)(R 0.657, F -0.887)]  [G loss: 1.078] \n",
      "1321 [D loss: (-0.151)(R 0.656, F -0.959)]  [G loss: 1.119] \n",
      "1322 [D loss: (-0.101)(R 0.674, F -0.877)]  [G loss: 1.131] \n",
      "1323 [D loss: (-0.094)(R 0.680, F -0.868)]  [G loss: 1.127] \n",
      "1324 [D loss: (-0.075)(R 0.716, F -0.866)]  [G loss: 1.143] \n",
      "1325 [D loss: (-0.077)(R 0.739, F -0.892)]  [G loss: 1.145] \n",
      "1326 [D loss: (-0.088)(R 0.729, F -0.904)]  [G loss: 1.153] \n",
      "1327 [D loss: (-0.081)(R 0.727, F -0.889)]  [G loss: 1.154] \n",
      "1328 [D loss: (-0.073)(R 0.726, F -0.872)]  [G loss: 1.112] \n",
      "1329 [D loss: (-0.084)(R 0.674, F -0.842)]  [G loss: 1.110] \n",
      "1330 [D loss: (-0.074)(R 0.720, F -0.868)]  [G loss: 1.143] \n",
      "1331 [D loss: (-0.092)(R 0.705, F -0.890)]  [G loss: 1.165] \n",
      "1332 [D loss: (-0.139)(R 0.661, F -0.938)]  [G loss: 1.106] \n",
      "1333 [D loss: (-0.142)(R 0.657, F -0.941)]  [G loss: 1.225] \n",
      "1334 [D loss: (-0.111)(R 0.671, F -0.893)]  [G loss: 1.147] \n",
      "1335 [D loss: (-0.067)(R 0.721, F -0.856)]  [G loss: 1.121] \n",
      "1336 [D loss: (-0.089)(R 0.673, F -0.852)]  [G loss: 1.120] \n",
      "1337 [D loss: (-0.102)(R 0.701, F -0.905)]  [G loss: 1.178] \n",
      "1338 [D loss: (-0.142)(R 0.682, F -0.967)]  [G loss: 1.114] \n",
      "1339 [D loss: (-0.073)(R 0.645, F -0.791)]  [G loss: 1.046] \n",
      "1340 [D loss: (-0.069)(R 0.649, F -0.787)]  [G loss: 1.096] \n",
      "1341 [D loss: (-0.091)(R 0.650, F -0.832)]  [G loss: 1.069] \n",
      "1342 [D loss: (-0.085)(R 0.687, F -0.856)]  [G loss: 1.086] \n",
      "1343 [D loss: (-0.054)(R 0.726, F -0.833)]  [G loss: 1.113] \n",
      "1344 [D loss: (-0.096)(R 0.695, F -0.887)]  [G loss: 1.084] \n",
      "1345 [D loss: (-0.093)(R 0.644, F -0.830)]  [G loss: 1.043] \n",
      "1346 [D loss: (-0.081)(R 0.656, F -0.818)]  [G loss: 1.074] \n",
      "1347 [D loss: (-0.117)(R 0.697, F -0.931)]  [G loss: 1.163] \n",
      "1348 [D loss: (-0.114)(R 0.739, F -0.967)]  [G loss: 1.219] \n",
      "1349 [D loss: (-0.131)(R 0.751, F -1.013)]  [G loss: 1.223] \n",
      "1350 [D loss: (-0.029)(R 0.846, F -0.904)]  [G loss: 1.223] \n",
      "1351 [D loss: (-0.112)(R 0.719, F -0.944)]  [G loss: 1.199] \n",
      "1352 [D loss: (-0.080)(R 0.792, F -0.953)]  [G loss: 1.233] \n",
      "1353 [D loss: (-0.055)(R 0.816, F -0.926)]  [G loss: 1.161] \n",
      "1354 [D loss: (-0.074)(R 0.773, F -0.920)]  [G loss: 1.189] \n",
      "1355 [D loss: (-0.128)(R 0.677, F -0.933)]  [G loss: 1.199] \n",
      "1356 [D loss: (-0.071)(R 0.779, F -0.922)]  [G loss: 1.181] \n",
      "1357 [D loss: (-0.053)(R 0.793, F -0.900)]  [G loss: 1.153] \n",
      "1358 [D loss: (-0.071)(R 0.757, F -0.899)]  [G loss: 1.147] \n",
      "1359 [D loss: (-0.042)(R 0.807, F -0.891)]  [G loss: 1.165] \n",
      "1360 [D loss: (-0.046)(R 0.797, F -0.890)]  [G loss: 1.162] \n",
      "1361 [D loss: (-0.087)(R 0.736, F -0.911)]  [G loss: 1.145] \n",
      "1362 [D loss: (-0.090)(R 0.719, F -0.899)]  [G loss: 1.150] \n",
      "1363 [D loss: (-0.092)(R 0.738, F -0.921)]  [G loss: 1.164] \n",
      "1364 [D loss: (-0.124)(R 0.679, F -0.926)]  [G loss: 1.201] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1365 [D loss: (-0.127)(R 0.660, F -0.915)]  [G loss: 1.072] \n",
      "1366 [D loss: (-0.079)(R 0.694, F -0.851)]  [G loss: 1.103] \n",
      "1367 [D loss: (-0.068)(R 0.726, F -0.863)]  [G loss: 1.131] \n",
      "1368 [D loss: (-0.064)(R 0.701, F -0.828)]  [G loss: 1.110] \n",
      "1369 [D loss: (-0.067)(R 0.693, F -0.827)]  [G loss: 1.089] \n",
      "1370 [D loss: (-0.062)(R 0.670, F -0.795)]  [G loss: 1.053] \n",
      "1371 [D loss: (-0.082)(R 0.625, F -0.789)]  [G loss: 1.018] \n",
      "1372 [D loss: (-0.104)(R 0.693, F -0.901)]  [G loss: 1.082] \n",
      "1373 [D loss: (-0.065)(R 0.678, F -0.807)]  [G loss: 1.073] \n",
      "1374 [D loss: (-0.102)(R 0.616, F -0.820)]  [G loss: 1.070] \n",
      "1375 [D loss: (-0.048)(R 0.735, F -0.831)]  [G loss: 1.020] \n",
      "1376 [D loss: (-0.068)(R 0.658, F -0.794)]  [G loss: 1.070] \n",
      "1377 [D loss: (-0.062)(R 0.683, F -0.807)]  [G loss: 1.054] \n",
      "1378 [D loss: (-0.067)(R 0.670, F -0.804)]  [G loss: 1.070] \n",
      "1379 [D loss: (-0.099)(R 0.627, F -0.824)]  [G loss: 1.054] \n",
      "1380 [D loss: (-0.121)(R 0.634, F -0.876)]  [G loss: 1.091] \n",
      "1381 [D loss: (-0.061)(R 0.673, F -0.794)]  [G loss: 1.115] \n",
      "1382 [D loss: (-0.081)(R 0.724, F -0.887)]  [G loss: 1.146] \n",
      "1383 [D loss: (-0.100)(R 0.707, F -0.907)]  [G loss: 1.094] \n",
      "1384 [D loss: (-0.087)(R 0.745, F -0.919)]  [G loss: 1.150] \n",
      "1385 [D loss: (-0.066)(R 0.722, F -0.855)]  [G loss: 1.117] \n",
      "1386 [D loss: (-0.056)(R 0.737, F -0.850)]  [G loss: 1.117] \n",
      "1387 [D loss: (-0.098)(R 0.777, F -0.973)]  [G loss: 1.154] \n",
      "1388 [D loss: (-0.057)(R 0.811, F -0.925)]  [G loss: 1.211] \n",
      "1389 [D loss: (-0.070)(R 0.781, F -0.921)]  [G loss: 1.189] \n",
      "1390 [D loss: (-0.099)(R 0.739, F -0.938)]  [G loss: 1.160] \n",
      "1391 [D loss: (-0.073)(R 0.731, F -0.877)]  [G loss: 1.128] \n",
      "1392 [D loss: (-0.097)(R 0.711, F -0.906)]  [G loss: 1.118] \n",
      "1393 [D loss: (-0.085)(R 0.765, F -0.934)]  [G loss: 1.197] \n",
      "1394 [D loss: (-0.065)(R 0.724, F -0.854)]  [G loss: 1.148] \n",
      "1395 [D loss: (-0.119)(R 0.688, F -0.925)]  [G loss: 1.175] \n",
      "1396 [D loss: (-0.116)(R 0.883, F -1.116)]  [G loss: 1.153] \n",
      "1397 [D loss: (-0.083)(R 0.693, F -0.860)]  [G loss: 1.122] \n",
      "1398 [D loss: (-0.072)(R 0.683, F -0.826)]  [G loss: 1.070] \n",
      "1399 [D loss: (-0.084)(R 0.673, F -0.841)]  [G loss: 1.129] \n",
      "1400 [D loss: (-0.064)(R 0.683, F -0.812)]  [G loss: 1.067] \n",
      "1401 [D loss: (-0.075)(R 0.638, F -0.788)]  [G loss: 1.070] \n",
      "1402 [D loss: (-0.062)(R 0.711, F -0.835)]  [G loss: 1.072] \n",
      "1403 [D loss: (-0.077)(R 0.664, F -0.819)]  [G loss: 1.063] \n",
      "1404 [D loss: (-0.061)(R 0.717, F -0.838)]  [G loss: 1.109] \n",
      "1405 [D loss: (-0.060)(R 0.733, F -0.853)]  [G loss: 1.133] \n",
      "1406 [D loss: (-0.088)(R 0.707, F -0.883)]  [G loss: 1.141] \n",
      "1407 [D loss: (-0.102)(R 0.704, F -0.907)]  [G loss: 1.162] \n",
      "1408 [D loss: (-0.123)(R 0.731, F -0.977)]  [G loss: 1.200] \n",
      "1409 [D loss: (-0.087)(R 0.742, F -0.915)]  [G loss: 1.170] \n",
      "1410 [D loss: (-0.069)(R 0.746, F -0.884)]  [G loss: 1.154] \n",
      "1411 [D loss: (-0.060)(R 0.773, F -0.893)]  [G loss: 1.164] \n",
      "1412 [D loss: (-0.093)(R 0.755, F -0.940)]  [G loss: 1.210] \n",
      "1413 [D loss: (-0.095)(R 0.692, F -0.883)]  [G loss: 1.160] \n",
      "1414 [D loss: (-0.111)(R 0.720, F -0.942)]  [G loss: 1.138] \n",
      "1415 [D loss: (-0.139)(R 0.699, F -0.977)]  [G loss: 1.220] \n",
      "1416 [D loss: (-0.071)(R 0.726, F -0.867)]  [G loss: 1.133] \n",
      "1417 [D loss: (-0.095)(R 0.692, F -0.883)]  [G loss: 1.154] \n",
      "1418 [D loss: (-0.083)(R 0.760, F -0.927)]  [G loss: 1.152] \n",
      "1419 [D loss: (-0.066)(R 0.707, F -0.840)]  [G loss: 1.134] \n",
      "1420 [D loss: (-0.120)(R 0.628, F -0.869)]  [G loss: 1.113] \n",
      "1421 [D loss: (-0.103)(R 0.712, F -0.919)]  [G loss: 1.129] \n",
      "1422 [D loss: (-0.063)(R 0.735, F -0.862)]  [G loss: 1.140] \n",
      "1423 [D loss: (-0.082)(R 0.709, F -0.874)]  [G loss: 1.109] \n",
      "1424 [D loss: (-0.061)(R 0.718, F -0.839)]  [G loss: 1.107] \n",
      "1425 [D loss: (-0.070)(R 0.688, F -0.828)]  [G loss: 1.094] \n",
      "1426 [D loss: (-0.089)(R 0.622, F -0.800)]  [G loss: 1.056] \n",
      "1427 [D loss: (-0.082)(R 0.621, F -0.785)]  [G loss: 1.068] \n",
      "1428 [D loss: (-0.081)(R 0.641, F -0.803)]  [G loss: 1.072] \n",
      "1429 [D loss: (-0.133)(R 0.657, F -0.923)]  [G loss: 1.147] \n",
      "1430 [D loss: (-0.076)(R 0.702, F -0.854)]  [G loss: 1.114] \n",
      "1431 [D loss: (-0.079)(R 0.715, F -0.872)]  [G loss: 1.137] \n",
      "1432 [D loss: (-0.089)(R 0.688, F -0.866)]  [G loss: 1.132] \n",
      "1433 [D loss: (-0.094)(R 0.717, F -0.906)]  [G loss: 1.096] \n",
      "1434 [D loss: (-0.092)(R 0.778, F -0.961)]  [G loss: 1.195] \n",
      "1435 [D loss: (-0.090)(R 0.750, F -0.929)]  [G loss: 1.210] \n",
      "1436 [D loss: (-0.076)(R 0.740, F -0.891)]  [G loss: 1.144] \n",
      "1437 [D loss: (-0.054)(R 0.736, F -0.845)]  [G loss: 1.126] \n",
      "1438 [D loss: (-0.062)(R 0.714, F -0.837)]  [G loss: 1.123] \n",
      "1439 [D loss: (-0.130)(R 0.667, F -0.928)]  [G loss: 1.165] \n",
      "1440 [D loss: (-0.082)(R 0.672, F -0.836)]  [G loss: 1.104] \n",
      "1441 [D loss: (-0.081)(R 0.675, F -0.837)]  [G loss: 1.085] \n",
      "1442 [D loss: (-0.090)(R 0.718, F -0.897)]  [G loss: 1.125] \n",
      "1443 [D loss: (-0.052)(R 0.713, F -0.818)]  [G loss: 1.071] \n",
      "1444 [D loss: (-0.090)(R 0.657, F -0.837)]  [G loss: 1.077] \n",
      "1445 [D loss: (-0.056)(R 0.679, F -0.791)]  [G loss: 1.071] \n",
      "1446 [D loss: (-0.071)(R 0.693, F -0.835)]  [G loss: 1.062] \n",
      "1447 [D loss: (-0.098)(R 0.689, F -0.886)]  [G loss: 1.101] \n",
      "1448 [D loss: (-0.087)(R 0.674, F -0.849)]  [G loss: 1.101] \n",
      "1449 [D loss: (-0.045)(R 0.704, F -0.795)]  [G loss: 1.101] \n",
      "1450 [D loss: (-0.070)(R 0.659, F -0.800)]  [G loss: 1.047] \n",
      "1451 [D loss: (-0.059)(R 0.655, F -0.773)]  [G loss: 1.043] \n",
      "1452 [D loss: (-0.106)(R 0.646, F -0.858)]  [G loss: 1.129] \n",
      "1453 [D loss: (-0.043)(R 0.760, F -0.846)]  [G loss: 1.139] \n",
      "1454 [D loss: (-0.057)(R 0.668, F -0.782)]  [G loss: 1.052] \n",
      "1455 [D loss: (-0.064)(R 0.655, F -0.783)]  [G loss: 1.036] \n",
      "1456 [D loss: (-0.056)(R 0.689, F -0.802)]  [G loss: 1.035] \n",
      "1457 [D loss: (-0.126)(R 0.679, F -0.932)]  [G loss: 1.110] \n",
      "1458 [D loss: (-0.070)(R 0.727, F -0.868)]  [G loss: 1.182] \n",
      "1459 [D loss: (-0.055)(R 0.713, F -0.823)]  [G loss: 1.103] \n",
      "1460 [D loss: (-0.052)(R 0.716, F -0.820)]  [G loss: 1.041] \n",
      "1461 [D loss: (-0.055)(R 0.671, F -0.782)]  [G loss: 1.026] \n",
      "1462 [D loss: (-0.059)(R 0.665, F -0.784)]  [G loss: 1.053] \n",
      "1463 [D loss: (-0.168)(R 0.652, F -0.989)]  [G loss: 1.163] \n",
      "1464 [D loss: (-0.043)(R 0.736, F -0.822)]  [G loss: 1.129] \n",
      "1465 [D loss: (-0.058)(R 0.714, F -0.829)]  [G loss: 1.092] \n",
      "1466 [D loss: (-0.084)(R 0.683, F -0.851)]  [G loss: 1.041] \n",
      "1467 [D loss: (-0.177)(R 0.685, F -1.038)]  [G loss: 1.232] \n",
      "1468 [D loss: (-0.086)(R 0.759, F -0.931)]  [G loss: 1.147] \n",
      "1469 [D loss: (-0.084)(R 0.719, F -0.887)]  [G loss: 1.147] \n",
      "1470 [D loss: (-0.036)(R 0.761, F -0.834)]  [G loss: 1.100] \n",
      "1471 [D loss: (-0.056)(R 0.729, F -0.841)]  [G loss: 1.091] \n",
      "1472 [D loss: (-0.078)(R 0.699, F -0.855)]  [G loss: 1.038] \n",
      "1473 [D loss: (-0.054)(R 0.699, F -0.807)]  [G loss: 1.090] \n",
      "1474 [D loss: (-0.076)(R 0.675, F -0.826)]  [G loss: 1.056] \n",
      "1475 [D loss: (-0.079)(R 0.702, F -0.860)]  [G loss: 1.067] \n",
      "1476 [D loss: (-0.062)(R 0.667, F -0.790)]  [G loss: 1.017] \n",
      "1477 [D loss: (-0.037)(R 0.703, F -0.777)]  [G loss: 1.043] \n",
      "1478 [D loss: (-0.085)(R 0.641, F -0.811)]  [G loss: 1.002] \n",
      "1479 [D loss: (-0.065)(R 0.672, F -0.802)]  [G loss: 1.022] \n",
      "1480 [D loss: (-0.070)(R 0.597, F -0.737)]  [G loss: 0.981] \n",
      "1481 [D loss: (-0.149)(R 0.575, F -0.872)]  [G loss: 0.990] \n",
      "1482 [D loss: (-0.092)(R 0.658, F -0.841)]  [G loss: 1.064] \n",
      "1483 [D loss: (-0.107)(R 0.647, F -0.861)]  [G loss: 1.065] \n",
      "1484 [D loss: (-0.031)(R 0.725, F -0.786)]  [G loss: 1.069] \n",
      "1485 [D loss: (-0.060)(R 0.660, F -0.780)]  [G loss: 1.017] \n",
      "1486 [D loss: (-0.073)(R 0.587, F -0.734)]  [G loss: 0.990] \n",
      "1487 [D loss: (-0.068)(R 0.612, F -0.749)]  [G loss: 0.972] \n",
      "1488 [D loss: (-0.079)(R 0.610, F -0.767)]  [G loss: 0.984] \n",
      "1489 [D loss: (-0.093)(R 0.559, F -0.745)]  [G loss: 0.947] \n",
      "1490 [D loss: (-0.061)(R 0.621, F -0.744)]  [G loss: 1.006] \n",
      "1491 [D loss: (-0.058)(R 0.630, F -0.746)]  [G loss: 0.980] \n",
      "1492 [D loss: (-0.034)(R 0.660, F -0.728)]  [G loss: 0.991] \n",
      "1493 [D loss: (-0.048)(R 0.641, F -0.737)]  [G loss: 0.971] \n",
      "1494 [D loss: (-0.114)(R 0.616, F -0.844)]  [G loss: 1.005] \n",
      "1495 [D loss: (-0.095)(R 0.603, F -0.793)]  [G loss: 1.063] \n",
      "1496 [D loss: (-0.069)(R 0.631, F -0.768)]  [G loss: 0.987] \n",
      "1497 [D loss: (-0.097)(R 0.592, F -0.786)]  [G loss: 1.027] \n",
      "1498 [D loss: (-0.099)(R 0.741, F -0.938)]  [G loss: 1.087] \n",
      "1499 [D loss: (-0.062)(R 0.702, F -0.825)]  [G loss: 1.048] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500 [D loss: (-0.049)(R 0.716, F -0.814)]  [G loss: 1.055] \n",
      "1501 [D loss: (-0.039)(R 0.737, F -0.815)]  [G loss: 1.056] \n",
      "1502 [D loss: (-0.047)(R 0.673, F -0.768)]  [G loss: 1.017] \n",
      "1503 [D loss: (-0.068)(R 0.627, F -0.762)]  [G loss: 0.953] \n",
      "1504 [D loss: (-0.060)(R 0.639, F -0.758)]  [G loss: 1.012] \n",
      "1505 [D loss: (-0.073)(R 0.631, F -0.777)]  [G loss: 0.916] \n",
      "1506 [D loss: (-0.079)(R 0.653, F -0.810)]  [G loss: 1.063] \n",
      "1507 [D loss: (-0.035)(R 0.624, F -0.695)]  [G loss: 1.004] \n",
      "1508 [D loss: (-0.083)(R 0.658, F -0.824)]  [G loss: 0.992] \n",
      "1509 [D loss: (-0.061)(R 0.630, F -0.752)]  [G loss: 1.006] \n",
      "1510 [D loss: (-0.068)(R 0.619, F -0.755)]  [G loss: 1.014] \n",
      "1511 [D loss: (-0.043)(R 0.639, F -0.726)]  [G loss: 0.981] \n",
      "1512 [D loss: (-0.074)(R 0.608, F -0.756)]  [G loss: 0.977] \n",
      "1513 [D loss: (-0.063)(R 0.644, F -0.769)]  [G loss: 0.980] \n",
      "1514 [D loss: (-0.066)(R 0.641, F -0.774)]  [G loss: 0.988] \n",
      "1515 [D loss: (-0.068)(R 0.633, F -0.769)]  [G loss: 0.954] \n",
      "1516 [D loss: (-0.067)(R 0.593, F -0.727)]  [G loss: 0.960] \n",
      "1517 [D loss: (-0.048)(R 0.618, F -0.715)]  [G loss: 0.957] \n",
      "1518 [D loss: (-0.057)(R 0.592, F -0.705)]  [G loss: 0.924] \n",
      "1519 [D loss: (-0.054)(R 0.589, F -0.698)]  [G loss: 0.914] \n",
      "1520 [D loss: (-0.062)(R 0.595, F -0.719)]  [G loss: 0.952] \n",
      "1521 [D loss: (-0.074)(R 0.619, F -0.767)]  [G loss: 0.969] \n",
      "1522 [D loss: (-0.071)(R 0.542, F -0.684)]  [G loss: 0.911] \n",
      "1523 [D loss: (-0.087)(R 0.554, F -0.728)]  [G loss: 0.913] \n",
      "1524 [D loss: (-0.066)(R 0.533, F -0.665)]  [G loss: 0.845] \n",
      "1525 [D loss: (-0.088)(R 0.514, F -0.690)]  [G loss: 0.897] \n",
      "1526 [D loss: (-0.082)(R 0.543, F -0.706)]  [G loss: 0.921] \n",
      "1527 [D loss: (-0.063)(R 0.585, F -0.711)]  [G loss: 0.953] \n",
      "1528 [D loss: (-0.080)(R 0.555, F -0.715)]  [G loss: 0.886] \n",
      "1529 [D loss: (-0.045)(R 0.555, F -0.646)]  [G loss: 0.894] \n",
      "1530 [D loss: (-0.038)(R 0.575, F -0.651)]  [G loss: 0.893] \n",
      "1531 [D loss: (-0.056)(R 0.547, F -0.659)]  [G loss: 0.914] \n",
      "1532 [D loss: (-0.055)(R 0.668, F -0.778)]  [G loss: 1.003] \n",
      "1533 [D loss: (-0.055)(R 0.672, F -0.782)]  [G loss: 0.979] \n",
      "1534 [D loss: (-0.084)(R 0.592, F -0.761)]  [G loss: 0.964] \n",
      "1535 [D loss: (-0.059)(R 0.616, F -0.734)]  [G loss: 0.957] \n",
      "1536 [D loss: (-0.057)(R 0.632, F -0.746)]  [G loss: 0.974] \n",
      "1537 [D loss: (-0.044)(R 0.605, F -0.693)]  [G loss: 0.946] \n",
      "1538 [D loss: (-0.052)(R 0.647, F -0.750)]  [G loss: 0.915] \n",
      "1539 [D loss: (-0.096)(R 0.559, F -0.751)]  [G loss: 0.986] \n",
      "1540 [D loss: (-0.088)(R 0.564, F -0.739)]  [G loss: 0.913] \n",
      "1541 [D loss: (-0.037)(R 0.611, F -0.684)]  [G loss: 0.946] \n",
      "1542 [D loss: (-0.035)(R 0.621, F -0.692)]  [G loss: 0.887] \n",
      "1543 [D loss: (-0.037)(R 0.619, F -0.694)]  [G loss: 0.923] \n",
      "1544 [D loss: (-0.045)(R 0.614, F -0.704)]  [G loss: 0.916] \n",
      "1545 [D loss: (-0.034)(R 0.623, F -0.692)]  [G loss: 0.922] \n",
      "1546 [D loss: (-0.069)(R 0.601, F -0.740)]  [G loss: 0.961] \n",
      "1547 [D loss: (-0.105)(R 0.603, F -0.813)]  [G loss: 0.968] \n",
      "1548 [D loss: (-0.031)(R 0.634, F -0.695)]  [G loss: 0.963] \n",
      "1549 [D loss: (-0.049)(R 0.622, F -0.721)]  [G loss: 0.945] \n",
      "1550 [D loss: (-0.070)(R 0.582, F -0.722)]  [G loss: 0.940] \n",
      "1551 [D loss: (-0.053)(R 0.611, F -0.716)]  [G loss: 0.942] \n",
      "1552 [D loss: (-0.029)(R 0.673, F -0.732)]  [G loss: 0.956] \n",
      "1553 [D loss: (-0.046)(R 0.648, F -0.740)]  [G loss: 0.980] \n",
      "1554 [D loss: (-0.045)(R 0.649, F -0.738)]  [G loss: 0.942] \n",
      "1555 [D loss: (-0.064)(R 0.601, F -0.730)]  [G loss: 0.927] \n",
      "1556 [D loss: (-0.054)(R 0.597, F -0.705)]  [G loss: 0.968] \n",
      "1557 [D loss: (-0.066)(R 0.610, F -0.742)]  [G loss: 0.955] \n",
      "1558 [D loss: (-0.046)(R 0.596, F -0.688)]  [G loss: 0.930] \n",
      "1559 [D loss: (-0.058)(R 0.594, F -0.710)]  [G loss: 0.930] \n",
      "1560 [D loss: (-0.056)(R 0.604, F -0.716)]  [G loss: 0.939] \n",
      "1561 [D loss: (-0.037)(R 0.669, F -0.743)]  [G loss: 0.938] \n",
      "1562 [D loss: (-0.088)(R 0.611, F -0.786)]  [G loss: 1.004] \n",
      "1563 [D loss: (-0.047)(R 0.617, F -0.711)]  [G loss: 0.958] \n",
      "1564 [D loss: (-0.057)(R 0.622, F -0.736)]  [G loss: 0.914] \n",
      "1565 [D loss: (-0.097)(R 0.584, F -0.778)]  [G loss: 0.981] \n",
      "1566 [D loss: (-0.075)(R 0.613, F -0.763)]  [G loss: 0.969] \n",
      "1567 [D loss: (-0.065)(R 0.588, F -0.717)]  [G loss: 0.923] \n",
      "1568 [D loss: (-0.085)(R 0.543, F -0.713)]  [G loss: 0.903] \n",
      "1569 [D loss: (-0.075)(R 0.545, F -0.694)]  [G loss: 0.902] \n",
      "1570 [D loss: (-0.062)(R 0.583, F -0.707)]  [G loss: 0.915] \n",
      "1571 [D loss: (-0.035)(R 0.684, F -0.754)]  [G loss: 0.978] \n",
      "1572 [D loss: (-0.079)(R 0.541, F -0.699)]  [G loss: 0.925] \n",
      "1573 [D loss: (-0.070)(R 0.574, F -0.713)]  [G loss: 0.914] \n",
      "1574 [D loss: (-0.062)(R 0.595, F -0.719)]  [G loss: 0.936] \n",
      "1575 [D loss: (-0.060)(R 0.606, F -0.726)]  [G loss: 0.937] \n",
      "1576 [D loss: (-0.108)(R 0.588, F -0.804)]  [G loss: 0.983] \n",
      "1577 [D loss: (-0.047)(R 0.632, F -0.727)]  [G loss: 0.928] \n",
      "1578 [D loss: (-0.081)(R 0.561, F -0.723)]  [G loss: 0.949] \n",
      "1579 [D loss: (-0.101)(R 0.700, F -0.902)]  [G loss: 0.977] \n",
      "1580 [D loss: (-0.036)(R 0.659, F -0.730)]  [G loss: 0.952] \n",
      "1581 [D loss: (-0.069)(R 0.621, F -0.760)]  [G loss: 0.978] \n",
      "1582 [D loss: (-0.061)(R 0.597, F -0.719)]  [G loss: 0.961] \n",
      "1583 [D loss: (-0.064)(R 0.604, F -0.732)]  [G loss: 0.934] \n",
      "1584 [D loss: (-0.115)(R 0.559, F -0.788)]  [G loss: 1.016] \n",
      "1585 [D loss: (-0.064)(R 0.649, F -0.778)]  [G loss: 0.983] \n",
      "1586 [D loss: (-0.072)(R 0.640, F -0.784)]  [G loss: 0.992] \n",
      "1587 [D loss: (-0.071)(R 0.601, F -0.743)]  [G loss: 0.946] \n",
      "1588 [D loss: (-0.109)(R 0.577, F -0.795)]  [G loss: 0.991] \n",
      "1589 [D loss: (-0.065)(R 0.644, F -0.773)]  [G loss: 1.004] \n",
      "1590 [D loss: (-0.055)(R 0.644, F -0.754)]  [G loss: 0.969] \n",
      "1591 [D loss: (-0.063)(R 0.614, F -0.740)]  [G loss: 0.966] \n",
      "1592 [D loss: (-0.022)(R 0.660, F -0.705)]  [G loss: 0.961] \n",
      "1593 [D loss: (-0.065)(R 0.645, F -0.775)]  [G loss: 0.976] \n",
      "1594 [D loss: (-0.053)(R 0.623, F -0.729)]  [G loss: 0.947] \n",
      "1595 [D loss: (-0.073)(R 0.622, F -0.769)]  [G loss: 0.957] \n",
      "1596 [D loss: (-0.086)(R 0.587, F -0.759)]  [G loss: 0.976] \n",
      "1597 [D loss: (-0.137)(R 0.587, F -0.861)]  [G loss: 0.962] \n",
      "1598 [D loss: (-0.073)(R 0.616, F -0.763)]  [G loss: 1.004] \n",
      "1599 [D loss: (-0.099)(R 0.581, F -0.778)]  [G loss: 0.977] \n",
      "1600 [D loss: (-0.072)(R 0.619, F -0.764)]  [G loss: 0.986] \n",
      "1601 [D loss: (-0.098)(R 0.589, F -0.786)]  [G loss: 0.983] \n",
      "1602 [D loss: (-0.066)(R 0.614, F -0.745)]  [G loss: 1.000] \n",
      "1603 [D loss: (-0.066)(R 0.600, F -0.731)]  [G loss: 0.952] \n",
      "1604 [D loss: (-0.072)(R 0.575, F -0.719)]  [G loss: 0.928] \n",
      "1605 [D loss: (-0.074)(R 0.594, F -0.742)]  [G loss: 0.933] \n",
      "1606 [D loss: (-0.101)(R 0.670, F -0.872)]  [G loss: 0.990] \n",
      "1607 [D loss: (-0.060)(R 0.677, F -0.796)]  [G loss: 1.010] \n",
      "1608 [D loss: (-0.055)(R 0.697, F -0.806)]  [G loss: 1.026] \n",
      "1609 [D loss: (-0.053)(R 0.750, F -0.856)]  [G loss: 1.060] \n",
      "1610 [D loss: (-0.079)(R 0.791, F -0.950)]  [G loss: 1.137] \n",
      "1611 [D loss: (-0.090)(R 0.723, F -0.902)]  [G loss: 1.127] \n",
      "1612 [D loss: (-0.062)(R 0.740, F -0.863)]  [G loss: 1.124] \n",
      "1613 [D loss: (-0.068)(R 0.748, F -0.883)]  [G loss: 1.083] \n",
      "1614 [D loss: (-0.071)(R 0.711, F -0.853)]  [G loss: 1.092] \n",
      "1615 [D loss: (-0.066)(R 0.726, F -0.858)]  [G loss: 1.071] \n",
      "1616 [D loss: (-0.061)(R 0.695, F -0.816)]  [G loss: 1.067] \n",
      "1617 [D loss: (-0.064)(R 0.673, F -0.802)]  [G loss: 1.025] \n",
      "1618 [D loss: (-0.067)(R 0.650, F -0.784)]  [G loss: 1.013] \n",
      "1619 [D loss: (-0.068)(R 0.677, F -0.813)]  [G loss: 0.998] \n",
      "1620 [D loss: (-0.087)(R 0.670, F -0.844)]  [G loss: 1.038] \n",
      "1621 [D loss: (-0.048)(R 0.665, F -0.761)]  [G loss: 1.002] \n",
      "1622 [D loss: (-0.063)(R 0.612, F -0.739)]  [G loss: 0.967] \n",
      "1623 [D loss: (-0.077)(R 0.635, F -0.788)]  [G loss: 0.965] \n",
      "1624 [D loss: (-0.068)(R 0.623, F -0.759)]  [G loss: 0.943] \n",
      "1625 [D loss: (-0.053)(R 0.603, F -0.709)]  [G loss: 0.914] \n",
      "1626 [D loss: (-0.088)(R 0.529, F -0.704)]  [G loss: 0.933] \n",
      "1627 [D loss: (-0.083)(R 0.514, F -0.681)]  [G loss: 0.930] \n",
      "1628 [D loss: (-0.119)(R 0.506, F -0.744)]  [G loss: 0.932] \n",
      "1629 [D loss: (-0.128)(R 0.496, F -0.751)]  [G loss: 0.942] \n",
      "1630 [D loss: (-0.057)(R 0.584, F -0.699)]  [G loss: 0.941] \n",
      "1631 [D loss: (-0.078)(R 0.543, F -0.700)]  [G loss: 0.904] \n",
      "1632 [D loss: (-0.112)(R 0.539, F -0.763)]  [G loss: 0.917] \n",
      "1633 [D loss: (-0.069)(R 0.587, F -0.724)]  [G loss: 0.922] \n",
      "1634 [D loss: (-0.080)(R 0.567, F -0.726)]  [G loss: 0.901] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1635 [D loss: (-0.086)(R 0.551, F -0.722)]  [G loss: 0.920] \n",
      "1636 [D loss: (-0.123)(R 0.595, F -0.841)]  [G loss: 0.983] \n",
      "1637 [D loss: (-0.011)(R 0.651, F -0.673)]  [G loss: 0.960] \n",
      "1638 [D loss: (-0.111)(R 0.765, F -0.987)]  [G loss: 1.074] \n",
      "1639 [D loss: (-0.034)(R 0.729, F -0.796)]  [G loss: 1.031] \n",
      "1640 [D loss: (-0.027)(R 0.751, F -0.806)]  [G loss: 1.006] \n",
      "1641 [D loss: (-0.037)(R 0.737, F -0.811)]  [G loss: 1.021] \n",
      "1642 [D loss: (-0.061)(R 0.758, F -0.881)]  [G loss: 1.006] \n",
      "1643 [D loss: (-0.042)(R 0.742, F -0.825)]  [G loss: 1.065] \n",
      "1644 [D loss: (-0.048)(R 0.723, F -0.819)]  [G loss: 1.066] \n",
      "1645 [D loss: (-0.063)(R 0.679, F -0.805)]  [G loss: 1.039] \n",
      "1646 [D loss: (-0.044)(R 0.667, F -0.755)]  [G loss: 0.985] \n",
      "1647 [D loss: (-0.080)(R 0.637, F -0.796)]  [G loss: 0.999] \n",
      "1648 [D loss: (-0.054)(R 0.669, F -0.776)]  [G loss: 1.019] \n",
      "1649 [D loss: (-0.053)(R 0.681, F -0.787)]  [G loss: 1.015] \n",
      "1650 [D loss: (-0.052)(R 0.647, F -0.751)]  [G loss: 0.964] \n",
      "1651 [D loss: (-0.055)(R 0.629, F -0.739)]  [G loss: 0.954] \n",
      "1652 [D loss: (-0.035)(R 0.652, F -0.722)]  [G loss: 0.951] \n",
      "1653 [D loss: (-0.065)(R 0.618, F -0.747)]  [G loss: 0.957] \n",
      "1654 [D loss: (-0.062)(R 0.590, F -0.715)]  [G loss: 0.929] \n",
      "1655 [D loss: (-0.077)(R 0.588, F -0.743)]  [G loss: 0.859] \n",
      "1656 [D loss: (-0.101)(R 0.623, F -0.824)]  [G loss: 1.025] \n",
      "1657 [D loss: (-0.039)(R 0.652, F -0.731)]  [G loss: 0.951] \n",
      "1658 [D loss: (-0.070)(R 0.699, F -0.839)]  [G loss: 0.933] \n",
      "1659 [D loss: (-0.061)(R 0.646, F -0.768)]  [G loss: 1.015] \n",
      "1660 [D loss: (-0.059)(R 0.649, F -0.768)]  [G loss: 0.981] \n",
      "1661 [D loss: (-0.067)(R 0.602, F -0.736)]  [G loss: 0.932] \n",
      "1662 [D loss: (-0.064)(R 0.666, F -0.795)]  [G loss: 0.992] \n",
      "1663 [D loss: (-0.042)(R 0.656, F -0.741)]  [G loss: 0.959] \n",
      "1664 [D loss: (-0.050)(R 0.628, F -0.727)]  [G loss: 0.924] \n",
      "1665 [D loss: (-0.037)(R 0.630, F -0.704)]  [G loss: 0.921] \n",
      "1666 [D loss: (-0.032)(R 0.605, F -0.668)]  [G loss: 0.927] \n",
      "1667 [D loss: (-0.042)(R 0.612, F -0.695)]  [G loss: 0.898] \n",
      "1668 [D loss: (-0.069)(R 0.578, F -0.716)]  [G loss: 0.929] \n",
      "1669 [D loss: (-0.054)(R 0.576, F -0.684)]  [G loss: 0.857] \n",
      "1670 [D loss: (-0.061)(R 0.565, F -0.687)]  [G loss: 0.904] \n",
      "1671 [D loss: (-0.088)(R 0.576, F -0.752)]  [G loss: 0.921] \n",
      "1672 [D loss: (-0.114)(R 0.544, F -0.773)]  [G loss: 0.944] \n",
      "1673 [D loss: (-0.036)(R 0.605, F -0.678)]  [G loss: 0.913] \n",
      "1674 [D loss: (-0.057)(R 0.587, F -0.701)]  [G loss: 0.928] \n",
      "1675 [D loss: (-0.060)(R 0.561, F -0.681)]  [G loss: 0.887] \n",
      "1676 [D loss: (-0.054)(R 0.566, F -0.674)]  [G loss: 0.884] \n",
      "1677 [D loss: (-0.110)(R 0.469, F -0.688)]  [G loss: 0.901] \n",
      "1678 [D loss: (-0.074)(R 0.537, F -0.685)]  [G loss: 0.900] \n",
      "1679 [D loss: (-0.138)(R 0.489, F -0.764)]  [G loss: 0.905] \n",
      "1680 [D loss: (-0.069)(R 0.593, F -0.730)]  [G loss: 0.932] \n",
      "1681 [D loss: (-0.066)(R 0.546, F -0.678)]  [G loss: 0.874] \n",
      "1682 [D loss: (-0.052)(R 0.537, F -0.641)]  [G loss: 0.861] \n",
      "1683 [D loss: (-0.043)(R 0.541, F -0.627)]  [G loss: 0.813] \n",
      "1684 [D loss: (-0.052)(R 0.550, F -0.654)]  [G loss: 0.849] \n",
      "1685 [D loss: (-0.054)(R 0.528, F -0.636)]  [G loss: 0.877] \n",
      "1686 [D loss: (-0.038)(R 0.615, F -0.692)]  [G loss: 0.901] \n",
      "1687 [D loss: (-0.064)(R 0.577, F -0.705)]  [G loss: 0.857] \n",
      "1688 [D loss: (-0.036)(R 0.624, F -0.695)]  [G loss: 0.920] \n",
      "1689 [D loss: (-0.046)(R 0.576, F -0.669)]  [G loss: 0.932] \n",
      "1690 [D loss: (-0.078)(R 0.650, F -0.805)]  [G loss: 0.908] \n",
      "1691 [D loss: (-0.059)(R 0.661, F -0.779)]  [G loss: 1.015] \n",
      "1692 [D loss: (-0.044)(R 0.700, F -0.788)]  [G loss: 0.976] \n",
      "1693 [D loss: (-0.038)(R 0.679, F -0.755)]  [G loss: 0.991] \n",
      "1694 [D loss: (-0.050)(R 0.696, F -0.796)]  [G loss: 0.984] \n",
      "1695 [D loss: (-0.045)(R 0.724, F -0.814)]  [G loss: 1.050] \n",
      "1696 [D loss: (-0.047)(R 0.704, F -0.799)]  [G loss: 1.003] \n",
      "1697 [D loss: (-0.086)(R 0.686, F -0.858)]  [G loss: 1.084] \n",
      "1698 [D loss: (-0.048)(R 0.734, F -0.830)]  [G loss: 1.024] \n",
      "1699 [D loss: (-0.046)(R 0.726, F -0.817)]  [G loss: 1.026] \n",
      "1700 [D loss: (-0.044)(R 0.740, F -0.827)]  [G loss: 1.020] \n",
      "1701 [D loss: (-0.036)(R 0.718, F -0.790)]  [G loss: 1.009] \n",
      "1702 [D loss: (-0.046)(R 0.715, F -0.806)]  [G loss: 1.027] \n",
      "1703 [D loss: (-0.054)(R 0.675, F -0.782)]  [G loss: 0.976] \n",
      "1704 [D loss: (-0.039)(R 0.663, F -0.741)]  [G loss: 0.942] \n",
      "1705 [D loss: (-0.050)(R 0.632, F -0.731)]  [G loss: 0.907] \n",
      "1706 [D loss: (-0.075)(R 0.622, F -0.771)]  [G loss: 0.881] \n",
      "1707 [D loss: (-0.095)(R 0.569, F -0.760)]  [G loss: 0.937] \n",
      "1708 [D loss: (-0.061)(R 0.576, F -0.698)]  [G loss: 0.880] \n",
      "1709 [D loss: (-0.102)(R 0.471, F -0.674)]  [G loss: 0.865] \n",
      "1710 [D loss: (-0.063)(R 0.517, F -0.643)]  [G loss: 0.845] \n",
      "1711 [D loss: (-0.066)(R 0.495, F -0.626)]  [G loss: 0.779] \n",
      "1712 [D loss: (-0.031)(R 0.542, F -0.603)]  [G loss: 0.808] \n",
      "1713 [D loss: (-0.060)(R 0.495, F -0.614)]  [G loss: 0.799] \n",
      "1714 [D loss: (-0.055)(R 0.540, F -0.649)]  [G loss: 0.851] \n",
      "1715 [D loss: (-0.048)(R 0.568, F -0.663)]  [G loss: 0.855] \n",
      "1716 [D loss: (-0.080)(R 0.538, F -0.698)]  [G loss: 0.865] \n",
      "1717 [D loss: (-0.114)(R 0.544, F -0.772)]  [G loss: 0.934] \n",
      "1718 [D loss: (-0.066)(R 0.569, F -0.701)]  [G loss: 0.898] \n",
      "1719 [D loss: (-0.065)(R 0.547, F -0.677)]  [G loss: 0.843] \n",
      "1720 [D loss: (-0.079)(R 0.535, F -0.693)]  [G loss: 0.868] \n",
      "1721 [D loss: (-0.062)(R 0.555, F -0.679)]  [G loss: 0.834] \n",
      "1722 [D loss: (-0.080)(R 0.557, F -0.718)]  [G loss: 0.881] \n",
      "1723 [D loss: (-0.064)(R 0.594, F -0.722)]  [G loss: 0.917] \n",
      "1724 [D loss: (-0.068)(R 0.592, F -0.728)]  [G loss: 0.901] \n",
      "1725 [D loss: (-0.055)(R 0.609, F -0.719)]  [G loss: 0.923] \n",
      "1726 [D loss: (-0.069)(R 0.626, F -0.763)]  [G loss: 0.944] \n",
      "1727 [D loss: (-0.053)(R 0.752, F -0.859)]  [G loss: 1.003] \n",
      "1728 [D loss: (-0.053)(R 0.697, F -0.802)]  [G loss: 0.987] \n",
      "1729 [D loss: (-0.050)(R 0.678, F -0.778)]  [G loss: 0.992] \n",
      "1730 [D loss: (-0.097)(R 0.632, F -0.825)]  [G loss: 0.990] \n",
      "1731 [D loss: (-0.057)(R 0.644, F -0.757)]  [G loss: 0.950] \n",
      "1732 [D loss: (-0.067)(R 0.602, F -0.736)]  [G loss: 0.943] \n",
      "1733 [D loss: (-0.090)(R 0.631, F -0.811)]  [G loss: 0.941] \n",
      "1734 [D loss: (-0.049)(R 0.633, F -0.730)]  [G loss: 0.930] \n",
      "1735 [D loss: (-0.086)(R 0.603, F -0.776)]  [G loss: 0.923] \n",
      "1736 [D loss: (-0.040)(R 0.617, F -0.698)]  [G loss: 0.907] \n",
      "1737 [D loss: (-0.064)(R 0.560, F -0.688)]  [G loss: 0.890] \n",
      "1738 [D loss: (-0.064)(R 0.549, F -0.678)]  [G loss: 0.871] \n",
      "1739 [D loss: (-0.058)(R 0.531, F -0.648)]  [G loss: 0.854] \n",
      "1740 [D loss: (-0.037)(R 0.591, F -0.665)]  [G loss: 0.859] \n",
      "1741 [D loss: (-0.056)(R 0.534, F -0.647)]  [G loss: 0.849] \n",
      "1742 [D loss: (-0.060)(R 0.525, F -0.645)]  [G loss: 0.844] \n",
      "1743 [D loss: (-0.059)(R 0.551, F -0.668)]  [G loss: 0.887] \n",
      "1744 [D loss: (-0.044)(R 0.544, F -0.633)]  [G loss: 0.883] \n",
      "1745 [D loss: (-0.036)(R 0.593, F -0.664)]  [G loss: 0.875] \n",
      "1746 [D loss: (-0.040)(R 0.550, F -0.631)]  [G loss: 0.841] \n",
      "1747 [D loss: (-0.078)(R 0.520, F -0.677)]  [G loss: 0.844] \n",
      "1748 [D loss: (-0.069)(R 0.606, F -0.744)]  [G loss: 0.881] \n",
      "1749 [D loss: (-0.074)(R 0.592, F -0.740)]  [G loss: 0.903] \n",
      "1750 [D loss: (-0.067)(R 0.611, F -0.746)]  [G loss: 0.937] \n",
      "1751 [D loss: (-0.030)(R 0.610, F -0.670)]  [G loss: 0.877] \n",
      "1752 [D loss: (-0.042)(R 0.617, F -0.700)]  [G loss: 0.882] \n",
      "1753 [D loss: (-0.040)(R 0.590, F -0.669)]  [G loss: 0.881] \n",
      "1754 [D loss: (-0.052)(R 0.568, F -0.673)]  [G loss: 0.915] \n",
      "1755 [D loss: (-0.044)(R 0.611, F -0.698)]  [G loss: 0.906] \n",
      "1756 [D loss: (-0.058)(R 0.589, F -0.705)]  [G loss: 0.918] \n",
      "1757 [D loss: (-0.064)(R 0.614, F -0.742)]  [G loss: 0.938] \n",
      "1758 [D loss: (-0.073)(R 0.635, F -0.781)]  [G loss: 0.952] \n",
      "1759 [D loss: (-0.049)(R 0.658, F -0.756)]  [G loss: 0.981] \n",
      "1760 [D loss: (-0.045)(R 0.686, F -0.777)]  [G loss: 0.931] \n",
      "1761 [D loss: (-0.036)(R 0.642, F -0.714)]  [G loss: 0.943] \n",
      "1762 [D loss: (-0.084)(R 0.582, F -0.750)]  [G loss: 0.891] \n",
      "1763 [D loss: (-0.029)(R 0.677, F -0.736)]  [G loss: 0.948] \n",
      "1764 [D loss: (-0.050)(R 0.621, F -0.720)]  [G loss: 0.901] \n",
      "1765 [D loss: (-0.087)(R 0.595, F -0.768)]  [G loss: 0.857] \n",
      "1766 [D loss: (-0.084)(R 0.643, F -0.810)]  [G loss: 1.026] \n",
      "1767 [D loss: (-0.084)(R 0.572, F -0.740)]  [G loss: 0.879] \n",
      "1768 [D loss: (-0.038)(R 0.556, F -0.632)]  [G loss: 0.868] \n",
      "1769 [D loss: (-0.159)(R 0.538, F -0.856)]  [G loss: 0.948] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1770 [D loss: (-0.052)(R 0.554, F -0.659)]  [G loss: 0.844] \n",
      "1771 [D loss: (-0.067)(R 0.530, F -0.663)]  [G loss: 0.833] \n",
      "1772 [D loss: (-0.046)(R 0.551, F -0.642)]  [G loss: 0.880] \n",
      "1773 [D loss: (-0.059)(R 0.538, F -0.656)]  [G loss: 0.842] \n",
      "1774 [D loss: (-0.065)(R 0.568, F -0.697)]  [G loss: 0.872] \n",
      "1775 [D loss: (-0.047)(R 0.580, F -0.674)]  [G loss: 0.863] \n",
      "1776 [D loss: (-0.071)(R 0.567, F -0.708)]  [G loss: 0.843] \n",
      "1777 [D loss: (-0.084)(R 0.532, F -0.700)]  [G loss: 0.883] \n",
      "1778 [D loss: (-0.045)(R 0.586, F -0.676)]  [G loss: 0.830] \n",
      "1779 [D loss: (-0.057)(R 0.591, F -0.704)]  [G loss: 0.868] \n",
      "1780 [D loss: (-0.069)(R 0.572, F -0.711)]  [G loss: 0.928] \n",
      "1781 [D loss: (-0.077)(R 0.544, F -0.698)]  [G loss: 0.931] \n",
      "1782 [D loss: (-0.062)(R 0.599, F -0.722)]  [G loss: 0.883] \n",
      "1783 [D loss: (-0.026)(R 0.661, F -0.713)]  [G loss: 0.893] \n",
      "1784 [D loss: (-0.032)(R 0.653, F -0.718)]  [G loss: 0.891] \n",
      "1785 [D loss: (-0.038)(R 0.608, F -0.684)]  [G loss: 0.879] \n",
      "1786 [D loss: (-0.032)(R 0.599, F -0.663)]  [G loss: 0.873] \n",
      "1787 [D loss: (-0.044)(R 0.588, F -0.675)]  [G loss: 0.861] \n",
      "1788 [D loss: (-0.086)(R 0.529, F -0.701)]  [G loss: 0.876] \n",
      "1789 [D loss: (-0.061)(R 0.596, F -0.718)]  [G loss: 0.868] \n",
      "1790 [D loss: (-0.041)(R 0.595, F -0.678)]  [G loss: 0.850] \n",
      "1791 [D loss: (-0.108)(R 0.623, F -0.840)]  [G loss: 0.937] \n",
      "1792 [D loss: (-0.029)(R 0.571, F -0.629)]  [G loss: 0.870] \n",
      "1793 [D loss: (-0.071)(R 0.532, F -0.675)]  [G loss: 0.829] \n",
      "1794 [D loss: (-0.075)(R 0.484, F -0.634)]  [G loss: 0.795] \n",
      "1795 [D loss: (-0.035)(R 0.519, F -0.589)]  [G loss: 0.783] \n",
      "1796 [D loss: (-0.056)(R 0.523, F -0.635)]  [G loss: 0.830] \n",
      "1797 [D loss: (-0.065)(R 0.531, F -0.660)]  [G loss: 0.832] \n",
      "1798 [D loss: (-0.048)(R 0.552, F -0.649)]  [G loss: 0.830] \n",
      "1799 [D loss: (-0.042)(R 0.558, F -0.641)]  [G loss: 0.824] \n",
      "1800 [D loss: (-0.028)(R 0.577, F -0.633)]  [G loss: 0.824] \n",
      "1801 [D loss: (-0.073)(R 0.535, F -0.681)]  [G loss: 0.849] \n",
      "1802 [D loss: (-0.062)(R 0.532, F -0.655)]  [G loss: 0.857] \n",
      "1803 [D loss: (-0.053)(R 0.608, F -0.714)]  [G loss: 0.854] \n",
      "1804 [D loss: (-0.067)(R 0.547, F -0.682)]  [G loss: 0.845] \n",
      "1805 [D loss: (-0.032)(R 0.588, F -0.653)]  [G loss: 0.855] \n",
      "1806 [D loss: (-0.034)(R 0.575, F -0.643)]  [G loss: 0.848] \n",
      "1807 [D loss: (-0.061)(R 0.566, F -0.687)]  [G loss: 0.828] \n",
      "1808 [D loss: (-0.083)(R 0.551, F -0.716)]  [G loss: 0.886] \n",
      "1809 [D loss: (-0.062)(R 0.593, F -0.718)]  [G loss: 0.864] \n",
      "1810 [D loss: (-0.051)(R 0.597, F -0.699)]  [G loss: 0.875] \n",
      "1811 [D loss: (-0.037)(R 0.597, F -0.672)]  [G loss: 0.863] \n",
      "1812 [D loss: (-0.045)(R 0.584, F -0.675)]  [G loss: 0.831] \n",
      "1813 [D loss: (-0.079)(R 0.603, F -0.761)]  [G loss: 0.843] \n",
      "1814 [D loss: (-0.078)(R 0.511, F -0.667)]  [G loss: 0.818] \n",
      "1815 [D loss: (-0.048)(R 0.529, F -0.626)]  [G loss: 0.824] \n",
      "1816 [D loss: (-0.062)(R 0.513, F -0.637)]  [G loss: 0.780] \n",
      "1817 [D loss: (-0.031)(R 0.543, F -0.604)]  [G loss: 0.818] \n",
      "1818 [D loss: (-0.031)(R 0.557, F -0.620)]  [G loss: 0.818] \n",
      "1819 [D loss: (-0.046)(R 0.527, F -0.619)]  [G loss: 0.820] \n",
      "1820 [D loss: (-0.055)(R 0.554, F -0.664)]  [G loss: 0.829] \n",
      "1821 [D loss: (-0.056)(R 0.556, F -0.667)]  [G loss: 0.841] \n",
      "1822 [D loss: (-0.070)(R 0.558, F -0.698)]  [G loss: 0.843] \n",
      "1823 [D loss: (-0.039)(R 0.573, F -0.650)]  [G loss: 0.823] \n",
      "1824 [D loss: (-0.044)(R 0.577, F -0.666)]  [G loss: 0.820] \n",
      "1825 [D loss: (-0.066)(R 0.600, F -0.731)]  [G loss: 0.825] \n",
      "1826 [D loss: (-0.061)(R 0.539, F -0.662)]  [G loss: 0.825] \n",
      "1827 [D loss: (-0.052)(R 0.538, F -0.642)]  [G loss: 0.801] \n",
      "1828 [D loss: (-0.029)(R 0.591, F -0.648)]  [G loss: 0.819] \n",
      "1829 [D loss: (-0.049)(R 0.560, F -0.658)]  [G loss: 0.820] \n",
      "1830 [D loss: (-0.049)(R 0.537, F -0.636)]  [G loss: 0.822] \n",
      "1831 [D loss: (-0.055)(R 0.538, F -0.648)]  [G loss: 0.811] \n",
      "1832 [D loss: (-0.067)(R 0.543, F -0.677)]  [G loss: 0.851] \n",
      "1833 [D loss: (-0.100)(R 0.540, F -0.740)]  [G loss: 0.820] \n",
      "1834 [D loss: (-0.049)(R 0.566, F -0.664)]  [G loss: 0.830] \n",
      "1835 [D loss: (-0.078)(R 0.551, F -0.707)]  [G loss: 0.873] \n",
      "1836 [D loss: (-0.083)(R 0.512, F -0.678)]  [G loss: 0.839] \n",
      "1837 [D loss: (-0.061)(R 0.570, F -0.692)]  [G loss: 0.846] \n",
      "1838 [D loss: (-0.084)(R 0.544, F -0.713)]  [G loss: 0.851] \n",
      "1839 [D loss: (-0.041)(R 0.592, F -0.675)]  [G loss: 0.872] \n",
      "1840 [D loss: (-0.052)(R 0.586, F -0.690)]  [G loss: 0.881] \n",
      "1841 [D loss: (-0.037)(R 0.613, F -0.686)]  [G loss: 0.864] \n",
      "1842 [D loss: (-0.036)(R 0.593, F -0.665)]  [G loss: 0.849] \n",
      "1843 [D loss: (-0.043)(R 0.584, F -0.671)]  [G loss: 0.868] \n",
      "1844 [D loss: (-0.038)(R 0.617, F -0.694)]  [G loss: 0.847] \n",
      "1845 [D loss: (-0.058)(R 0.567, F -0.684)]  [G loss: 0.839] \n",
      "1846 [D loss: (-0.073)(R 0.558, F -0.704)]  [G loss: 0.896] \n",
      "1847 [D loss: (-0.085)(R 0.570, F -0.741)]  [G loss: 0.890] \n",
      "1848 [D loss: (-0.047)(R 0.595, F -0.689)]  [G loss: 0.855] \n",
      "1849 [D loss: (-0.050)(R 0.579, F -0.679)]  [G loss: 0.823] \n",
      "1850 [D loss: (-0.060)(R 0.522, F -0.642)]  [G loss: 0.810] \n",
      "1851 [D loss: (-0.065)(R 0.561, F -0.690)]  [G loss: 0.808] \n",
      "1852 [D loss: (-0.068)(R 0.527, F -0.663)]  [G loss: 0.837] \n",
      "1853 [D loss: (-0.090)(R 0.509, F -0.689)]  [G loss: 0.799] \n",
      "1854 [D loss: (-0.047)(R 0.580, F -0.673)]  [G loss: 0.865] \n",
      "1855 [D loss: (-0.059)(R 0.541, F -0.659)]  [G loss: 0.830] \n",
      "1856 [D loss: (-0.048)(R 0.554, F -0.650)]  [G loss: 0.819] \n",
      "1857 [D loss: (-0.052)(R 0.545, F -0.648)]  [G loss: 0.830] \n",
      "1858 [D loss: (-0.056)(R 0.546, F -0.658)]  [G loss: 0.835] \n",
      "1859 [D loss: (-0.052)(R 0.550, F -0.654)]  [G loss: 0.821] \n",
      "1860 [D loss: (-0.056)(R 0.565, F -0.677)]  [G loss: 0.818] \n",
      "1861 [D loss: (-0.060)(R 0.550, F -0.669)]  [G loss: 0.853] \n",
      "1862 [D loss: (-0.051)(R 0.560, F -0.662)]  [G loss: 0.814] \n",
      "1863 [D loss: (-0.060)(R 0.528, F -0.648)]  [G loss: 0.826] \n",
      "1864 [D loss: (-0.059)(R 0.576, F -0.695)]  [G loss: 0.824] \n",
      "1865 [D loss: (-0.037)(R 0.605, F -0.680)]  [G loss: 0.870] \n",
      "1866 [D loss: (-0.037)(R 0.604, F -0.679)]  [G loss: 0.843] \n",
      "1867 [D loss: (-0.038)(R 0.589, F -0.664)]  [G loss: 0.860] \n",
      "1868 [D loss: (-0.046)(R 0.592, F -0.685)]  [G loss: 0.846] \n",
      "1869 [D loss: (-0.042)(R 0.582, F -0.665)]  [G loss: 0.840] \n",
      "1870 [D loss: (-0.045)(R 0.577, F -0.668)]  [G loss: 0.844] \n",
      "1871 [D loss: (-0.059)(R 0.565, F -0.682)]  [G loss: 0.837] \n",
      "1872 [D loss: (-0.069)(R 0.550, F -0.689)]  [G loss: 0.836] \n",
      "1873 [D loss: (-0.033)(R 0.657, F -0.724)]  [G loss: 0.898] \n",
      "1874 [D loss: (-0.064)(R 0.582, F -0.710)]  [G loss: 0.858] \n",
      "1875 [D loss: (-0.055)(R 0.594, F -0.704)]  [G loss: 0.862] \n",
      "1876 [D loss: (-0.035)(R 0.590, F -0.660)]  [G loss: 0.824] \n",
      "1877 [D loss: (-0.059)(R 0.523, F -0.640)]  [G loss: 0.818] \n",
      "1878 [D loss: (-0.042)(R 0.588, F -0.671)]  [G loss: 0.838] \n",
      "1879 [D loss: (-0.062)(R 0.560, F -0.685)]  [G loss: 0.834] \n",
      "1880 [D loss: (-0.092)(R 0.592, F -0.775)]  [G loss: 0.870] \n",
      "1881 [D loss: (-0.060)(R 0.549, F -0.670)]  [G loss: 0.818] \n",
      "1882 [D loss: (-0.064)(R 0.541, F -0.668)]  [G loss: 0.839] \n",
      "1883 [D loss: (-0.051)(R 0.549, F -0.651)]  [G loss: 0.818] \n",
      "1884 [D loss: (-0.094)(R 0.540, F -0.728)]  [G loss: 0.833] \n",
      "1885 [D loss: (-0.048)(R 0.539, F -0.635)]  [G loss: 0.820] \n",
      "1886 [D loss: (-0.030)(R 0.592, F -0.652)]  [G loss: 0.815] \n",
      "1887 [D loss: (-0.047)(R 0.537, F -0.631)]  [G loss: 0.798] \n",
      "1888 [D loss: (-0.070)(R 0.564, F -0.705)]  [G loss: 0.846] \n",
      "1889 [D loss: (-0.072)(R 0.559, F -0.703)]  [G loss: 0.842] \n",
      "1890 [D loss: (-0.062)(R 0.557, F -0.681)]  [G loss: 0.844] \n",
      "1891 [D loss: (-0.057)(R 0.543, F -0.658)]  [G loss: 0.824] \n",
      "1892 [D loss: (-0.041)(R 0.578, F -0.660)]  [G loss: 0.812] \n",
      "1893 [D loss: (-0.064)(R 0.555, F -0.682)]  [G loss: 0.845] \n",
      "1894 [D loss: (-0.051)(R 0.557, F -0.658)]  [G loss: 0.851] \n",
      "1895 [D loss: (-0.077)(R 0.543, F -0.697)]  [G loss: 0.864] \n",
      "1896 [D loss: (-0.040)(R 0.585, F -0.664)]  [G loss: 0.842] \n",
      "1897 [D loss: (-0.046)(R 0.557, F -0.648)]  [G loss: 0.805] \n",
      "1898 [D loss: (-0.041)(R 0.543, F -0.625)]  [G loss: 0.817] \n",
      "1899 [D loss: (-0.046)(R 0.538, F -0.630)]  [G loss: 0.815] \n",
      "1900 [D loss: (-0.061)(R 0.548, F -0.671)]  [G loss: 0.804] \n",
      "1901 [D loss: (-0.098)(R 0.550, F -0.746)]  [G loss: 0.871] \n",
      "1902 [D loss: (-0.031)(R 0.602, F -0.665)]  [G loss: 0.858] \n",
      "1903 [D loss: (-0.044)(R 0.590, F -0.679)]  [G loss: 0.860] \n",
      "1904 [D loss: (-0.061)(R 0.609, F -0.732)]  [G loss: 0.882] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1905 [D loss: (-0.041)(R 0.642, F -0.723)]  [G loss: 0.882] \n",
      "1906 [D loss: (-0.033)(R 0.632, F -0.698)]  [G loss: 0.834] \n",
      "1907 [D loss: (-0.056)(R 0.583, F -0.694)]  [G loss: 0.856] \n",
      "1908 [D loss: (-0.036)(R 0.600, F -0.672)]  [G loss: 0.855] \n",
      "1909 [D loss: (-0.057)(R 0.578, F -0.692)]  [G loss: 0.820] \n",
      "1910 [D loss: (-0.060)(R 0.572, F -0.692)]  [G loss: 0.880] \n",
      "1911 [D loss: (-0.071)(R 0.550, F -0.693)]  [G loss: 0.837] \n",
      "1912 [D loss: (-0.053)(R 0.548, F -0.655)]  [G loss: 0.834] \n",
      "1913 [D loss: (-0.058)(R 0.590, F -0.706)]  [G loss: 0.877] \n",
      "1914 [D loss: (-0.048)(R 0.609, F -0.705)]  [G loss: 0.834] \n",
      "1915 [D loss: (-0.039)(R 0.576, F -0.655)]  [G loss: 0.830] \n",
      "1916 [D loss: (-0.075)(R 0.544, F -0.694)]  [G loss: 0.835] \n",
      "1917 [D loss: (-0.025)(R 0.606, F -0.656)]  [G loss: 0.817] \n",
      "1918 [D loss: (-0.066)(R 0.559, F -0.691)]  [G loss: 0.840] \n",
      "1919 [D loss: (-0.084)(R 0.527, F -0.694)]  [G loss: 0.853] \n",
      "1920 [D loss: (-0.040)(R 0.575, F -0.655)]  [G loss: 0.827] \n",
      "1921 [D loss: (-0.048)(R 0.582, F -0.677)]  [G loss: 0.774] \n",
      "1922 [D loss: (-0.072)(R 0.560, F -0.703)]  [G loss: 0.838] \n",
      "1923 [D loss: (-0.064)(R 0.562, F -0.690)]  [G loss: 0.827] \n",
      "1924 [D loss: (-0.043)(R 0.570, F -0.656)]  [G loss: 0.826] \n",
      "1925 [D loss: (-0.061)(R 0.538, F -0.659)]  [G loss: 0.818] \n",
      "1926 [D loss: (-0.060)(R 0.548, F -0.668)]  [G loss: 0.829] \n",
      "1927 [D loss: (-0.063)(R 0.541, F -0.666)]  [G loss: 0.815] \n",
      "1928 [D loss: (-0.040)(R 0.546, F -0.625)]  [G loss: 0.787] \n",
      "1929 [D loss: (-0.044)(R 0.535, F -0.623)]  [G loss: 0.784] \n",
      "1930 [D loss: (-0.059)(R 0.520, F -0.638)]  [G loss: 0.785] \n",
      "1931 [D loss: (-0.062)(R 0.582, F -0.706)]  [G loss: 0.836] \n",
      "1932 [D loss: (-0.078)(R 0.519, F -0.675)]  [G loss: 0.838] \n",
      "1933 [D loss: (-0.048)(R 0.550, F -0.646)]  [G loss: 0.817] \n",
      "1934 [D loss: (-0.059)(R 0.521, F -0.639)]  [G loss: 0.791] \n",
      "1935 [D loss: (-0.048)(R 0.514, F -0.611)]  [G loss: 0.784] \n",
      "1936 [D loss: (-0.037)(R 0.525, F -0.600)]  [G loss: 0.782] \n",
      "1937 [D loss: (-0.054)(R 0.504, F -0.612)]  [G loss: 0.768] \n",
      "1938 [D loss: (-0.054)(R 0.522, F -0.630)]  [G loss: 0.791] \n",
      "1939 [D loss: (-0.058)(R 0.526, F -0.642)]  [G loss: 0.803] \n",
      "1940 [D loss: (-0.037)(R 0.556, F -0.629)]  [G loss: 0.778] \n",
      "1941 [D loss: (-0.045)(R 0.544, F -0.634)]  [G loss: 0.801] \n",
      "1942 [D loss: (-0.063)(R 0.518, F -0.644)]  [G loss: 0.766] \n",
      "1943 [D loss: (-0.061)(R 0.550, F -0.672)]  [G loss: 0.812] \n",
      "1944 [D loss: (-0.041)(R 0.520, F -0.603)]  [G loss: 0.782] \n",
      "1945 [D loss: (-0.018)(R 0.624, F -0.660)]  [G loss: 0.831] \n",
      "1946 [D loss: (-0.045)(R 0.552, F -0.642)]  [G loss: 0.806] \n",
      "1947 [D loss: (-0.046)(R 0.530, F -0.623)]  [G loss: 0.790] \n",
      "1948 [D loss: (-0.040)(R 0.592, F -0.671)]  [G loss: 0.797] \n",
      "1949 [D loss: (-0.046)(R 0.561, F -0.653)]  [G loss: 0.800] \n",
      "1950 [D loss: (-0.064)(R 0.543, F -0.672)]  [G loss: 0.822] \n",
      "1951 [D loss: (-0.032)(R 0.601, F -0.665)]  [G loss: 0.826] \n",
      "1952 [D loss: (-0.045)(R 0.563, F -0.653)]  [G loss: 0.805] \n",
      "1953 [D loss: (-0.068)(R 0.514, F -0.650)]  [G loss: 0.790] \n",
      "1954 [D loss: (-0.042)(R 0.611, F -0.695)]  [G loss: 0.843] \n",
      "1955 [D loss: (-0.072)(R 0.541, F -0.684)]  [G loss: 0.814] \n",
      "1956 [D loss: (-0.012)(R 0.600, F -0.623)]  [G loss: 0.801] \n",
      "1957 [D loss: (-0.031)(R 0.527, F -0.589)]  [G loss: 0.767] \n",
      "1958 [D loss: (-0.103)(R 0.517, F -0.724)]  [G loss: 0.806] \n",
      "1959 [D loss: (-0.057)(R 0.487, F -0.601)]  [G loss: 0.765] \n",
      "1960 [D loss: (-0.054)(R 0.518, F -0.627)]  [G loss: 0.751] \n",
      "1961 [D loss: (-0.088)(R 0.487, F -0.662)]  [G loss: 0.764] \n",
      "1962 [D loss: (-0.034)(R 0.526, F -0.594)]  [G loss: 0.760] \n",
      "1963 [D loss: (-0.026)(R 0.540, F -0.593)]  [G loss: 0.762] \n",
      "1964 [D loss: (-0.033)(R 0.520, F -0.586)]  [G loss: 0.756] \n",
      "1965 [D loss: (-0.064)(R 0.510, F -0.638)]  [G loss: 0.759] \n",
      "1966 [D loss: (-0.031)(R 0.506, F -0.568)]  [G loss: 0.752] \n",
      "1967 [D loss: (-0.041)(R 0.510, F -0.593)]  [G loss: 0.739] \n",
      "1968 [D loss: (-0.022)(R 0.536, F -0.581)]  [G loss: 0.762] \n",
      "1969 [D loss: (-0.059)(R 0.518, F -0.635)]  [G loss: 0.763] \n",
      "1970 [D loss: (-0.036)(R 0.599, F -0.672)]  [G loss: 0.791] \n",
      "1971 [D loss: (-0.020)(R 0.525, F -0.565)]  [G loss: 0.771] \n",
      "1972 [D loss: (-0.048)(R 0.556, F -0.652)]  [G loss: 0.791] \n",
      "1973 [D loss: (-0.018)(R 0.567, F -0.603)]  [G loss: 0.777] \n",
      "1974 [D loss: (-0.039)(R 0.525, F -0.602)]  [G loss: 0.763] \n",
      "1975 [D loss: (-0.023)(R 0.542, F -0.588)]  [G loss: 0.740] \n",
      "1976 [D loss: (-0.014)(R 0.575, F -0.603)]  [G loss: 0.774] \n",
      "1977 [D loss: (-0.059)(R 0.501, F -0.619)]  [G loss: 0.754] \n",
      "1978 [D loss: (-0.093)(R 0.521, F -0.708)]  [G loss: 0.771] \n",
      "1979 [D loss: (-0.036)(R 0.489, F -0.560)]  [G loss: 0.717] \n",
      "1980 [D loss: (-0.040)(R 0.476, F -0.556)]  [G loss: 0.714] \n",
      "1981 [D loss: (-0.042)(R 0.480, F -0.563)]  [G loss: 0.715] \n",
      "1982 [D loss: (-0.037)(R 0.537, F -0.610)]  [G loss: 0.712] \n",
      "1983 [D loss: (-0.056)(R 0.478, F -0.589)]  [G loss: 0.730] \n",
      "1984 [D loss: (-0.035)(R 0.503, F -0.572)]  [G loss: 0.733] \n",
      "1985 [D loss: (-0.026)(R 0.512, F -0.564)]  [G loss: 0.744] \n",
      "1986 [D loss: (-0.061)(R 0.486, F -0.608)]  [G loss: 0.745] \n",
      "1987 [D loss: (-0.078)(R 0.481, F -0.637)]  [G loss: 0.752] \n",
      "1988 [D loss: (-0.042)(R 0.522, F -0.606)]  [G loss: 0.764] \n",
      "1989 [D loss: (-0.023)(R 0.553, F -0.600)]  [G loss: 0.764] \n",
      "1990 [D loss: (-0.035)(R 0.540, F -0.610)]  [G loss: 0.769] \n",
      "1991 [D loss: (-0.041)(R 0.541, F -0.622)]  [G loss: 0.757] \n",
      "1992 [D loss: (-0.055)(R 0.541, F -0.651)]  [G loss: 0.780] \n",
      "1993 [D loss: (-0.034)(R 0.560, F -0.628)]  [G loss: 0.785] \n",
      "1994 [D loss: (-0.060)(R 0.526, F -0.647)]  [G loss: 0.782] \n",
      "1995 [D loss: (-0.065)(R 0.523, F -0.653)]  [G loss: 0.765] \n",
      "1996 [D loss: (-0.030)(R 0.566, F -0.625)]  [G loss: 0.759] \n",
      "1997 [D loss: (-0.032)(R 0.540, F -0.604)]  [G loss: 0.767] \n",
      "1998 [D loss: (-0.029)(R 0.531, F -0.589)]  [G loss: 0.742] \n",
      "1999 [D loss: (-0.035)(R 0.543, F -0.612)]  [G loss: 0.758] \n",
      "2000 [D loss: (-0.025)(R 0.526, F -0.575)]  [G loss: 0.741] \n",
      "2001 [D loss: (-0.030)(R 0.529, F -0.590)]  [G loss: 0.746] \n",
      "2002 [D loss: (-0.039)(R 0.517, F -0.596)]  [G loss: 0.741] \n",
      "2003 [D loss: (-0.034)(R 0.503, F -0.571)]  [G loss: 0.725] \n",
      "2004 [D loss: (-0.038)(R 0.505, F -0.582)]  [G loss: 0.721] \n",
      "2005 [D loss: (-0.068)(R 0.459, F -0.594)]  [G loss: 0.723] \n",
      "2006 [D loss: (-0.075)(R 0.498, F -0.647)]  [G loss: 0.734] \n",
      "2007 [D loss: (-0.045)(R 0.506, F -0.596)]  [G loss: 0.740] \n",
      "2008 [D loss: (-0.051)(R 0.486, F -0.587)]  [G loss: 0.707] \n",
      "2009 [D loss: (-0.046)(R 0.471, F -0.562)]  [G loss: 0.702] \n",
      "2010 [D loss: (-0.030)(R 0.507, F -0.567)]  [G loss: 0.722] \n",
      "2011 [D loss: (-0.060)(R 0.477, F -0.597)]  [G loss: 0.710] \n",
      "2012 [D loss: (-0.030)(R 0.510, F -0.570)]  [G loss: 0.723] \n",
      "2013 [D loss: (-0.073)(R 0.465, F -0.611)]  [G loss: 0.717] \n",
      "2014 [D loss: (-0.022)(R 0.534, F -0.577)]  [G loss: 0.727] \n",
      "2015 [D loss: (-0.032)(R 0.502, F -0.565)]  [G loss: 0.686] \n",
      "2016 [D loss: (-0.090)(R 0.464, F -0.643)]  [G loss: 0.769] \n",
      "2017 [D loss: (-0.044)(R 0.464, F -0.552)]  [G loss: 0.675] \n",
      "2018 [D loss: (-0.031)(R 0.491, F -0.554)]  [G loss: 0.703] \n",
      "2019 [D loss: (-0.060)(R 0.480, F -0.601)]  [G loss: 0.725] \n",
      "2020 [D loss: (-0.059)(R 0.512, F -0.630)]  [G loss: 0.711] \n",
      "2021 [D loss: (-0.029)(R 0.518, F -0.576)]  [G loss: 0.739] \n",
      "2022 [D loss: (-0.045)(R 0.513, F -0.602)]  [G loss: 0.733] \n",
      "2023 [D loss: (-0.036)(R 0.531, F -0.603)]  [G loss: 0.755] \n",
      "2024 [D loss: (-0.047)(R 0.514, F -0.608)]  [G loss: 0.732] \n",
      "2025 [D loss: (-0.033)(R 0.525, F -0.592)]  [G loss: 0.748] \n",
      "2026 [D loss: (-0.036)(R 0.532, F -0.605)]  [G loss: 0.737] \n",
      "2027 [D loss: (-0.028)(R 0.550, F -0.605)]  [G loss: 0.751] \n",
      "2028 [D loss: (-0.025)(R 0.544, F -0.594)]  [G loss: 0.737] \n",
      "2029 [D loss: (-0.028)(R 0.552, F -0.608)]  [G loss: 0.773] \n",
      "2030 [D loss: (-0.041)(R 0.545, F -0.626)]  [G loss: 0.754] \n",
      "2031 [D loss: (-0.042)(R 0.507, F -0.592)]  [G loss: 0.762] \n",
      "2032 [D loss: (-0.053)(R 0.505, F -0.610)]  [G loss: 0.719] \n",
      "2033 [D loss: (-0.042)(R 0.518, F -0.603)]  [G loss: 0.748] \n",
      "2034 [D loss: (-0.058)(R 0.502, F -0.617)]  [G loss: 0.733] \n",
      "2035 [D loss: (-0.043)(R 0.533, F -0.618)]  [G loss: 0.752] \n",
      "2036 [D loss: (-0.044)(R 0.513, F -0.600)]  [G loss: 0.715] \n",
      "2037 [D loss: (-0.044)(R 0.518, F -0.605)]  [G loss: 0.703] \n",
      "2038 [D loss: (-0.041)(R 0.495, F -0.576)]  [G loss: 0.717] \n",
      "2039 [D loss: (-0.053)(R 0.473, F -0.579)]  [G loss: 0.721] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2040 [D loss: (-0.041)(R 0.499, F -0.581)]  [G loss: 0.707] \n",
      "2041 [D loss: (-0.057)(R 0.467, F -0.582)]  [G loss: 0.712] \n",
      "2042 [D loss: (-0.017)(R 0.535, F -0.569)]  [G loss: 0.725] \n",
      "2043 [D loss: (-0.032)(R 0.516, F -0.580)]  [G loss: 0.726] \n",
      "2044 [D loss: (-0.031)(R 0.502, F -0.565)]  [G loss: 0.709] \n",
      "2045 [D loss: (-0.038)(R 0.489, F -0.566)]  [G loss: 0.716] \n",
      "2046 [D loss: (-0.047)(R 0.476, F -0.571)]  [G loss: 0.711] \n",
      "2047 [D loss: (-0.022)(R 0.520, F -0.563)]  [G loss: 0.734] \n",
      "2048 [D loss: (-0.053)(R 0.520, F -0.626)]  [G loss: 0.731] \n",
      "2049 [D loss: (-0.020)(R 0.564, F -0.605)]  [G loss: 0.749] \n",
      "2050 [D loss: (-0.035)(R 0.521, F -0.590)]  [G loss: 0.733] \n",
      "2051 [D loss: (-0.033)(R 0.518, F -0.583)]  [G loss: 0.739] \n",
      "2052 [D loss: (-0.034)(R 0.543, F -0.610)]  [G loss: 0.718] \n",
      "2053 [D loss: (-0.045)(R 0.523, F -0.613)]  [G loss: 0.740] \n",
      "2054 [D loss: (-0.047)(R 0.494, F -0.589)]  [G loss: 0.710] \n",
      "2055 [D loss: (-0.044)(R 0.483, F -0.571)]  [G loss: 0.718] \n",
      "2056 [D loss: (-0.033)(R 0.509, F -0.575)]  [G loss: 0.707] \n",
      "2057 [D loss: (-0.052)(R 0.488, F -0.591)]  [G loss: 0.714] \n",
      "2058 [D loss: (-0.040)(R 0.496, F -0.576)]  [G loss: 0.720] \n",
      "2059 [D loss: (-0.039)(R 0.504, F -0.582)]  [G loss: 0.706] \n",
      "2060 [D loss: (-0.038)(R 0.504, F -0.580)]  [G loss: 0.725] \n",
      "2061 [D loss: (-0.022)(R 0.529, F -0.572)]  [G loss: 0.745] \n",
      "2062 [D loss: (-0.028)(R 0.537, F -0.594)]  [G loss: 0.719] \n",
      "2063 [D loss: (-0.040)(R 0.515, F -0.595)]  [G loss: 0.716] \n",
      "2064 [D loss: (-0.015)(R 0.556, F -0.586)]  [G loss: 0.733] \n",
      "2065 [D loss: (-0.041)(R 0.532, F -0.614)]  [G loss: 0.730] \n",
      "2066 [D loss: (-0.045)(R 0.533, F -0.622)]  [G loss: 0.760] \n",
      "2067 [D loss: (-0.032)(R 0.542, F -0.606)]  [G loss: 0.747] \n",
      "2068 [D loss: (-0.036)(R 0.522, F -0.595)]  [G loss: 0.743] \n",
      "2069 [D loss: (-0.028)(R 0.526, F -0.583)]  [G loss: 0.727] \n",
      "2070 [D loss: (-0.030)(R 0.519, F -0.578)]  [G loss: 0.704] \n",
      "2071 [D loss: (-0.049)(R 0.524, F -0.622)]  [G loss: 0.751] \n",
      "2072 [D loss: (-0.067)(R 0.496, F -0.630)]  [G loss: 0.723] \n",
      "2073 [D loss: (-0.067)(R 0.493, F -0.626)]  [G loss: 0.719] \n",
      "2074 [D loss: (-0.055)(R 0.508, F -0.618)]  [G loss: 0.713] \n",
      "2075 [D loss: (-0.031)(R 0.505, F -0.567)]  [G loss: 0.725] \n",
      "2076 [D loss: (-0.039)(R 0.518, F -0.596)]  [G loss: 0.717] \n",
      "2077 [D loss: (-0.029)(R 0.517, F -0.576)]  [G loss: 0.709] \n",
      "2078 [D loss: (-0.032)(R 0.526, F -0.591)]  [G loss: 0.730] \n",
      "2079 [D loss: (-0.034)(R 0.525, F -0.592)]  [G loss: 0.737] \n",
      "2080 [D loss: (-0.031)(R 0.537, F -0.599)]  [G loss: 0.736] \n",
      "2081 [D loss: (-0.034)(R 0.516, F -0.584)]  [G loss: 0.731] \n",
      "2082 [D loss: (-0.034)(R 0.513, F -0.581)]  [G loss: 0.717] \n",
      "2083 [D loss: (-0.022)(R 0.534, F -0.578)]  [G loss: 0.729] \n",
      "2084 [D loss: (-0.034)(R 0.514, F -0.582)]  [G loss: 0.722] \n",
      "2085 [D loss: (-0.038)(R 0.505, F -0.581)]  [G loss: 0.719] \n",
      "2086 [D loss: (-0.050)(R 0.509, F -0.608)]  [G loss: 0.718] \n",
      "2087 [D loss: (-0.056)(R 0.500, F -0.612)]  [G loss: 0.730] \n",
      "2088 [D loss: (-0.058)(R 0.484, F -0.600)]  [G loss: 0.702] \n",
      "2089 [D loss: (-0.036)(R 0.471, F -0.544)]  [G loss: 0.691] \n",
      "2090 [D loss: (-0.023)(R 0.481, F -0.527)]  [G loss: 0.675] \n",
      "2091 [D loss: (-0.024)(R 0.493, F -0.540)]  [G loss: 0.652] \n",
      "2092 [D loss: (-0.032)(R 0.481, F -0.546)]  [G loss: 0.673] \n",
      "2093 [D loss: (-0.046)(R 0.467, F -0.559)]  [G loss: 0.683] \n",
      "2094 [D loss: (-0.020)(R 0.500, F -0.539)]  [G loss: 0.684] \n",
      "2095 [D loss: (-0.034)(R 0.484, F -0.552)]  [G loss: 0.666] \n",
      "2096 [D loss: (-0.034)(R 0.516, F -0.584)]  [G loss: 0.702] \n",
      "2097 [D loss: (-0.066)(R 0.493, F -0.626)]  [G loss: 0.726] \n",
      "2098 [D loss: (-0.050)(R 0.524, F -0.625)]  [G loss: 0.727] \n",
      "2099 [D loss: (-0.026)(R 0.525, F -0.578)]  [G loss: 0.699] \n",
      "2100 [D loss: (-0.027)(R 0.512, F -0.566)]  [G loss: 0.703] \n",
      "2101 [D loss: (-0.020)(R 0.542, F -0.583)]  [G loss: 0.720] \n",
      "2102 [D loss: (-0.031)(R 0.543, F -0.605)]  [G loss: 0.748] \n",
      "2103 [D loss: (-0.035)(R 0.530, F -0.601)]  [G loss: 0.720] \n",
      "2104 [D loss: (-0.034)(R 0.547, F -0.616)]  [G loss: 0.712] \n",
      "2105 [D loss: (-0.030)(R 0.525, F -0.586)]  [G loss: 0.709] \n",
      "2106 [D loss: (-0.028)(R 0.533, F -0.589)]  [G loss: 0.734] \n",
      "2107 [D loss: (-0.025)(R 0.545, F -0.595)]  [G loss: 0.722] \n",
      "2108 [D loss: (-0.018)(R 0.544, F -0.581)]  [G loss: 0.734] \n",
      "2109 [D loss: (-0.032)(R 0.548, F -0.612)]  [G loss: 0.748] \n",
      "2110 [D loss: (-0.026)(R 0.546, F -0.599)]  [G loss: 0.741] \n",
      "2111 [D loss: (-0.022)(R 0.559, F -0.604)]  [G loss: 0.735] \n",
      "2112 [D loss: (-0.016)(R 0.534, F -0.566)]  [G loss: 0.721] \n",
      "2113 [D loss: (-0.038)(R 0.513, F -0.589)]  [G loss: 0.717] \n",
      "2114 [D loss: (-0.045)(R 0.531, F -0.621)]  [G loss: 0.702] \n",
      "2115 [D loss: (-0.023)(R 0.494, F -0.540)]  [G loss: 0.702] \n",
      "2116 [D loss: (-0.028)(R 0.516, F -0.572)]  [G loss: 0.699] \n",
      "2117 [D loss: (-0.033)(R 0.486, F -0.552)]  [G loss: 0.690] \n",
      "2118 [D loss: (-0.050)(R 0.492, F -0.592)]  [G loss: 0.710] \n",
      "2119 [D loss: (-0.047)(R 0.494, F -0.588)]  [G loss: 0.691] \n",
      "2120 [D loss: (-0.046)(R 0.459, F -0.550)]  [G loss: 0.680] \n",
      "2121 [D loss: (-0.039)(R 0.487, F -0.564)]  [G loss: 0.689] \n",
      "2122 [D loss: (-0.054)(R 0.486, F -0.595)]  [G loss: 0.688] \n",
      "2123 [D loss: (-0.048)(R 0.481, F -0.577)]  [G loss: 0.705] \n",
      "2124 [D loss: (-0.036)(R 0.503, F -0.574)]  [G loss: 0.695] \n",
      "2125 [D loss: (-0.040)(R 0.522, F -0.601)]  [G loss: 0.707] \n",
      "2126 [D loss: (-0.032)(R 0.513, F -0.577)]  [G loss: 0.706] \n",
      "2127 [D loss: (-0.029)(R 0.530, F -0.587)]  [G loss: 0.720] \n",
      "2128 [D loss: (-0.031)(R 0.546, F -0.608)]  [G loss: 0.733] \n",
      "2129 [D loss: (-0.054)(R 0.522, F -0.630)]  [G loss: 0.746] \n",
      "2130 [D loss: (-0.034)(R 0.544, F -0.613)]  [G loss: 0.706] \n",
      "2131 [D loss: (-0.029)(R 0.527, F -0.584)]  [G loss: 0.717] \n",
      "2132 [D loss: (-0.025)(R 0.541, F -0.590)]  [G loss: 0.715] \n",
      "2133 [D loss: (-0.034)(R 0.508, F -0.576)]  [G loss: 0.709] \n",
      "2134 [D loss: (-0.047)(R 0.501, F -0.594)]  [G loss: 0.726] \n",
      "2135 [D loss: (-0.110)(R 0.462, F -0.681)]  [G loss: 0.684] \n",
      "2136 [D loss: (-0.063)(R 0.473, F -0.598)]  [G loss: 0.711] \n",
      "2137 [D loss: (-0.058)(R 0.472, F -0.588)]  [G loss: 0.695] \n",
      "2138 [D loss: (-0.047)(R 0.498, F -0.592)]  [G loss: 0.670] \n",
      "2139 [D loss: (-0.038)(R 0.508, F -0.584)]  [G loss: 0.693] \n",
      "2140 [D loss: (-0.022)(R 0.525, F -0.569)]  [G loss: 0.690] \n",
      "2141 [D loss: (-0.019)(R 0.543, F -0.581)]  [G loss: 0.686] \n",
      "2142 [D loss: (-0.047)(R 0.502, F -0.596)]  [G loss: 0.695] \n",
      "2143 [D loss: (-0.038)(R 0.508, F -0.584)]  [G loss: 0.700] \n",
      "2144 [D loss: (-0.037)(R 0.503, F -0.577)]  [G loss: 0.682] \n",
      "2145 [D loss: (-0.032)(R 0.486, F -0.550)]  [G loss: 0.669] \n",
      "2146 [D loss: (-0.023)(R 0.527, F -0.573)]  [G loss: 0.675] \n",
      "2147 [D loss: (-0.043)(R 0.479, F -0.564)]  [G loss: 0.667] \n",
      "2148 [D loss: (-0.044)(R 0.484, F -0.571)]  [G loss: 0.674] \n",
      "2149 [D loss: (-0.041)(R 0.470, F -0.551)]  [G loss: 0.659] \n",
      "2150 [D loss: (-0.030)(R 0.480, F -0.540)]  [G loss: 0.661] \n",
      "2151 [D loss: (-0.037)(R 0.476, F -0.550)]  [G loss: 0.668] \n",
      "2152 [D loss: (-0.037)(R 0.472, F -0.546)]  [G loss: 0.674] \n",
      "2153 [D loss: (-0.021)(R 0.481, F -0.523)]  [G loss: 0.654] \n",
      "2154 [D loss: (-0.025)(R 0.463, F -0.513)]  [G loss: 0.645] \n",
      "2155 [D loss: (-0.026)(R 0.487, F -0.539)]  [G loss: 0.641] \n",
      "2156 [D loss: (-0.046)(R 0.460, F -0.553)]  [G loss: 0.679] \n",
      "2157 [D loss: (-0.024)(R 0.502, F -0.550)]  [G loss: 0.673] \n",
      "2158 [D loss: (-0.053)(R 0.476, F -0.582)]  [G loss: 0.648] \n",
      "2159 [D loss: (-0.036)(R 0.504, F -0.576)]  [G loss: 0.689] \n",
      "2160 [D loss: (-0.030)(R 0.490, F -0.551)]  [G loss: 0.674] \n",
      "2161 [D loss: (-0.019)(R 0.514, F -0.553)]  [G loss: 0.674] \n",
      "2162 [D loss: (-0.041)(R 0.501, F -0.584)]  [G loss: 0.691] \n",
      "2163 [D loss: (-0.032)(R 0.528, F -0.591)]  [G loss: 0.693] \n",
      "2164 [D loss: (-0.025)(R 0.498, F -0.548)]  [G loss: 0.687] \n",
      "2165 [D loss: (-0.036)(R 0.516, F -0.587)]  [G loss: 0.713] \n",
      "2166 [D loss: (-0.033)(R 0.535, F -0.601)]  [G loss: 0.710] \n",
      "2167 [D loss: (-0.015)(R 0.567, F -0.598)]  [G loss: 0.720] \n",
      "2168 [D loss: (-0.044)(R 0.532, F -0.620)]  [G loss: 0.716] \n",
      "2169 [D loss: (-0.067)(R 0.483, F -0.616)]  [G loss: 0.716] \n",
      "2170 [D loss: (-0.030)(R 0.511, F -0.571)]  [G loss: 0.698] \n",
      "2171 [D loss: (-0.026)(R 0.505, F -0.558)]  [G loss: 0.703] \n",
      "2172 [D loss: (-0.024)(R 0.509, F -0.557)]  [G loss: 0.694] \n",
      "2173 [D loss: (-0.027)(R 0.519, F -0.572)]  [G loss: 0.693] \n",
      "2174 [D loss: (-0.028)(R 0.512, F -0.568)]  [G loss: 0.688] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2175 [D loss: (-0.052)(R 0.501, F -0.606)]  [G loss: 0.689] \n",
      "2176 [D loss: (-0.010)(R 0.547, F -0.566)]  [G loss: 0.704] \n",
      "2177 [D loss: (-0.031)(R 0.500, F -0.563)]  [G loss: 0.677] \n",
      "2178 [D loss: (-0.012)(R 0.532, F -0.556)]  [G loss: 0.676] \n",
      "2179 [D loss: (-0.021)(R 0.520, F -0.563)]  [G loss: 0.674] \n",
      "2180 [D loss: (-0.036)(R 0.488, F -0.559)]  [G loss: 0.675] \n",
      "2181 [D loss: (-0.037)(R 0.493, F -0.567)]  [G loss: 0.693] \n",
      "2182 [D loss: (-0.038)(R 0.471, F -0.547)]  [G loss: 0.666] \n",
      "2183 [D loss: (-0.035)(R 0.486, F -0.556)]  [G loss: 0.652] \n",
      "2184 [D loss: (-0.030)(R 0.482, F -0.543)]  [G loss: 0.686] \n",
      "2185 [D loss: (-0.045)(R 0.475, F -0.565)]  [G loss: 0.654] \n",
      "2186 [D loss: (-0.039)(R 0.476, F -0.555)]  [G loss: 0.691] \n",
      "2187 [D loss: (-0.022)(R 0.497, F -0.541)]  [G loss: 0.651] \n",
      "2188 [D loss: (-0.026)(R 0.498, F -0.549)]  [G loss: 0.686] \n",
      "2189 [D loss: (-0.037)(R 0.465, F -0.540)]  [G loss: 0.674] \n",
      "2190 [D loss: (-0.026)(R 0.496, F -0.549)]  [G loss: 0.676] \n",
      "2191 [D loss: (-0.027)(R 0.522, F -0.576)]  [G loss: 0.679] \n",
      "2192 [D loss: (-0.034)(R 0.509, F -0.576)]  [G loss: 0.664] \n",
      "2193 [D loss: (-0.036)(R 0.508, F -0.580)]  [G loss: 0.676] \n",
      "2194 [D loss: (-0.033)(R 0.487, F -0.552)]  [G loss: 0.682] \n",
      "2195 [D loss: (-0.020)(R 0.506, F -0.547)]  [G loss: 0.681] \n",
      "2196 [D loss: (-0.026)(R 0.492, F -0.544)]  [G loss: 0.674] \n",
      "2197 [D loss: (-0.057)(R 0.465, F -0.580)]  [G loss: 0.679] \n",
      "2198 [D loss: (-0.036)(R 0.521, F -0.593)]  [G loss: 0.677] \n",
      "2199 [D loss: (-0.038)(R 0.496, F -0.572)]  [G loss: 0.676] \n",
      "2200 [D loss: (-0.032)(R 0.475, F -0.538)]  [G loss: 0.663] \n",
      "2201 [D loss: (-0.026)(R 0.495, F -0.546)]  [G loss: 0.662] \n",
      "2202 [D loss: (-0.020)(R 0.505, F -0.546)]  [G loss: 0.679] \n",
      "2203 [D loss: (-0.029)(R 0.496, F -0.553)]  [G loss: 0.677] \n",
      "2204 [D loss: (-0.030)(R 0.500, F -0.560)]  [G loss: 0.661] \n",
      "2205 [D loss: (-0.038)(R 0.491, F -0.566)]  [G loss: 0.666] \n",
      "2206 [D loss: (-0.018)(R 0.518, F -0.555)]  [G loss: 0.677] \n",
      "2207 [D loss: (-0.012)(R 0.544, F -0.568)]  [G loss: 0.692] \n",
      "2208 [D loss: (-0.048)(R 0.522, F -0.617)]  [G loss: 0.716] \n",
      "2209 [D loss: (-0.111)(R 0.484, F -0.707)]  [G loss: 0.694] \n",
      "2210 [D loss: (-0.063)(R 0.475, F -0.602)]  [G loss: 0.711] \n",
      "2211 [D loss: (-0.023)(R 0.532, F -0.578)]  [G loss: 0.698] \n",
      "2212 [D loss: (-0.022)(R 0.528, F -0.571)]  [G loss: 0.677] \n",
      "2213 [D loss: (-0.028)(R 0.506, F -0.561)]  [G loss: 0.676] \n",
      "2214 [D loss: (-0.009)(R 0.525, F -0.543)]  [G loss: 0.678] \n",
      "2215 [D loss: (-0.033)(R 0.498, F -0.565)]  [G loss: 0.661] \n",
      "2216 [D loss: (-0.051)(R 0.483, F -0.585)]  [G loss: 0.668] \n",
      "2217 [D loss: (-0.030)(R 0.480, F -0.540)]  [G loss: 0.663] \n",
      "2218 [D loss: (-0.037)(R 0.474, F -0.548)]  [G loss: 0.672] \n",
      "2219 [D loss: (-0.023)(R 0.486, F -0.532)]  [G loss: 0.646] \n",
      "2220 [D loss: (-0.027)(R 0.477, F -0.530)]  [G loss: 0.658] \n",
      "2221 [D loss: (-0.029)(R 0.490, F -0.547)]  [G loss: 0.658] \n",
      "2222 [D loss: (-0.032)(R 0.493, F -0.556)]  [G loss: 0.673] \n",
      "2223 [D loss: (-0.046)(R 0.490, F -0.582)]  [G loss: 0.674] \n",
      "2224 [D loss: (-0.041)(R 0.481, F -0.562)]  [G loss: 0.679] \n",
      "2225 [D loss: (-0.026)(R 0.498, F -0.549)]  [G loss: 0.661] \n",
      "2226 [D loss: (-0.037)(R 0.501, F -0.576)]  [G loss: 0.672] \n",
      "2227 [D loss: (-0.054)(R 0.475, F -0.582)]  [G loss: 0.663] \n",
      "2228 [D loss: (-0.058)(R 0.462, F -0.579)]  [G loss: 0.663] \n",
      "2229 [D loss: (-0.020)(R 0.490, F -0.530)]  [G loss: 0.652] \n",
      "2230 [D loss: (-0.029)(R 0.471, F -0.529)]  [G loss: 0.652] \n",
      "2231 [D loss: (-0.023)(R 0.492, F -0.537)]  [G loss: 0.662] \n",
      "2232 [D loss: (-0.031)(R 0.485, F -0.547)]  [G loss: 0.663] \n",
      "2233 [D loss: (-0.016)(R 0.509, F -0.542)]  [G loss: 0.677] \n",
      "2234 [D loss: (-0.044)(R 0.489, F -0.577)]  [G loss: 0.686] \n",
      "2235 [D loss: (-0.061)(R 0.458, F -0.580)]  [G loss: 0.698] \n",
      "2236 [D loss: (-0.019)(R 0.490, F -0.529)]  [G loss: 0.677] \n",
      "2237 [D loss: (-0.006)(R 0.553, F -0.565)]  [G loss: 0.687] \n",
      "2238 [D loss: (-0.012)(R 0.539, F -0.563)]  [G loss: 0.691] \n",
      "2239 [D loss: (-0.024)(R 0.543, F -0.591)]  [G loss: 0.702] \n",
      "2240 [D loss: (-0.055)(R 0.516, F -0.625)]  [G loss: 0.709] \n",
      "2241 [D loss: (-0.060)(R 0.539, F -0.660)]  [G loss: 0.745] \n",
      "2242 [D loss: (-0.028)(R 0.535, F -0.590)]  [G loss: 0.708] \n",
      "2243 [D loss: (-0.017)(R 0.543, F -0.577)]  [G loss: 0.692] \n",
      "2244 [D loss: (-0.024)(R 0.537, F -0.586)]  [G loss: 0.703] \n",
      "2245 [D loss: (-0.035)(R 0.523, F -0.592)]  [G loss: 0.708] \n",
      "2246 [D loss: (-0.024)(R 0.521, F -0.569)]  [G loss: 0.696] \n",
      "2247 [D loss: (-0.037)(R 0.523, F -0.597)]  [G loss: 0.686] \n",
      "2248 [D loss: (-0.051)(R 0.521, F -0.623)]  [G loss: 0.707] \n",
      "2249 [D loss: (-0.043)(R 0.501, F -0.588)]  [G loss: 0.657] \n",
      "2250 [D loss: (-0.029)(R 0.470, F -0.529)]  [G loss: 0.646] \n",
      "2251 [D loss: (-0.017)(R 0.493, F -0.527)]  [G loss: 0.652] \n",
      "2252 [D loss: (-0.039)(R 0.464, F -0.543)]  [G loss: 0.652] \n",
      "2253 [D loss: (-0.035)(R 0.472, F -0.542)]  [G loss: 0.647] \n",
      "2254 [D loss: (-0.031)(R 0.476, F -0.539)]  [G loss: 0.644] \n",
      "2255 [D loss: (-0.029)(R 0.492, F -0.550)]  [G loss: 0.661] \n",
      "2256 [D loss: (-0.031)(R 0.483, F -0.546)]  [G loss: 0.672] \n",
      "2257 [D loss: (-0.037)(R 0.477, F -0.551)]  [G loss: 0.651] \n",
      "2258 [D loss: (-0.028)(R 0.490, F -0.545)]  [G loss: 0.648] \n",
      "2259 [D loss: (-0.028)(R 0.480, F -0.536)]  [G loss: 0.628] \n",
      "2260 [D loss: (-0.041)(R 0.443, F -0.525)]  [G loss: 0.650] \n",
      "2261 [D loss: (-0.027)(R 0.493, F -0.547)]  [G loss: 0.664] \n",
      "2262 [D loss: (-0.026)(R 0.466, F -0.517)]  [G loss: 0.657] \n",
      "2263 [D loss: (-0.034)(R 0.463, F -0.531)]  [G loss: 0.630] \n",
      "2264 [D loss: (-0.044)(R 0.462, F -0.549)]  [G loss: 0.656] \n",
      "2265 [D loss: (-0.034)(R 0.486, F -0.553)]  [G loss: 0.663] \n",
      "2266 [D loss: (-0.030)(R 0.494, F -0.554)]  [G loss: 0.679] \n",
      "2267 [D loss: (-0.053)(R 0.505, F -0.611)]  [G loss: 0.665] \n",
      "2268 [D loss: (-0.052)(R 0.503, F -0.607)]  [G loss: 0.701] \n",
      "2269 [D loss: (-0.028)(R 0.495, F -0.551)]  [G loss: 0.672] \n",
      "2270 [D loss: (-0.020)(R 0.517, F -0.558)]  [G loss: 0.676] \n",
      "2271 [D loss: (-0.019)(R 0.527, F -0.566)]  [G loss: 0.687] \n",
      "2272 [D loss: (-0.030)(R 0.520, F -0.579)]  [G loss: 0.687] \n",
      "2273 [D loss: (-0.013)(R 0.546, F -0.573)]  [G loss: 0.686] \n",
      "2274 [D loss: (-0.037)(R 0.503, F -0.576)]  [G loss: 0.696] \n",
      "2275 [D loss: (-0.050)(R 0.497, F -0.596)]  [G loss: 0.668] \n",
      "2276 [D loss: (-0.057)(R 0.470, F -0.583)]  [G loss: 0.663] \n",
      "2277 [D loss: (-0.011)(R 0.496, F -0.518)]  [G loss: 0.648] \n",
      "2278 [D loss: (-0.031)(R 0.482, F -0.543)]  [G loss: 0.667] \n",
      "2279 [D loss: (-0.018)(R 0.492, F -0.528)]  [G loss: 0.649] \n",
      "2280 [D loss: (-0.025)(R 0.479, F -0.528)]  [G loss: 0.654] \n",
      "2281 [D loss: (-0.020)(R 0.489, F -0.528)]  [G loss: 0.656] \n",
      "2282 [D loss: (-0.068)(R 0.448, F -0.584)]  [G loss: 0.656] \n",
      "2283 [D loss: (-0.024)(R 0.569, F -0.616)]  [G loss: 0.693] \n",
      "2284 [D loss: (-0.034)(R 0.483, F -0.550)]  [G loss: 0.658] \n",
      "2285 [D loss: (-0.026)(R 0.470, F -0.522)]  [G loss: 0.635] \n",
      "2286 [D loss: (-0.029)(R 0.495, F -0.554)]  [G loss: 0.654] \n",
      "2287 [D loss: (-0.030)(R 0.471, F -0.530)]  [G loss: 0.636] \n",
      "2288 [D loss: (-0.022)(R 0.494, F -0.537)]  [G loss: 0.632] \n",
      "2289 [D loss: (-0.031)(R 0.467, F -0.528)]  [G loss: 0.646] \n",
      "2290 [D loss: (-0.046)(R 0.434, F -0.525)]  [G loss: 0.636] \n",
      "2291 [D loss: (-0.037)(R 0.463, F -0.536)]  [G loss: 0.627] \n",
      "2292 [D loss: (-0.023)(R 0.476, F -0.522)]  [G loss: 0.637] \n",
      "2293 [D loss: (-0.025)(R 0.501, F -0.552)]  [G loss: 0.658] \n",
      "2294 [D loss: (-0.031)(R 0.496, F -0.558)]  [G loss: 0.660] \n",
      "2295 [D loss: (-0.064)(R 0.475, F -0.603)]  [G loss: 0.656] \n",
      "2296 [D loss: (-0.034)(R 0.488, F -0.556)]  [G loss: 0.651] \n",
      "2297 [D loss: (-0.022)(R 0.503, F -0.547)]  [G loss: 0.649] \n",
      "2298 [D loss: (-0.035)(R 0.485, F -0.555)]  [G loss: 0.661] \n",
      "2299 [D loss: (-0.029)(R 0.509, F -0.566)]  [G loss: 0.668] \n",
      "2300 [D loss: (-0.038)(R 0.501, F -0.577)]  [G loss: 0.678] \n",
      "2301 [D loss: (-0.011)(R 0.523, F -0.544)]  [G loss: 0.663] \n",
      "2302 [D loss: (-0.044)(R 0.520, F -0.608)]  [G loss: 0.680] \n",
      "2303 [D loss: (-0.016)(R 0.529, F -0.562)]  [G loss: 0.675] \n",
      "2304 [D loss: (-0.032)(R 0.515, F -0.579)]  [G loss: 0.682] \n",
      "2305 [D loss: (-0.033)(R 0.517, F -0.583)]  [G loss: 0.671] \n",
      "2306 [D loss: (-0.018)(R 0.527, F -0.564)]  [G loss: 0.669] \n",
      "2307 [D loss: (-0.026)(R 0.511, F -0.562)]  [G loss: 0.671] \n",
      "2308 [D loss: (-0.031)(R 0.504, F -0.567)]  [G loss: 0.666] \n",
      "2309 [D loss: (-0.030)(R 0.490, F -0.549)]  [G loss: 0.667] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2310 [D loss: (-0.049)(R 0.489, F -0.586)]  [G loss: 0.637] \n",
      "2311 [D loss: (-0.071)(R 0.449, F -0.591)]  [G loss: 0.676] \n",
      "2312 [D loss: (-0.017)(R 0.489, F -0.523)]  [G loss: 0.636] \n",
      "2313 [D loss: (-0.048)(R 0.483, F -0.579)]  [G loss: 0.641] \n",
      "2314 [D loss: (-0.023)(R 0.483, F -0.529)]  [G loss: 0.623] \n",
      "2315 [D loss: (-0.039)(R 0.456, F -0.534)]  [G loss: 0.633] \n",
      "2316 [D loss: (-0.032)(R 0.457, F -0.522)]  [G loss: 0.620] \n",
      "2317 [D loss: (-0.019)(R 0.459, F -0.496)]  [G loss: 0.596] \n",
      "2318 [D loss: (-0.015)(R 0.470, F -0.499)]  [G loss: 0.614] \n",
      "2319 [D loss: (-0.033)(R 0.461, F -0.527)]  [G loss: 0.640] \n",
      "2320 [D loss: (-0.039)(R 0.452, F -0.530)]  [G loss: 0.626] \n",
      "2321 [D loss: (-0.060)(R 0.448, F -0.568)]  [G loss: 0.616] \n",
      "2322 [D loss: (-0.019)(R 0.466, F -0.504)]  [G loss: 0.631] \n",
      "2323 [D loss: (-0.029)(R 0.473, F -0.531)]  [G loss: 0.629] \n",
      "2324 [D loss: (-0.032)(R 0.452, F -0.515)]  [G loss: 0.634] \n",
      "2325 [D loss: (-0.037)(R 0.449, F -0.523)]  [G loss: 0.618] \n",
      "2326 [D loss: (-0.033)(R 0.456, F -0.522)]  [G loss: 0.629] \n",
      "2327 [D loss: (-0.044)(R 0.449, F -0.537)]  [G loss: 0.615] \n",
      "2328 [D loss: (-0.031)(R 0.479, F -0.540)]  [G loss: 0.640] \n",
      "2329 [D loss: (-0.027)(R 0.478, F -0.532)]  [G loss: 0.636] \n",
      "2330 [D loss: (-0.035)(R 0.471, F -0.541)]  [G loss: 0.652] \n",
      "2331 [D loss: (-0.030)(R 0.478, F -0.538)]  [G loss: 0.647] \n",
      "2332 [D loss: (-0.015)(R 0.507, F -0.538)]  [G loss: 0.655] \n",
      "2333 [D loss: (-0.022)(R 0.518, F -0.562)]  [G loss: 0.653] \n",
      "2334 [D loss: (-0.022)(R 0.528, F -0.572)]  [G loss: 0.659] \n",
      "2335 [D loss: (-0.027)(R 0.510, F -0.564)]  [G loss: 0.654] \n",
      "2336 [D loss: (-0.034)(R 0.491, F -0.558)]  [G loss: 0.664] \n",
      "2337 [D loss: (-0.034)(R 0.486, F -0.553)]  [G loss: 0.649] \n",
      "2338 [D loss: (-0.023)(R 0.482, F -0.528)]  [G loss: 0.628] \n",
      "2339 [D loss: (-0.041)(R 0.455, F -0.538)]  [G loss: 0.628] \n",
      "2340 [D loss: (-0.015)(R 0.490, F -0.520)]  [G loss: 0.622] \n",
      "2341 [D loss: (-0.020)(R 0.472, F -0.512)]  [G loss: 0.611] \n",
      "2342 [D loss: (-0.026)(R 0.448, F -0.499)]  [G loss: 0.626] \n",
      "2343 [D loss: (-0.024)(R 0.450, F -0.499)]  [G loss: 0.614] \n",
      "2344 [D loss: (-0.023)(R 0.460, F -0.505)]  [G loss: 0.602] \n",
      "2345 [D loss: (-0.036)(R 0.424, F -0.497)]  [G loss: 0.594] \n",
      "2346 [D loss: (-0.048)(R 0.427, F -0.523)]  [G loss: 0.620] \n",
      "2347 [D loss: (-0.024)(R 0.458, F -0.506)]  [G loss: 0.608] \n",
      "2348 [D loss: (-0.031)(R 0.439, F -0.502)]  [G loss: 0.602] \n",
      "2349 [D loss: (-0.027)(R 0.458, F -0.512)]  [G loss: 0.628] \n",
      "2350 [D loss: (-0.018)(R 0.448, F -0.485)]  [G loss: 0.605] \n",
      "2351 [D loss: (-0.022)(R 0.442, F -0.486)]  [G loss: 0.602] \n",
      "2352 [D loss: (-0.030)(R 0.450, F -0.510)]  [G loss: 0.618] \n",
      "2353 [D loss: (-0.019)(R 0.460, F -0.499)]  [G loss: 0.620] \n",
      "2354 [D loss: (-0.032)(R 0.456, F -0.521)]  [G loss: 0.624] \n",
      "2355 [D loss: (-0.028)(R 0.468, F -0.525)]  [G loss: 0.619] \n",
      "2356 [D loss: (-0.014)(R 0.483, F -0.511)]  [G loss: 0.638] \n",
      "2357 [D loss: (-0.019)(R 0.468, F -0.505)]  [G loss: 0.628] \n",
      "2358 [D loss: (-0.025)(R 0.476, F -0.526)]  [G loss: 0.630] \n",
      "2359 [D loss: (-0.027)(R 0.472, F -0.526)]  [G loss: 0.637] \n",
      "2360 [D loss: (-0.018)(R 0.478, F -0.514)]  [G loss: 0.635] \n",
      "2361 [D loss: (-0.028)(R 0.472, F -0.528)]  [G loss: 0.634] \n",
      "2362 [D loss: (-0.038)(R 0.493, F -0.570)]  [G loss: 0.654] \n",
      "2363 [D loss: (-0.031)(R 0.490, F -0.551)]  [G loss: 0.650] \n",
      "2364 [D loss: (-0.026)(R 0.485, F -0.537)]  [G loss: 0.644] \n",
      "2365 [D loss: (-0.038)(R 0.470, F -0.546)]  [G loss: 0.625] \n",
      "2366 [D loss: (-0.030)(R 0.474, F -0.534)]  [G loss: 0.622] \n",
      "2367 [D loss: (-0.034)(R 0.450, F -0.519)]  [G loss: 0.626] \n",
      "2368 [D loss: (-0.024)(R 0.453, F -0.501)]  [G loss: 0.616] \n",
      "2369 [D loss: (-0.034)(R 0.444, F -0.512)]  [G loss: 0.607] \n",
      "2370 [D loss: (-0.040)(R 0.419, F -0.500)]  [G loss: 0.605] \n",
      "2371 [D loss: (-0.024)(R 0.468, F -0.516)]  [G loss: 0.605] \n",
      "2372 [D loss: (-0.027)(R 0.440, F -0.495)]  [G loss: 0.597] \n",
      "2373 [D loss: (-0.017)(R 0.433, F -0.467)]  [G loss: 0.593] \n",
      "2374 [D loss: (-0.023)(R 0.430, F -0.476)]  [G loss: 0.578] \n",
      "2375 [D loss: (-0.039)(R 0.403, F -0.481)]  [G loss: 0.599] \n",
      "2376 [D loss: (-0.035)(R 0.424, F -0.494)]  [G loss: 0.581] \n",
      "2377 [D loss: (-0.033)(R 0.412, F -0.479)]  [G loss: 0.566] \n",
      "2378 [D loss: (-0.028)(R 0.423, F -0.478)]  [G loss: 0.579] \n",
      "2379 [D loss: (-0.021)(R 0.441, F -0.483)]  [G loss: 0.569] \n",
      "2380 [D loss: (-0.039)(R 0.407, F -0.484)]  [G loss: 0.581] \n",
      "2381 [D loss: (-0.032)(R 0.425, F -0.490)]  [G loss: 0.583] \n",
      "2382 [D loss: (-0.032)(R 0.437, F -0.500)]  [G loss: 0.579] \n",
      "2383 [D loss: (-0.020)(R 0.451, F -0.491)]  [G loss: 0.602] \n",
      "2384 [D loss: (-0.027)(R 0.441, F -0.494)]  [G loss: 0.601] \n",
      "2385 [D loss: (-0.023)(R 0.472, F -0.517)]  [G loss: 0.606] \n",
      "2386 [D loss: (-0.012)(R 0.479, F -0.504)]  [G loss: 0.618] \n",
      "2387 [D loss: (-0.027)(R 0.456, F -0.510)]  [G loss: 0.610] \n",
      "2388 [D loss: (-0.034)(R 0.482, F -0.551)]  [G loss: 0.644] \n",
      "2389 [D loss: (-0.027)(R 0.461, F -0.516)]  [G loss: 0.628] \n",
      "2390 [D loss: (-0.021)(R 0.458, F -0.499)]  [G loss: 0.627] \n",
      "2391 [D loss: (-0.026)(R 0.475, F -0.526)]  [G loss: 0.642] \n",
      "2392 [D loss: (-0.030)(R 0.495, F -0.555)]  [G loss: 0.650] \n",
      "2393 [D loss: (-0.070)(R 0.484, F -0.623)]  [G loss: 0.645] \n",
      "2394 [D loss: (-0.017)(R 0.469, F -0.503)]  [G loss: 0.624] \n",
      "2395 [D loss: (-0.028)(R 0.487, F -0.543)]  [G loss: 0.616] \n",
      "2396 [D loss: (-0.030)(R 0.470, F -0.530)]  [G loss: 0.608] \n",
      "2397 [D loss: (-0.017)(R 0.482, F -0.515)]  [G loss: 0.624] \n",
      "2398 [D loss: (-0.037)(R 0.444, F -0.518)]  [G loss: 0.626] \n",
      "2399 [D loss: (-0.027)(R 0.448, F -0.502)]  [G loss: 0.623] \n",
      "2400 [D loss: (-0.014)(R 0.462, F -0.491)]  [G loss: 0.598] \n",
      "2401 [D loss: (-0.029)(R 0.469, F -0.527)]  [G loss: 0.630] \n",
      "2402 [D loss: (-0.032)(R 0.453, F -0.516)]  [G loss: 0.611] \n",
      "2403 [D loss: (-0.040)(R 0.447, F -0.528)]  [G loss: 0.631] \n",
      "2404 [D loss: (-0.018)(R 0.455, F -0.492)]  [G loss: 0.610] \n",
      "2405 [D loss: (-0.034)(R 0.455, F -0.524)]  [G loss: 0.603] \n",
      "2406 [D loss: (-0.016)(R 0.447, F -0.479)]  [G loss: 0.591] \n",
      "2407 [D loss: (-0.024)(R 0.448, F -0.496)]  [G loss: 0.593] \n",
      "2408 [D loss: (-0.039)(R 0.430, F -0.507)]  [G loss: 0.587] \n",
      "2409 [D loss: (-0.018)(R 0.432, F -0.468)]  [G loss: 0.574] \n",
      "2410 [D loss: (-0.015)(R 0.454, F -0.484)]  [G loss: 0.594] \n",
      "2411 [D loss: (-0.024)(R 0.429, F -0.478)]  [G loss: 0.591] \n",
      "2412 [D loss: (-0.035)(R 0.444, F -0.514)]  [G loss: 0.586] \n",
      "2413 [D loss: (-0.034)(R 0.423, F -0.490)]  [G loss: 0.582] \n",
      "2414 [D loss: (-0.021)(R 0.417, F -0.458)]  [G loss: 0.557] \n",
      "2415 [D loss: (-0.021)(R 0.414, F -0.456)]  [G loss: 0.563] \n",
      "2416 [D loss: (-0.023)(R 0.407, F -0.453)]  [G loss: 0.554] \n",
      "2417 [D loss: (-0.037)(R 0.403, F -0.476)]  [G loss: 0.578] \n",
      "2418 [D loss: (-0.022)(R 0.414, F -0.459)]  [G loss: 0.556] \n",
      "2419 [D loss: (-0.021)(R 0.413, F -0.455)]  [G loss: 0.542] \n",
      "2420 [D loss: (-0.023)(R 0.411, F -0.457)]  [G loss: 0.558] \n",
      "2421 [D loss: (-0.017)(R 0.416, F -0.451)]  [G loss: 0.558] \n",
      "2422 [D loss: (-0.037)(R 0.406, F -0.481)]  [G loss: 0.573] \n",
      "2423 [D loss: (-0.042)(R 0.405, F -0.488)]  [G loss: 0.543] \n",
      "2424 [D loss: (-0.016)(R 0.417, F -0.448)]  [G loss: 0.558] \n",
      "2425 [D loss: (-0.025)(R 0.422, F -0.473)]  [G loss: 0.582] \n",
      "2426 [D loss: (-0.023)(R 0.436, F -0.483)]  [G loss: 0.581] \n",
      "2427 [D loss: (-0.042)(R 0.444, F -0.527)]  [G loss: 0.575] \n",
      "2428 [D loss: (-0.043)(R 0.422, F -0.509)]  [G loss: 0.582] \n",
      "2429 [D loss: (-0.021)(R 0.432, F -0.475)]  [G loss: 0.581] \n",
      "2430 [D loss: (-0.025)(R 0.435, F -0.485)]  [G loss: 0.587] \n",
      "2431 [D loss: (-0.023)(R 0.444, F -0.491)]  [G loss: 0.605] \n",
      "2432 [D loss: (-0.034)(R 0.431, F -0.499)]  [G loss: 0.605] \n",
      "2433 [D loss: (-0.021)(R 0.449, F -0.492)]  [G loss: 0.596] \n",
      "2434 [D loss: (-0.021)(R 0.447, F -0.488)]  [G loss: 0.600] \n",
      "2435 [D loss: (-0.034)(R 0.444, F -0.512)]  [G loss: 0.622] \n",
      "2436 [D loss: (-0.022)(R 0.457, F -0.500)]  [G loss: 0.617] \n",
      "2437 [D loss: (-0.023)(R 0.493, F -0.540)]  [G loss: 0.635] \n",
      "2438 [D loss: (-0.054)(R 0.459, F -0.568)]  [G loss: 0.631] \n",
      "2439 [D loss: (-0.011)(R 0.473, F -0.495)]  [G loss: 0.616] \n",
      "2440 [D loss: (-0.031)(R 0.467, F -0.530)]  [G loss: 0.620] \n",
      "2441 [D loss: (-0.019)(R 0.470, F -0.508)]  [G loss: 0.605] \n",
      "2442 [D loss: (-0.022)(R 0.457, F -0.502)]  [G loss: 0.609] \n",
      "2443 [D loss: (-0.018)(R 0.469, F -0.504)]  [G loss: 0.592] \n",
      "2444 [D loss: (-0.020)(R 0.466, F -0.506)]  [G loss: 0.611] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2445 [D loss: (-0.026)(R 0.454, F -0.507)]  [G loss: 0.598] \n",
      "2446 [D loss: (-0.036)(R 0.432, F -0.505)]  [G loss: 0.586] \n",
      "2447 [D loss: (-0.046)(R 0.432, F -0.523)]  [G loss: 0.599] \n",
      "2448 [D loss: (-0.015)(R 0.456, F -0.486)]  [G loss: 0.578] \n",
      "2449 [D loss: (-0.026)(R 0.431, F -0.483)]  [G loss: 0.574] \n",
      "2450 [D loss: (-0.021)(R 0.423, F -0.466)]  [G loss: 0.566] \n",
      "2451 [D loss: (-0.032)(R 0.404, F -0.468)]  [G loss: 0.564] \n",
      "2452 [D loss: (-0.025)(R 0.391, F -0.442)]  [G loss: 0.539] \n",
      "2453 [D loss: (-0.020)(R 0.391, F -0.432)]  [G loss: 0.536] \n",
      "2454 [D loss: (-0.037)(R 0.365, F -0.440)]  [G loss: 0.532] \n",
      "2455 [D loss: (-0.029)(R 0.365, F -0.423)]  [G loss: 0.516] \n",
      "2456 [D loss: (-0.027)(R 0.370, F -0.423)]  [G loss: 0.516] \n",
      "2457 [D loss: (-0.048)(R 0.364, F -0.460)]  [G loss: 0.527] \n",
      "2458 [D loss: (-0.040)(R 0.368, F -0.447)]  [G loss: 0.530] \n",
      "2459 [D loss: (-0.028)(R 0.394, F -0.450)]  [G loss: 0.508] \n",
      "2460 [D loss: (-0.044)(R 0.356, F -0.445)]  [G loss: 0.537] \n",
      "2461 [D loss: (-0.025)(R 0.381, F -0.430)]  [G loss: 0.533] \n",
      "2462 [D loss: (-0.027)(R 0.389, F -0.443)]  [G loss: 0.546] \n",
      "2463 [D loss: (-0.020)(R 0.419, F -0.459)]  [G loss: 0.549] \n",
      "2464 [D loss: (-0.031)(R 0.402, F -0.464)]  [G loss: 0.569] \n",
      "2465 [D loss: (-0.020)(R 0.439, F -0.479)]  [G loss: 0.590] \n",
      "2466 [D loss: (-0.018)(R 0.436, F -0.472)]  [G loss: 0.574] \n",
      "2467 [D loss: (-0.017)(R 0.439, F -0.474)]  [G loss: 0.573] \n",
      "2468 [D loss: (-0.035)(R 0.432, F -0.503)]  [G loss: 0.602] \n",
      "2469 [D loss: (-0.036)(R 0.449, F -0.522)]  [G loss: 0.602] \n",
      "2470 [D loss: (-0.039)(R 0.432, F -0.511)]  [G loss: 0.617] \n",
      "2471 [D loss: (-0.013)(R 0.469, F -0.496)]  [G loss: 0.603] \n",
      "2472 [D loss: (-0.017)(R 0.469, F -0.503)]  [G loss: 0.612] \n",
      "2473 [D loss: (-0.029)(R 0.455, F -0.512)]  [G loss: 0.613] \n",
      "2474 [D loss: (-0.021)(R 0.474, F -0.515)]  [G loss: 0.612] \n",
      "2475 [D loss: (-0.026)(R 0.467, F -0.518)]  [G loss: 0.606] \n",
      "2476 [D loss: (-0.043)(R 0.441, F -0.528)]  [G loss: 0.611] \n",
      "2477 [D loss: (-0.020)(R 0.492, F -0.532)]  [G loss: 0.617] \n",
      "2478 [D loss: (-0.029)(R 0.455, F -0.514)]  [G loss: 0.595] \n",
      "2479 [D loss: (-0.004)(R 0.488, F -0.496)]  [G loss: 0.585] \n",
      "2480 [D loss: (-0.007)(R 0.457, F -0.470)]  [G loss: 0.586] \n",
      "2481 [D loss: (-0.012)(R 0.455, F -0.478)]  [G loss: 0.598] \n",
      "2482 [D loss: (-0.023)(R 0.445, F -0.490)]  [G loss: 0.593] \n",
      "2483 [D loss: (-0.026)(R 0.426, F -0.479)]  [G loss: 0.583] \n",
      "2484 [D loss: (-0.035)(R 0.423, F -0.492)]  [G loss: 0.571] \n",
      "2485 [D loss: (-0.018)(R 0.422, F -0.458)]  [G loss: 0.562] \n",
      "2486 [D loss: (-0.033)(R 0.408, F -0.474)]  [G loss: 0.552] \n",
      "2487 [D loss: (-0.037)(R 0.392, F -0.465)]  [G loss: 0.555] \n",
      "2488 [D loss: (-0.038)(R 0.372, F -0.449)]  [G loss: 0.522] \n",
      "2489 [D loss: (-0.024)(R 0.383, F -0.431)]  [G loss: 0.531] \n",
      "2490 [D loss: (-0.035)(R 0.368, F -0.439)]  [G loss: 0.518] \n",
      "2491 [D loss: (-0.019)(R 0.374, F -0.413)]  [G loss: 0.518] \n",
      "2492 [D loss: (-0.039)(R 0.372, F -0.449)]  [G loss: 0.535] \n",
      "2493 [D loss: (-0.025)(R 0.410, F -0.461)]  [G loss: 0.522] \n",
      "2494 [D loss: (-0.023)(R 0.389, F -0.434)]  [G loss: 0.537] \n",
      "2495 [D loss: (-0.026)(R 0.387, F -0.439)]  [G loss: 0.535] \n",
      "2496 [D loss: (-0.034)(R 0.389, F -0.458)]  [G loss: 0.559] \n",
      "2497 [D loss: (-0.025)(R 0.423, F -0.474)]  [G loss: 0.569] \n",
      "2498 [D loss: (-0.024)(R 0.424, F -0.472)]  [G loss: 0.562] \n",
      "2499 [D loss: (-0.014)(R 0.446, F -0.473)]  [G loss: 0.571] \n",
      "2500 [D loss: (-0.018)(R 0.433, F -0.469)]  [G loss: 0.558] \n",
      "2501 [D loss: (-0.018)(R 0.426, F -0.461)]  [G loss: 0.583] \n",
      "2502 [D loss: (-0.025)(R 0.439, F -0.490)]  [G loss: 0.585] \n",
      "2503 [D loss: (-0.025)(R 0.440, F -0.489)]  [G loss: 0.600] \n",
      "2504 [D loss: (-0.037)(R 0.449, F -0.523)]  [G loss: 0.587] \n",
      "2505 [D loss: (-0.038)(R 0.444, F -0.519)]  [G loss: 0.618] \n",
      "2506 [D loss: (-0.037)(R 0.439, F -0.513)]  [G loss: 0.599] \n",
      "2507 [D loss: (-0.049)(R 0.445, F -0.543)]  [G loss: 0.596] \n",
      "2508 [D loss: (-0.027)(R 0.457, F -0.511)]  [G loss: 0.610] \n",
      "2509 [D loss: (-0.023)(R 0.463, F -0.509)]  [G loss: 0.580] \n",
      "2510 [D loss: (-0.027)(R 0.455, F -0.510)]  [G loss: 0.604] \n",
      "2511 [D loss: (-0.026)(R 0.461, F -0.512)]  [G loss: 0.612] \n",
      "2512 [D loss: (-0.011)(R 0.486, F -0.508)]  [G loss: 0.603] \n",
      "2513 [D loss: (-0.021)(R 0.460, F -0.503)]  [G loss: 0.592] \n",
      "2514 [D loss: (-0.021)(R 0.462, F -0.504)]  [G loss: 0.584] \n",
      "2515 [D loss: (-0.015)(R 0.453, F -0.484)]  [G loss: 0.588] \n",
      "2516 [D loss: (-0.019)(R 0.456, F -0.493)]  [G loss: 0.583] \n",
      "2517 [D loss: (-0.031)(R 0.431, F -0.494)]  [G loss: 0.568] \n",
      "2518 [D loss: (-0.018)(R 0.435, F -0.472)]  [G loss: 0.565] \n",
      "2519 [D loss: (-0.023)(R 0.418, F -0.464)]  [G loss: 0.555] \n",
      "2520 [D loss: (-0.031)(R 0.392, F -0.455)]  [G loss: 0.550] \n",
      "2521 [D loss: (-0.043)(R 0.374, F -0.460)]  [G loss: 0.535] \n",
      "2522 [D loss: (-0.024)(R 0.390, F -0.439)]  [G loss: 0.531] \n",
      "2523 [D loss: (-0.016)(R 0.390, F -0.421)]  [G loss: 0.534] \n",
      "2524 [D loss: (-0.031)(R 0.376, F -0.439)]  [G loss: 0.512] \n",
      "2525 [D loss: (-0.027)(R 0.364, F -0.417)]  [G loss: 0.501] \n",
      "2526 [D loss: (-0.024)(R 0.371, F -0.418)]  [G loss: 0.507] \n",
      "2527 [D loss: (-0.036)(R 0.384, F -0.456)]  [G loss: 0.519] \n",
      "2528 [D loss: (-0.026)(R 0.374, F -0.426)]  [G loss: 0.515] \n",
      "2529 [D loss: (-0.026)(R 0.374, F -0.425)]  [G loss: 0.502] \n",
      "2530 [D loss: (-0.029)(R 0.376, F -0.434)]  [G loss: 0.517] \n",
      "2531 [D loss: (-0.035)(R 0.371, F -0.441)]  [G loss: 0.508] \n",
      "2532 [D loss: (-0.054)(R 0.397, F -0.504)]  [G loss: 0.543] \n",
      "2533 [D loss: (-0.030)(R 0.388, F -0.447)]  [G loss: 0.545] \n",
      "2534 [D loss: (-0.041)(R 0.387, F -0.469)]  [G loss: 0.554] \n",
      "2535 [D loss: (-0.023)(R 0.423, F -0.469)]  [G loss: 0.563] \n",
      "2536 [D loss: (-0.021)(R 0.419, F -0.462)]  [G loss: 0.577] \n",
      "2537 [D loss: (-0.029)(R 0.430, F -0.488)]  [G loss: 0.592] \n",
      "2538 [D loss: (-0.032)(R 0.442, F -0.506)]  [G loss: 0.594] \n",
      "2539 [D loss: (-0.028)(R 0.450, F -0.506)]  [G loss: 0.613] \n",
      "2540 [D loss: (-0.009)(R 0.483, F -0.501)]  [G loss: 0.591] \n",
      "2541 [D loss: (-0.023)(R 0.453, F -0.499)]  [G loss: 0.595] \n",
      "2542 [D loss: (-0.022)(R 0.466, F -0.510)]  [G loss: 0.608] \n",
      "2543 [D loss: (-0.029)(R 0.461, F -0.518)]  [G loss: 0.608] \n",
      "2544 [D loss: (-0.026)(R 0.464, F -0.517)]  [G loss: 0.596] \n",
      "2545 [D loss: (-0.040)(R 0.442, F -0.522)]  [G loss: 0.599] \n",
      "2546 [D loss: (-0.026)(R 0.446, F -0.498)]  [G loss: 0.570] \n",
      "2547 [D loss: (-0.022)(R 0.436, F -0.479)]  [G loss: 0.578] \n",
      "2548 [D loss: (-0.027)(R 0.429, F -0.484)]  [G loss: 0.582] \n",
      "2549 [D loss: (-0.022)(R 0.441, F -0.485)]  [G loss: 0.572] \n",
      "2550 [D loss: (-0.020)(R 0.432, F -0.471)]  [G loss: 0.569] \n",
      "2551 [D loss: (-0.030)(R 0.415, F -0.474)]  [G loss: 0.561] \n",
      "2552 [D loss: (-0.032)(R 0.402, F -0.465)]  [G loss: 0.556] \n",
      "2553 [D loss: (-0.019)(R 0.407, F -0.445)]  [G loss: 0.547] \n",
      "2554 [D loss: (-0.021)(R 0.397, F -0.438)]  [G loss: 0.537] \n",
      "2555 [D loss: (-0.048)(R 0.373, F -0.468)]  [G loss: 0.547] \n",
      "2556 [D loss: (-0.027)(R 0.399, F -0.452)]  [G loss: 0.546] \n",
      "2557 [D loss: (-0.032)(R 0.388, F -0.453)]  [G loss: 0.538] \n",
      "2558 [D loss: (-0.019)(R 0.395, F -0.433)]  [G loss: 0.526] \n",
      "2559 [D loss: (-0.026)(R 0.381, F -0.433)]  [G loss: 0.525] \n",
      "2560 [D loss: (-0.033)(R 0.390, F -0.456)]  [G loss: 0.539] \n",
      "2561 [D loss: (-0.018)(R 0.430, F -0.466)]  [G loss: 0.536] \n",
      "2562 [D loss: (-0.027)(R 0.407, F -0.460)]  [G loss: 0.551] \n",
      "2563 [D loss: (-0.016)(R 0.422, F -0.454)]  [G loss: 0.577] \n",
      "2564 [D loss: (-0.030)(R 0.421, F -0.481)]  [G loss: 0.558] \n",
      "2565 [D loss: (-0.024)(R 0.418, F -0.465)]  [G loss: 0.545] \n",
      "2566 [D loss: (-0.016)(R 0.424, F -0.456)]  [G loss: 0.547] \n",
      "2567 [D loss: (-0.019)(R 0.421, F -0.460)]  [G loss: 0.552] \n",
      "2568 [D loss: (-0.034)(R 0.408, F -0.476)]  [G loss: 0.580] \n",
      "2569 [D loss: (-0.044)(R 0.419, F -0.507)]  [G loss: 0.578] \n",
      "2570 [D loss: (-0.018)(R 0.415, F -0.450)]  [G loss: 0.560] \n",
      "2571 [D loss: (-0.045)(R 0.446, F -0.536)]  [G loss: 0.587] \n",
      "2572 [D loss: (-0.036)(R 0.420, F -0.492)]  [G loss: 0.572] \n",
      "2573 [D loss: (-0.013)(R 0.448, F -0.474)]  [G loss: 0.557] \n",
      "2574 [D loss: (-0.027)(R 0.427, F -0.481)]  [G loss: 0.580] \n",
      "2575 [D loss: (-0.024)(R 0.431, F -0.480)]  [G loss: 0.561] \n",
      "2576 [D loss: (-0.024)(R 0.420, F -0.467)]  [G loss: 0.560] \n",
      "2577 [D loss: (-0.014)(R 0.431, F -0.458)]  [G loss: 0.546] \n",
      "2578 [D loss: (-0.026)(R 0.409, F -0.460)]  [G loss: 0.559] \n",
      "2579 [D loss: (-0.018)(R 0.418, F -0.454)]  [G loss: 0.540] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2580 [D loss: (-0.028)(R 0.406, F -0.462)]  [G loss: 0.545] \n",
      "2581 [D loss: (-0.016)(R 0.405, F -0.438)]  [G loss: 0.531] \n",
      "2582 [D loss: (-0.021)(R 0.403, F -0.445)]  [G loss: 0.546] \n",
      "2583 [D loss: (-0.030)(R 0.397, F -0.457)]  [G loss: 0.528] \n",
      "2584 [D loss: (-0.026)(R 0.397, F -0.450)]  [G loss: 0.538] \n",
      "2585 [D loss: (-0.026)(R 0.385, F -0.438)]  [G loss: 0.514] \n",
      "2586 [D loss: (-0.045)(R 0.363, F -0.453)]  [G loss: 0.512] \n"
     ]
    }
   ],
   "source": [
    "gan.train(     \n",
    "    x_train\n",
    "    , batch_size = BATCH_SIZE\n",
    "    , epochs = EPOCHS\n",
    "    , run_folder = RUN_FOLDER\n",
    "    , print_every_n_batches = PRINT_EVERY_N_BATCHES\n",
    "    , n_critic = N_CRITIC\n",
    "    , clip_threshold = CLIP_THRESHOLD\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
